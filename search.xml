<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>也谈苏东坡</title>
      <link href="/sudongpo/"/>
      <url>/sudongpo/</url>
      
        <content type="html"><![CDATA[<blockquote><p>何夜无月，何夜无竹柏，但少闲人如吾两人尔</p></blockquote><p>李一冰的《苏东坡新传》在微信读书里躺了一年有余，偶尔想起翻开几页，不过大多时候是想不起的。年纪越大越发的浮躁，少了上学时的心平气和，工作之后阅读的功利心重了许多，总想通过几段文字掌握多少知识，开阔些许眼界，对于这些“不务正业”的书难有闲心了。<br>&nbsp;</p><blockquote><p>春江水暖鸭先知</p></blockquote><p>对于苏东坡，上学前就会背他的《赤壁怀古》，记得小学时老师问谁会背词，便大东江东去了一番，引得不少目光。但对东坡其人知之甚少，后来开始慢慢接触他的诗与杂文，读了林语堂的《苏东坡传》，被他的乐观旷达所吸引。遇到不如意的，“一蓑烟雨任平生”；</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 杂记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>golang zk大量disconnected event</title>
      <link href="/golang-zk-statedisconnected/"/>
      <url>/golang-zk-statedisconnected/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在容器平台上我们提供了<code>zk</code>做白名单功能，<code>Pod</code>启动时 sidecar会自动注册<code>zk</code>。昨天遇到<code>zk server</code>抖动，<code>sidecar</code>容器输出大量<code>StateDisconnected</code>事件，zk正常后仍无法恢复，由于大量日志造成<code>sidecar</code>容器 cpu占用过高，进而引发<code>dockerd</code>cpu占用过高，严重时影响dockerd正常调用。</p><h2 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h2><h3 id="问题复现"><a href="#问题复现" class="headerlink" title="问题复现"></a>问题复现</h3><p>正常情况下，<code>sidecar</code>启动后会去注册<code>zk</code>：<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker logs -f 01a1a4a74785</span></span><br><span class="line">I0302 15:04:05.476463       1 manager.go:116] start run plugin zk</span><br><span class="line">2021/03/02 15:04:05 Connected to 10.38.161.60:11000</span><br><span class="line">I0302 15:04:05.488006       1 zk.go:152] zookeeper connect succeed: zk.srv:11000</span><br><span class="line">2021/03/02 15:04:05 authenticated: id=33746806328105493, timeout=30000</span><br><span class="line">2021/03/02 15:04:05 re-submitting `0` credentials after reconnect</span><br><span class="line">I0302 15:04:05.516446       1 zk.go:220] watching zk node:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72] <span class="keyword">in</span> cluster[xxx] <span class="comment">#注册成功，开始watch</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>通过<code>iptable</code>s来模拟异常，首先进入到容器<code>network namesapce</code><br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pod=htool-6875bcb898-w7llc</span><br><span class="line">containerid=$(docker ps |grep <span class="variable">$pod</span>|awk <span class="string">'{print $1}'</span>|head -n 1)</span><br><span class="line">pid=$(docker inspect -f {{.State.Pid}} <span class="variable">$containerid</span>)</span><br><span class="line">nsenter -n --target <span class="variable">$pid</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>使用<code>iptables</code> <code>drop</code>掉发往<code>zk</code>的请求(11000为zk server端口)<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -A OUTPUT -p tcp -m tcp --dport 11000 -j DROP</span><br></pre></td></tr></tbody></table></figure><p></p><p>zk client自动重试（1s一次），日志显示<code>Failed to connect to 10.38.161.54:11000: dial tcp 10.38.161.54:11000: i/o timeout</code><br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">I0302 15:04:05.516446       1 zk.go:220] watching zk node:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72] <span class="keyword">in</span> cluster[xxx]</span><br><span class="line">2021/03/02 15:08:55 recv loop terminated: err=failed to <span class="built_in">read</span> from connection: <span class="built_in">read</span> tcp 10.46.12.72:36884-&gt;10.38.161.60:11000: i/o timeout</span><br><span class="line">2021/03/02 15:08:55 send loop terminated: err=&lt;nil&gt;</span><br><span class="line">2021/03/02 15:08:56 Failed to connect to 10.38.161.54:11000: dial tcp 10.38.161.54:11000: i/o timeout</span><br></pre></td></tr></tbody></table></figure><p></p><p>网络恢复，删除<code>iptables</code><br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -D OUTPUT -p tcp -m tcp --dport 11000 -j DROP</span><br></pre></td></tr></tbody></table></figure><p></p><p>出现大量<code>StateDisconnected</code>日志<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I0302 15:09:50.951897       1 zk.go:232] Unknown zk event[StateDisconnected] <span class="keyword">for</span> znode:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72]</span><br><span class="line">I0302 15:09:50.951893       1 zk.go:232] Unknown zk event[StateDisconnected] <span class="keyword">for</span> znode:[/tasks/cluster.xxx_default_deployment.htool/10.46.12.72]</span><br><span class="line">...</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="问题分析-1"><a href="#问题分析-1" class="headerlink" title="问题分析"></a>问题分析</h3><p><code>sidecar</code>中zk watch代码如下：<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">exist, _, eventCh, err := conn.ExistsW(node) <span class="comment">//监听zk事件</span></span><br><span class="line">watcher:</span><br><span class="line">        <span class="keyword">for</span> {</span><br><span class="line">                <span class="keyword">select</span> {</span><br><span class="line">                <span class="keyword">case</span> e := &lt;-eventCh:</span><br><span class="line">                        <span class="keyword">switch</span> e.State {</span><br><span class="line">                        <span class="keyword">case</span> zk.StateExpired:</span><br><span class="line">                                <span class="keyword">return</span> fmt.Errorf(<span class="string">"node[%v] expired"</span>, node)</span><br><span class="line">                        <span class="keyword">case</span> zk.StateConnected, zk.StateHasSession:</span><br><span class="line">                                <span class="keyword">return</span> fmt.Errorf(<span class="string">"Get zk event: %v "</span>, e.State)</span><br><span class="line">                        <span class="keyword">default</span>:</span><br><span class="line">                                klog.Infof(<span class="string">"Get zk event[%v] for znode:[%v]"</span>, e.State, node) <span class="comment">// 出错位置</span></span><br><span class="line">                        }</span><br><span class="line">                <span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line">                        <span class="comment">// we close the conn in caller</span></span><br><span class="line">                        <span class="keyword">break</span> watcher</span><br><span class="line">                }</span><br><span class="line">        }</span><br></pre></td></tr></tbody></table></figure><p></p><p><code>ExistsW</code>函数由<code>github.com/samuel/go-zookeeper/zk</code>库提供，监听zk给定目录的事件<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Conn)</span> <span class="title">ExistsW</span><span class="params">(path <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">bool</span>, *Stat, &lt;-<span class="keyword">chan</span> Event, error)</span></span> {</span><br><span class="line">    <span class="keyword">var</span> ech &lt;-<span class="keyword">chan</span> Event</span><br><span class="line">    ...</span><br><span class="line">    ech = c.addWatcher(path, watchTypeData)</span><br><span class="line">    <span class="keyword">return</span> exists, &amp;res.Stat, ech, err</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>当zk异常恢复后，<code>c.addWatcher</code>中的<code>channel</code>被<code>close</code>，即<code>sidecar</code>中<code>eventCh</code>关闭，进入死循环。</p><h3 id="修复验证"><a href="#修复验证" class="headerlink" title="修复验证"></a>修复验证</h3><p>知道了原因，修复很简单，判断下eventCh状态即可<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> {</span><br><span class="line">    <span class="keyword">select</span> {</span><br><span class="line">    <span class="keyword">case</span> e, ok := &lt;-eventCh:</span><br><span class="line">        <span class="keyword">if</span> !ok {</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"event channel closed"</span>)</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> e.Err != <span class="literal">nil</span> {</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"Get zk event: %v, err: %v"</span>, e.State, e.Err)</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">switch</span> e.State {</span><br><span class="line">        <span class="keyword">case</span> zk.StateExpired:</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"node[%v] expired"</span>, node)</span><br><span class="line">        <span class="keyword">case</span> zk.StateConnected, zk.StateHasSession:</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"Get zk event: %v "</span>, e.State)</span><br><span class="line">        <span class="keyword">default</span>:</span><br><span class="line">            klog.Infof(<span class="string">"Get zk event[%v] for znode:[%v]"</span>, e.State, node)</span><br><span class="line">        }</span><br><span class="line">    }</span><br></pre></td></tr></tbody></table></figure><p></p><p>在修复代码后，再次验证可正常注册<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2021</span>/<span class="number">03</span>/<span class="number">02</span> <span class="number">15</span>:<span class="number">13</span>:<span class="number">40</span> Failed to connect to <span class="number">10.38</span><span class="number">.161</span><span class="number">.60</span>:<span class="number">11000</span>: dial tcp <span class="number">10.38</span><span class="number">.161</span><span class="number">.60</span>:<span class="number">11000</span>: i/o timeout</span><br><span class="line"><span class="number">2021</span>/<span class="number">03</span>/<span class="number">02</span> <span class="number">15</span>:<span class="number">13</span>:<span class="number">40</span> Connected to <span class="number">10.38</span><span class="number">.161</span><span class="number">.55</span>:<span class="number">11000</span></span><br><span class="line"><span class="number">2021</span>/<span class="number">03</span>/<span class="number">02</span> <span class="number">15</span>:<span class="number">13</span>:<span class="number">40</span> authentication failed: zk: session has been expired by the server</span><br><span class="line">W0302 <span class="number">15</span>:<span class="number">13</span>:<span class="number">40.222923</span>       <span class="number">1</span> zk.<span class="keyword">go</span>:<span class="number">300</span>] meet error when watching node path: Get zk event: StateDisconnected, err: zk: session has been expired by the server</span><br><span class="line"><span class="number">2021</span>/<span class="number">03</span>/<span class="number">02</span> <span class="number">15</span>:<span class="number">13</span>:<span class="number">40</span> Connected to <span class="number">10.38</span><span class="number">.161</span><span class="number">.54</span>:<span class="number">11000</span></span><br><span class="line"><span class="number">2021</span>/<span class="number">03</span>/<span class="number">02</span> <span class="number">15</span>:<span class="number">13</span>:<span class="number">40</span> authenticated: id=<span class="number">177861994644216038</span>, timeout=<span class="number">30000</span></span><br><span class="line"><span class="number">2021</span>/<span class="number">03</span>/<span class="number">02</span> <span class="number">15</span>:<span class="number">13</span>:<span class="number">40</span> re-submitting <span class="string">`1`</span> credentials after reconnect</span><br><span class="line">I0302 <span class="number">15</span>:<span class="number">13</span>:<span class="number">41.238524</span>       <span class="number">1</span> zk.<span class="keyword">go</span>:<span class="number">220</span>] watching zk node:[/tasks/cluster.xxx_default_deployment.htool/<span class="number">10.46</span><span class="number">.12</span><span class="number">.72</span>] in cluster[xxx]</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这个问题其实与<code>zk</code>没关系，是由于没有判断<code>channel</code>状态，陷入死循环。通常情况下大部分应用只有退出时才会关闭<code>channel</code>，不需要特殊处理。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> golang </tag>
            
            <tag> zk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s中shell脚本启动如何传递信号</title>
      <link href="/docker-shell-signal/"/>
      <url>/docker-shell-signal/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在k8s或docker中，有时候我们需要通过shell来启动程序，但是默认shell不会传递信号（sigterm）给子进程，当在pod终止时应用无法优雅退出，直到最大时间时间后强制退出（<code>kill -9</code>）。</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>普通情况下，大多业务的启动命令如下<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">command</span>: [<span class="string">"binary"</span>, <span class="string">"-flags"</span>, ...]</span><br></pre></td></tr></tbody></table></figure><p></p><p>主进程做为1号进程会收到<code>sigterm</code>信号，优雅退出(需要程序捕获信号); 而通过脚本启动时，<code>shell</code>作为1号进程，不会显示传递信号给子进程，造成子进程无法优雅退出，直到最大退出时间后强制终止。</p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><h3 id="exec"><a href="#exec" class="headerlink" title="exec"></a>exec</h3><p>如何只需一个进程收到信号，可通过<code>exec</code>，<code>exec</code>会替换当前shell进程，即<code>pid</code>不变<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="comment"># do something</span></span><br><span class="line"><span class="built_in">exec</span> binay -flags ...</span><br></pre></td></tr></tbody></table></figure><p></p><p>正常情况测试命令如下，使用sleep来模拟应用<code>sh -c 'echo "start"; sleep 100'</code>：<br><code>pstree</code>展示如下，<code>sleep</code>进程会生成一个子进程<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash(28701)───sh(24588)───sleep(24589)</span><br></pre></td></tr></tbody></table></figure><p></p><p>通过<code>exec</code>运行后，命令<code>sh -c 'echo "start"; exec sleep 100'</code><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash(28701)───sleep(24664)</span><br></pre></td></tr></tbody></table></figure><p></p><p>加入<code>exec</code>后，<code>sleep</code>进程替换了shell进程，没有生成子进程</p><p>此种方式可以收到信号，但只适用于一个子进程的情况</p><h3 id="trap"><a href="#trap" class="headerlink" title="trap"></a>trap</h3><p>在shell中可以显示通过<code>trap</code>捕捉信号传递给子进程<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"start"</span></span><br><span class="line">binary -flags... &amp;</span><br><span class="line">pid=<span class="string">"$!"</span></span><br><span class="line"></span><br><span class="line"><span class="function">_kill</span>() {</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"receive sigterm"</span></span><br><span class="line">  <span class="built_in">kill</span> <span class="variable">$pid</span> <span class="comment">#传递给子进程</span></span><br><span class="line">  <span class="built_in">wait</span> <span class="variable">$pid</span></span><br><span class="line">  <span class="built_in">exit</span> 0</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="built_in">trap</span> _kill SIGTERM <span class="comment">#捕获信号</span></span><br><span class="line"><span class="built_in">wait</span> <span class="comment">#等待子进程退出</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>此种方式需要改动启动脚本，显示传递信号给子进程</p><h2 id="docker-init"><a href="#docker-init" class="headerlink" title="docker-init"></a>docker-init</h2><p><a href="https://docs.docker.com/engine/reference/run/#specify-an-init-process" target="_blank" rel="noopener">docker-init</a>即在docker启动时加入<code>--init</code>参数，docker-int会作为一号进程，会向子进程传递信号并且会回收僵尸进程。</p><p>遗憾的是k8s并不支持<code>--init</code>参数，用户可在镜像中声明init进程，更多可参考<a href="./container-init.md">container-init</a><br></p><figure class="highlight dockerfile"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="bash"> wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init_1.2.2_amd64</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> chmod +x /usr/bin/dumb-init</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"/usr/bin/dumb-init"</span>, <span class="string">"-v"</span>, <span class="string">"--"</span>]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"nginx"</span>, <span class="string">"-g"</span>, <span class="string">"daemon off;"</span>]</span></span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优化Kubernetes集群内DNS</title>
      <link href="/k8s-dns-optimize/"/>
      <url>/k8s-dns-optimize/</url>
      
        <content type="html"><![CDATA[<p>kubernetes集群内置的dns插件<code>kubedns/coredns</code>在高并发情况下可能遇到性能瓶颈，以下从配置与本地缓存方面说明如何减少dns查询失败率，提高性能。</p><h2 id="配置优化"><a href="#配置优化" class="headerlink" title="配置优化"></a>配置优化</h2><h3 id="dnsPolicy"><a href="#dnsPolicy" class="headerlink" title="dnsPolicy"></a>dnsPolicy</h3><p>k8s 默认的 <code>dnsPolicy</code> 是<code>ClusterFirst</code>，因为 <code>ndots</code> 和 <code>serach domain</code> 在访问外部 dns 会有额外的查询次数。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># cat /etc/resolv.conf </span></span><br><span class="line">nameserver 10.254.0.2</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br><span class="line">/ <span class="comment"># </span></span><br><span class="line">/ <span class="comment"># </span></span><br><span class="line">/ <span class="comment">#  host -v mi.com</span></span><br><span class="line">Trying <span class="string">"mi.com.default.svc.cluster.local"</span></span><br><span class="line">Trying <span class="string">"mi.com.svc.cluster.local"</span></span><br><span class="line">Trying <span class="string">"mi.com.cluster.local"</span></span><br><span class="line">Trying <span class="string">"mi.com"</span></span><br><span class="line">;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 38967</span><br><span class="line">;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0</span><br><span class="line"></span><br><span class="line">;; QUESTION SECTION:</span><br><span class="line">;mi.com.                                IN        A</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">mi.com.                        30        IN        A        58.83.160.156</span><br></pre></td></tr></tbody></table></figure><p>如果不访问service，调整<code>dnsPolicy</code>为<code>Default</code>，直接走宿主机的dns</p><h3 id="ndots"><a href="#ndots" class="headerlink" title="ndots"></a>ndots</h3><p>如需访问service，尽量减少<code>ndots</code>（默认5）即域名中点的个数小于<code>ndots</code>会按照search域（mi.com.default.svc.cluster.local）依次查询，若查询不到再查询原始域名，总共进行8次dns查询（4次ipv4, 4次ipv6）</p><p>设置<code>ndots</code>为1后，只有两次查询（1次ipv4, ipv6）<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment">#  host -v mi.com</span></span><br><span class="line">Trying <span class="string">"mi.com"</span></span><br><span class="line">;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 23894</span><br><span class="line">;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0</span><br><span class="line"></span><br><span class="line">;; QUESTION SECTION:</span><br><span class="line">;mi.com.                                IN        A</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">mi.com.                        30        IN        A        58.83.160.156</span><br></pre></td></tr></tbody></table></figure><p></p><p>但此种方式service域名分割大于等于<code>ndots</code>，则解析不到，需要业务自行判断合适的<code>ndots</code>值<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment">#  host -v prometheus.kube-system</span></span><br><span class="line">Trying <span class="string">"prometheus.kube-system"</span></span><br><span class="line">Host prometheus.kube-system not found: 3(NXDOMAIN)</span><br><span class="line">Received 115 bytes from 10.254.0.2<span class="comment">#53 in 8 ms</span></span><br><span class="line">Received 115 bytes from 10.254.0.2<span class="comment">#53 in 8 ms</span></span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="coredns优化"><a href="#coredns优化" class="headerlink" title="coredns优化"></a>coredns优化</h3><p>调整合理的副本数，阿里建议<code>coredns:node=1:8</code>，启动<code>AutoPath</code>插件减少查询次数，见<a href="2">DNS性能优化</a></p><h2 id="DNS缓存"><a href="#DNS缓存" class="headerlink" title="DNS缓存"></a>DNS缓存</h2><h3 id="NodeLocalDNS"><a href="#NodeLocalDNS" class="headerlink" title="NodeLocalDNS"></a>NodeLocalDNS</h3><p>NodeLocal DNSCache 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能，<br>借助这种新架构，Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了 iptables DNAT 规则和连接跟踪。</p><p>架构如下:<br><img src="https://d33wubrfki0l68.cloudfront.net/bf8e5eaac697bac89c5b36a0edb8855c860bfb45/6944f/images/docs/nodelocaldns.svg" alt="local-dns"></p><p>NodeLocalDNS的设计提案见（<a href="3">nodelocal-dns-cache</a>）</p><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p>官方安装方式见<a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns" target="_blank" rel="noopener">nodelocaldns</a>，需要自行替换变量</p><p>可通过如下脚本，一键安装（注意设置kubedns svc ClusterIP）<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># registery</span></span><br><span class="line">docker_registery=k8s.gcr.io/dns/k8s-dns-node-cache</span><br><span class="line"><span class="comment"># kube-dns svc clusterip</span></span><br><span class="line">kubedns_svc=10.254.0.2</span><br><span class="line"><span class="comment"># nodelocaldns ip</span></span><br><span class="line">nodelocaldns_ip=169.254.20.10</span><br><span class="line"><span class="comment"># kube-proxy mode, iptables or ipvs</span></span><br><span class="line">kubeproxy_mode=iptables</span><br><span class="line">result=result.yaml</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">${kubeproxy_mode}</span> == <span class="string">"ipvs"</span> ]; <span class="keyword">then</span></span><br><span class="line">    sed -e <span class="string">"s|k8s.gcr.io/dns/k8s-dns-node-cache|<span class="variable">$docker_registery</span>|g"</span> \</span><br><span class="line">        -e <span class="string">"s/__PILLAR__CLUSTER__DNS__/<span class="variable">$kubedns_svc</span>/g"</span> \</span><br><span class="line">        -e <span class="string">"s/__PILLAR__LOCAL__DNS__/<span class="variable">$nodelocaldns_ip</span>/g"</span> \</span><br><span class="line">        -e <span class="string">'s/[ |,]__PILLAR__DNS__SERVER__//g'</span> \</span><br><span class="line">        -e <span class="string">"s/__PILLAR__DNS__DOMAIN__/cluster.local/g"</span> nodelocaldns.yaml &gt;<span class="variable">$result</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    sed -e <span class="string">"s|k8s.gcr.io/dns/k8s-dns-node-cache|<span class="variable">$docker_registery</span>|g"</span> \</span><br><span class="line">        -e <span class="string">"s/__PILLAR__DNS__SERVER__/<span class="variable">$kubedns_svc</span>/g"</span> \</span><br><span class="line">        -e <span class="string">"s/__PILLAR__LOCAL__DNS__/<span class="variable">$nodelocaldns_ip</span>/g"</span> \</span><br><span class="line">        -e <span class="string">"s/__PILLAR__DNS__DOMAIN__/cluster.local/g"</span> nodelocaldns.yaml &gt;<span class="variable">$result</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">kubectl apply -f <span class="variable">$result</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>创建完成后，每个节点运行一个pod，查看pod(个别节点ingress-nginx占用8080端口，导致nodelocaldns启动失败)<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl  get po -n kube-system -l k8s-app=node-local-dns -o wide</span></span><br><span class="line">NAME                   READY   STATUS             RESTARTS   AGE    IP              NODE                            NOMINATED NODE   READINESS GATES</span><br><span class="line">node-local-dns-2fvxb   0/1     CrashLoopBackOff   4          103s   10.38.200.195   node04          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-local-dns-4zmcd   1/1     Running            0          54d    10.38.201.55    node06   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-local-dns-55tzg   1/1     Running            0          60d    10.38.200.186   node02          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-local-dns-cctg7   1/1     Running            0          54d    10.38.200.242   node07   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-local-dns-khgmm   1/1     Running            0          54d    10.38.201.36    node08   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-local-dns-mbr64   1/1     Running            0          60d    10.38.200.187   node05          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-local-dns-t67vw   1/1     Running            0          60d    10.38.200.188   node03          &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-local-dns-tmm92   1/1     Running            14         54d    10.38.200.57    node09   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></tbody></table></figure><p></p><p>默认配置如下：<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">cluster.local:53 {</span><br><span class="line">    errors</span><br><span class="line">    cache {</span><br><span class="line">            success 9984 30 <span class="comment"># 默认成功缓存30s</span></span><br><span class="line">            denial 9984 5 <span class="comment">#失败缓存5s</span></span><br><span class="line">    }</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    <span class="built_in">bind</span> 169.254.20.10 10.254.0.2 <span class="comment">#本地监听ip</span></span><br><span class="line">    forward . 10.254.132.95 { <span class="comment">#转发到kubedns-upstream</span></span><br><span class="line">            force_tcp</span><br><span class="line">    }</span><br><span class="line">    prometheus :9253 <span class="comment">#监控接口</span></span><br><span class="line">    health 169.254.20.10:8080 <span class="comment">#健康检测端口</span></span><br><span class="line">    }</span><br><span class="line"><span class="keyword">in</span>-addr.arpa:53 {</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    <span class="built_in">bind</span> 169.254.20.10 10.254.0.2</span><br><span class="line">    forward . 10.254.132.95 {</span><br><span class="line">            force_tcp</span><br><span class="line">    }</span><br><span class="line">    prometheus :9253</span><br><span class="line">    }</span><br><span class="line">ip6.arpa:53 {</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    <span class="built_in">bind</span> 169.254.20.10 10.254.0.2</span><br><span class="line">    forward . 10.254.132.95 {</span><br><span class="line">            force_tcp</span><br><span class="line">    }</span><br><span class="line">    prometheus :9253</span><br><span class="line">    }</span><br><span class="line">.:53 {</span><br><span class="line">    errors</span><br><span class="line">    cache 30</span><br><span class="line">    reload</span><br><span class="line">    loop</span><br><span class="line">    <span class="built_in">bind</span> 169.254.20.10 10.254.0.2</span><br><span class="line">    forward . /etc/resolv.conf</span><br><span class="line">    prometheus :9253</span><br><span class="line">    }</span><br></pre></td></tr></tbody></table></figure><p></p><p>节点上查看localdns的网卡，本地将监听<code>169.254.20.10</code>与<code>10.254.0.2</code>两个地址，拦截kubedns((默认<code>10.254.0.2</code>)的请求，命中后直接返回，若未命中转发到kubedns(对应service <code>kube-dns-upstream</code>，kube-dns-upstream由localdns创建绑定kubedns pod)<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ip addr show nodelocaldns</span></span><br><span class="line">182232: nodelocaldns: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN </span><br><span class="line">    link/ether 4e:62:1c:fd:56:12 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 169.254.20.10/32 brd 169.254.20.10 scope global nodelocaldns</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.254.0.2/32 brd 10.254.0.2 scope global nodelocaldns</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></tbody></table></figure><p></p><p>iptables规则，使用<code>NOTRACK</code>跳过其它表处理<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">iptables-save | egrep <span class="string">"10.254.0.2|169.254.20.10"</span></span><br><span class="line">-A PREROUTING -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK</span><br><span class="line">-A PREROUTING -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK</span><br><span class="line">-A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -j NOTRACK</span><br><span class="line">-A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -j NOTRACK</span><br><span class="line"></span><br><span class="line">-A OUTPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j NOTRACK</span><br><span class="line">-A OUTPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j NOTRACK</span><br><span class="line"></span><br><span class="line">-A INPUT -d 10.254.0.2/32 -p udp -m udp --dport 53 -j ACCEPT</span><br><span class="line">-A INPUT -d 10.254.0.2/32 -p tcp -m tcp --dport 53 -j ACCEPT</span><br><span class="line">-A OUTPUT -s 10.254.0.2/32 -p udp -m udp --sport 53 -j ACCEPT</span><br><span class="line">-A OUTPUT -s 10.254.0.2/32 -p tcp -m tcp --sport 53 -j ACCEPT</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">-A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment <span class="string">"kube-system/kube-dns:dns-tcp cluster IP"</span> -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4</span><br><span class="line">-A KUBE-SERVICES -d 10.254.0.2/32 -p tcp -m comment --comment <span class="string">"kube-system/kube-dns:metrics cluster IP"</span> -m tcp --dport 9153 -j KUBE-SVC-JD5MR3NA4I4DYORP</span><br><span class="line">-A KUBE-SERVICES -d 10.254.0.2/32 -p udp -m comment --comment <span class="string">"kube-system/kube-dns:dns cluster IP"</span> -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU</span><br></pre></td></tr></tbody></table></figure><p></p><p>在pod通过localdns解析域名<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl  exec -it dns-perf-client-64cfb49f9-9c5hg sh</span></span><br><span class="line">/ <span class="comment"># nslookup kubernetes 169.254.20.10</span></span><br><span class="line">Server:                169.254.20.10</span><br><span class="line">Address:        169.254.20.10<span class="comment">#53</span></span><br><span class="line"></span><br><span class="line">Name:        kubernetes.default.svc.cluster.local</span><br><span class="line">Address: 10.254.0.1</span><br></pre></td></tr></tbody></table></figure><p></p><h4 id="压测"><a href="#压测" class="headerlink" title="压测"></a>压测</h4><p>通过<code>dnsperf</code>进行压测</p><p>测试域名列表如下<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat records.txt </span></span><br><span class="line">mi.com A</span><br><span class="line">github.com A</span><br><span class="line">www.microsoft.com A</span><br><span class="line">www.aliyun.com A</span><br><span class="line">kubernetes.io A</span><br><span class="line">nginx A</span><br><span class="line">nginx.default A</span><br><span class="line">kubernetes A</span><br><span class="line">kubernetes.default.svc.cluster.local A</span><br><span class="line">kube-dns.kube-system.svc.cluster.local A</span><br></pre></td></tr></tbody></table></figure><p></p><p>测试命令<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnsperf -l 120 -s 10.254.0.2 -d records.txt</span><br></pre></td></tr></tbody></table></figure><p></p><p>结果如下<br>| |client number|qps|avg-lantency(ms)|stddev(ms)|lost| |<br>|:—-|:—-|:—-|:—-|:—-|:—-|:—-|<br>|kubedns(1 pod)|1|53910|1.83|6.07|0%| |<br>|kubedns(2 pod)|2|110000|1.83|1.94|9%| |<br>|kubedns(4 pod)|4|120000|3.2|0.8|24%| |<br>|nodelocaldns|1|71494|1.39|1.66|0%| |<br>|nodelocaldns|2|142000|1.37|1.55|0%| |</p><p>相比<code>nodelocaldns</code>，<code>localdns</code>查询性能提高了33%，而且延时相对更小，由于<code>localdns</code>是分布式的整体qps相对kubedns有较大优势。当前测试相对简单，大部分请求会命中缓存，完整的测试结果待进一步验证。</p><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：</p><ul><li>大幅减少dns查询延时</li><li>提高dns qps</li><li>不经过<code>iptables</code>与<code>conntrack</code></li><li>默认使用tcp查询dns，避免 dns 5秒延时</li></ul><p>缺点：</p><ul><li>单点故障（OOM/Evicted/Config Error/Upgrade），社区通过起一个探测daemonset监听localdns状态，如果localdns异常将去掉iptables规则</li><li><code>hostnetwork</code>, 占用多个端口（8080, 9253等）</li><li>ipvs模式下，需要改动kubelet默认dns配置（<code>NOTRACK</code>将对<code>ipvs</code>无效，除非service后端实例为0）</li></ul><p>注意事项</p><ul><li>低版本dns存在tcp请求内存泄露</li><li>安装时<code>iptables</code>与<code>ipvs</code>配置不同</li></ul><h4 id="HA"><a href="#HA" class="headerlink" title="HA"></a>HA</h4><ul><li>社区提案将<code>iptables</code>写入规则从<code>nodelocaldns</code>拆分为单独的daemonset，通过监听<code>localdns</code>地址来判断是否写入或删除<code>iptables</code>规则（ipvs默认下无效）</li><li>在<code>/etc/resolv.conf</code>配置多个<code>nameservers</code>(不推荐，不同基础库表现不同，如<code>glibc 2.16+</code>查询dns时会向多个<code>nameservers</code>发送请求，反而造成了请求激增)</li></ul><h4 id="灰度方式"><a href="#灰度方式" class="headerlink" title="灰度方式"></a>灰度方式</h4><ul><li>通过<code>dnsConfi</code>g配置Pod级别dns（需要配置启动参数localip）</li><li>通过设置<code>nodeselector</code>灰度Node级别dns策略</li></ul><h3 id="本地DNS缓存"><a href="#本地DNS缓存" class="headerlink" title="本地DNS缓存"></a>本地DNS缓存</h3><p>除了nodelocaldns，用户还可以在容器内或者添加sidecar来启用dns缓存</p><ol><li><p>通过在镜像中加入nscd进程，缓存dns，如下：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y nscd &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class="line">CMD service nscd start; bash -c <span class="string">"sleep 3600"</span></span><br></pre></td></tr></tbody></table></figure><p> 此种方式需要用户改动镜像，或者加入额外脚本配置<code>nscd</code></p></li><li><p>另外可以配置可配置dns缓存 sidecar（如<code>coredns</code>, <code>dnsmasq</code>）来提高性能，此种方式灵活性高，但需要改动pod配置，而且较<code>nodelocaldns</code>浪费资源</p></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/nodelocaldns/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tasks/administer-cluster/nodelocaldns/</a><br>[2] <a href="https://help.aliyun.com/document_detail/172339.html" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/172339.html</a><br>[3] <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0030-nodelocal-dns-cache.md" target="_blank" rel="noopener">https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/0030-nodelocal-dns-cache.md</a><br>[4] <a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md" target="_blank" rel="noopener">https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md</a><br>[5] <a href="https://lework.github.io/2020/11/09/node-local-dns/" target="_blank" rel="noopener">https://lework.github.io/2020/11/09/node-local-dns/</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> dns </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubernetes apiserver限流方案</title>
      <link href="/k8s-rate-limit/"/>
      <url>/k8s-rate-limit/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了防止突发流量影响apiserver可用性，k8s支持多种限流配置，包括：</p><ul><li>MaxInFlightLimit，server级别整体限流</li><li>Client限流</li><li>EventRateLimit, 限制event</li><li>APF，更细力度的限制配置</li></ul><h3 id="MaxInFlightLimit"><a href="#MaxInFlightLimit" class="headerlink" title="MaxInFlightLimit"></a>MaxInFlightLimit</h3><p>MaxInFlightLimit限流，apiserver默认可设置最大并发量（集群级别，区分只读与修改操作），通过参数<code>--max-requests-inflight</code>和 <code>--max-mutating-requests-inflight</code>， 可以简单实现限流。</p><h3 id="Client限流"><a href="#Client限流" class="headerlink" title="Client限流"></a>Client限流</h3><p>例如client-go默认的qps为5，但是只支持客户端限流，集群管理员无法控制用户行为。</p><h3 id="EventRateLimit"><a href="#EventRateLimit" class="headerlink" title="EventRateLimit"></a>EventRateLimit</h3><p>EventRateLimit在1.13之后支持，只限制event请求，集成在apiserver内部webhoook中，可配置某个用户、namespace、server等event操作限制，通过webhook形式实现。</p><p>具体原理可以参考<a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/#eventratelimit" target="_blank" rel="noopener">提案</a>，每个eventratelimit 配置使用一个单独的令牌桶限速器，每次event操作，遍历每个匹配的限速器检查是否能获取令牌，如果可以允许请求，否则返回<code>429</code>。</p><p><strong>优点</strong></p><ul><li>实现简单，允许一定量的并发</li><li>可支持server/namespace/user等级别的限流</li></ul><p><strong>缺点</strong></p><ul><li>仅支持event，通过webhook实现只能拦截修改类请求</li><li>所有namespace的限流相同，没有优先级</li></ul><h3 id="API-优先级和公平性"><a href="#API-优先级和公平性" class="headerlink" title="API 优先级和公平性"></a>API 优先级和公平性</h3><p>apiserver默认的限流方式太过简单，一个错误的客户端发送大量请求可能造成其他客户端请求异常，也不支持突发流量。</p><p>API 优先级和公平性（APF）是MaxInFlightLimit限流的一种替代方案，设计文档见<a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness" target="_blank" rel="noopener">提案</a>。</p><p>API 优先级和公平性（1.15以上，alpha版本）， 以更细粒度（byUser，byNamespace）对请求进行分类和隔离。 支持突发流量，通过使用公平排队技术从队列中分发请求从而避免饥饿。</p><p>APF限流通过两种资源，<code>PriorityLevelConfigurations</code>定义隔离类型和可处理的并发预算量，还可以调整排队行为。 <code>FlowSchemas</code>用于对每个入站请求进行分类，并与一个 <code>PriorityLevelConfigurations</code>相匹配。</p><p>可对用户或用户组或全局进行某些资源某些请求的限制，如限制default namespace写services put/patch请求。</p><p><strong>优点</strong></p><ul><li>考虑情况较全面，支持优先级，白名单等</li><li>可支持server/namespace/user/resource等细粒度级别的限流</li></ul><p><strong>缺点</strong></p><ul><li>配置复杂，不直观，需要对APF原理深入了解</li><li>功能较新，缺少生产环境验证</li></ul><p><strong>APF测试</strong><br>开启APF，需要在apiserver配置<code>--feature-gates=APIPriorityAndFairness=true --runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true</code></p><p>开启后，获取默认的FlowSchemas</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get flowschemas.flowcontrol.apiserver.k8s.io </span><br><span class="line">NAME                           PRIORITYLEVEL     MATCHINGPRECEDENCE   DISTINGUISHERMETHOD   AGE    MISSINGPL</span><br><span class="line">system-leader-election         leader-election   100                  ByUser                152m   False</span><br><span class="line">workload-leader-election       leader-election   200                  ByUser                152m   False</span><br><span class="line">system-nodes                   system            500                  ByUser                152m   False</span><br><span class="line">kube-controller-manager        workload-high     800                  ByNamespace           152m   False</span><br><span class="line">kube-scheduler                 workload-high     800                  ByNamespace           152m   False</span><br><span class="line">kube-system-service-accounts   workload-high     900                  ByNamespace           152m   False</span><br><span class="line">health-for-strangers           exempt            1000                 &lt;none&gt;                151m   False</span><br><span class="line">service-accounts               workload-low      9000                 ByUser                152m   False</span><br><span class="line">global-default                 global-default    9900                 ByUser                152m   False</span><br><span class="line">catch-all                      catch-all         10000                ByUser                152m   False</span><br></pre></td></tr></tbody></table></figure><p>FlowShema配置<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">flowcontrol.apiserver.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">FlowSchema</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">health-for-strangers</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  matchingPrecedence:</span> <span class="number">1000</span> <span class="comment">#匹配优先级，1~1000，越小优先级越高</span></span><br><span class="line"><span class="attr">  priorityLevelConfiguration:</span> <span class="comment">#关联的PriorityLevelConfigurations</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">exempt</span> <span class="comment">#排除rules，即不限制当前flowshema的rules</span></span><br><span class="line"><span class="attr">  rules:</span> <span class="comment">#请求规则</span></span><br><span class="line"><span class="attr">  - nonResourceRules:</span> <span class="comment">#非资源</span></span><br><span class="line"><span class="attr">    - nonResourceURLs:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"/healthz"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"/livez"</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"/readyz"</span></span><br><span class="line"><span class="attr">      verbs:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"*"</span></span><br><span class="line"><span class="attr">    subjects:</span> <span class="comment">#对应的用户或用户组</span></span><br><span class="line"><span class="attr">    - kind:</span> <span class="string">Group</span></span><br><span class="line"><span class="attr">      group:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="attr">system:unauthenticated</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>PriorityLevelConfiguration配置<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">flowcontrol.apiserver.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityLevelConfiguration</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">leader-election</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  limited:</span> <span class="comment">#限制策略</span></span><br><span class="line"><span class="attr">    assuredConcurrencyShares:</span> <span class="number">10</span> </span><br><span class="line"><span class="attr">    limitResponse:</span> <span class="comment">#如何处理被限制的请求</span></span><br><span class="line"><span class="attr">      queuing:</span> <span class="comment">#类型为Queue时，列队的设置</span></span><br><span class="line"><span class="attr">        handSize:</span> <span class="number">4</span> <span class="comment">#队列</span></span><br><span class="line"><span class="attr">        queueLengthLimit:</span> <span class="number">50</span> <span class="comment">#队列长度</span></span><br><span class="line"><span class="attr">        queues:</span> <span class="number">16</span> <span class="comment">#队列数</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">Queue</span> <span class="comment">#Queue或者Reject，Reject直接返回429，Queue将请求加入队列</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">Limited</span> <span class="comment">#类型，Limited或Exempt， Exempt即不限制</span></span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以上是k8s相关的限流策略，通过多种策略来保证集群的稳定性。</p><p>目前MaxInFlightLimit可以轻松开启，但是限制策略不精细，而APF功能较新，实现较复杂，在充分验证后，可通过APF对全集群进行限流。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> apiserver </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes中Sidecar生命周期管理</title>
      <link href="/k8s-sideccar-lifecycle/"/>
      <url>/k8s-sideccar-lifecycle/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在多个容器的Pod中，通常业务容器需要依赖sidecar。启动时sidecar需要先启动，退出时sidecar需要在业务容器退出后再退出。k8s目前对于sidecar的生命周期比较有争议，见<a href="https://github.com/kubernetes/enhancements/issues/753" target="_blank" rel="noopener">issue</a>、<a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/0753-sidecarcontainers.md" target="_blank" rel="noopener">sidecarcontainers</a>。</p><p>Kubernetes Pod 内有两种容器: 初始化容器(init container)和应用容器(app container)。</p><p>其中初始化容器的执行先于应用容器，按顺序启动，执行成功启动下一个：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> container := podContainerChanges.NextInitContainerToStart; container != <span class="literal">nil</span> {</span><br><span class="line">    <span class="comment">// Start the next init container.</span></span><br><span class="line">    <span class="keyword">if</span> err := start(<span class="string">"init container"</span>, containerStartSpec(container)); err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Successfully started the container; clear the entry in the failure</span></span><br><span class="line">    klog.V(<span class="number">4</span>).Infof(<span class="string">"Completed init container %q for pod %q"</span>, container.Name, format.Pod(pod))</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>而对于应用容器，无法保证容器ready顺序，启动代码如下:<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Step 7: start containers in podContainerChanges.ContainersToStart.</span></span><br><span class="line"><span class="keyword">for</span> _, idx := <span class="keyword">range</span> podContainerChanges.ContainersToStart {</span><br><span class="line">    <span class="comment">// start函数向docker发请求启动容器，这里没有检测函数返回而且不确定ENTRYPOINT是否成功</span></span><br><span class="line">    start(<span class="string">"container"</span>, containerStartSpec(&amp;pod.Spec.Containers[idx]))</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>在删除时，同样无法保证删除顺序，代码如下<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _, container := <span class="keyword">range</span> runningPod.Containers {</span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">(container *kubecontainer.Container)</span></span> {</span><br><span class="line">        killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, container.Name)</span><br><span class="line">        <span class="comment">// 每一个容器起goroutine执行删除</span></span><br><span class="line">        <span class="keyword">if</span> err := m.killContainer(pod, container.ID, container.Name, <span class="string">""</span>, gracePeriodOverride); err != <span class="literal">nil</span> {</span><br><span class="line">           ...</span><br><span class="line">        }</span><br><span class="line">        containerResults &lt;- killContainerResult</span><br><span class="line">    }(container)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="启动顺序"><a href="#启动顺序" class="headerlink" title="启动顺序"></a>启动顺序</h2><p>k8s原生方式，对于pod中一个容器依赖另一个容器，目前需要业务进程判断依赖服务是否启动或者sleep 10s，这种方式可以工作，但不太优雅。需要业务更改启动脚本。</p><p>那么，有没有其他的解决办法？</p><h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>在启动时，start函数调用startContainer来创建容器，主要代码如下：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *kubeGenericRuntimeManager)</span> <span class="title">startContainer</span><span class="params">(podSandboxID <span class="keyword">string</span>, podSandboxConfig *runtimeapi.PodSandboxConfig, spec *startSpec, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP <span class="keyword">string</span>, podIPs []<span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">string</span>, error)</span></span> {</span><br><span class="line">    container := spec.container</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 1: 拉镜像.</span></span><br><span class="line">    imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets, podSandboxConfig)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        ...</span><br><span class="line">     }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 2: 调用cri创建容器</span></span><br><span class="line">    <span class="comment">// For a new container, the RestartCount should be 0</span></span><br><span class="line">    containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig)</span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 3: 启动容器</span></span><br><span class="line">    err = m.runtimeService.StartContainer(containerID)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 4: 执行 post start hook.</span></span><br><span class="line">    <span class="keyword">if</span> container.Lifecycle != <span class="literal">nil</span> &amp;&amp; container.Lifecycle.PostStart != <span class="literal">nil</span> {</span><br><span class="line">        kubeContainerID := kubecontainer.ContainerID{</span><br><span class="line">            Type: m.runtimeName,</span><br><span class="line">            ID:   containerID,</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 调用Run来执行hook</span></span><br><span class="line">        msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart)</span><br><span class="line">        ...</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">""</span>, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>步骤如下：</p><ol><li>拉取镜像</li><li>创建容器</li><li>启动容器</li><li>执行hook</li></ol><p>一个Pod中容器的启动是有顺序的，排在前面容器的先启动。同时第一个容器执行完ENTRYPOINT和PostStart之后（异步执行，无法确定顺序），k8s才会创建第二个容器（这样的话就可以保证第一个容器创建多长时间后再启动第二个容器）</p><p>如果我们PostStart阶段去检测容器是否ready，那么只有在ready后才去执行下一个容器。</p><p><img src="/img/blogImg/sidecar-lifecycle.png" alt="sidecar-lifecycle"></p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>配置如下，sidecar模拟需要依赖的容器，main为业务容器<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-start</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sidecar</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">["/bin/sh",</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 3600"</span><span class="string">]</span></span><br><span class="line"><span class="attr">    lifecycle:</span></span><br><span class="line"><span class="attr">      postStart:</span></span><br><span class="line"><span class="attr">        exec:</span></span><br><span class="line"><span class="attr">          command:</span> <span class="string">["/bin/sh",</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 20"</span><span class="string">]</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">["/bin/sh",</span> <span class="string">"-c"</span><span class="string">,</span> <span class="string">"sleep 3600"</span><span class="string">]</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>得到结果如下，可以看到sidecar启动21s后才开始启动main容器，满足需求<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From                                          Message</span><br><span class="line">  ----    ------     ----  ----                                          -------</span><br><span class="line">  Normal  Scheduled  54s   default-scheduler                             Successfully assigned default/<span class="built_in">test</span>-start to tj1-staging-k8s-slave95-202008.kscn</span><br><span class="line">  Normal  Pulling    53s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Pulling image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Pulled     44s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Successfully pulled image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Created    44s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Created container sidecar</span><br><span class="line">  Normal  Started    44s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Started container sidecar</span><br><span class="line">  Normal  Pulling    23s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Pulling image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Pulled     19s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Successfully pulled image <span class="string">"busybox"</span></span><br><span class="line">  Normal  Created    18s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Created container main</span><br><span class="line">  Normal  Started    18s   kubelet, tj1-staging-k8s-slave95-202008.kscn  Started container main</span><br></pre></td></tr></tbody></table></figure><p></p><p>此方案可能存在的缺点：</p><ol><li>如果sidecar启动失败或者hook失败，其他容器会立即启动</li></ol><h2 id="退出顺序"><a href="#退出顺序" class="headerlink" title="退出顺序"></a>退出顺序</h2><p>容器启动顺序比较好解决，退出顺序则是按照相反的顺序，业务容器先退出，之后sidecar再退出。</p><p>目前，在kubelet删除pod步骤如下;</p><ol><li>遍历容器，每个容器起一个goroutine删除</li><li>删除时，先执行pre stop hook，得到gracePeriod=DeletionGracePeriodSeconds-period(stophook)</li><li>再调用cri删除接口m.runtimeService.StopContainer(containerID.ID, gracePeriod)</li></ol><p>如果在sidecar的pre stop hook检测业务容器状态，那么可以延迟退出。</p><h3 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h3><p>业务容器main退出时，创建文件；sidecar通过post-stop检测到文件后，执行退出<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-stop</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sidecar</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    command:</span> </span><br><span class="line"><span class="bullet">    -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">|</span></span><br><span class="line"><span class="string">      trap "touch /lifecycle/sidecar-terminated" 15</span></span><br><span class="line"><span class="string">      until [ -f "/lifecycle/sidecar-terminated" ];do</span></span><br><span class="line"><span class="string">        date</span></span><br><span class="line"><span class="string">        sleep 1</span></span><br><span class="line"><span class="string">      done</span></span><br><span class="line"><span class="string">      sleep 5</span></span><br><span class="line"><span class="string">      cat /lifecycle/main-terminated</span></span><br><span class="line"><span class="string">      t=$(date)</span></span><br><span class="line"><span class="string">      echo "sidecar exit at $t"</span></span><br><span class="line"><span class="string"></span><span class="attr">    lifecycle:</span></span><br><span class="line"><span class="attr">      preStop:</span></span><br><span class="line"><span class="attr">        exec:</span></span><br><span class="line"><span class="attr">          command:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">|</span></span><br><span class="line"><span class="string">            until [ -f "/lifecycle/main-terminated" ];do</span></span><br><span class="line"><span class="string">              sleep 1</span></span><br><span class="line"><span class="string">            done</span></span><br><span class="line"><span class="string">            t=$(date)</span></span><br><span class="line"><span class="string">            echo "main exit at $t" &gt; /lifecycle/main-terminated</span></span><br><span class="line"><span class="string"></span><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">lifecycle</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/lifecycle</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    command:</span> </span><br><span class="line"><span class="bullet">    -</span> <span class="string">"/bin/sh"</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">"-c"</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">|</span></span><br><span class="line"><span class="string">      trap "touch /lifecycle/main-terminated" 15</span></span><br><span class="line"><span class="string">      until [ -f "/lifecycle/main-terminated" ];do</span></span><br><span class="line"><span class="string">        date</span></span><br><span class="line"><span class="string">        sleep 1</span></span><br><span class="line"><span class="string">      done</span></span><br><span class="line"><span class="string"></span><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">lifecycle</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/lifecycle</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">lifecycle</span></span><br><span class="line"><span class="attr">    emptyDir:</span> <span class="string">{}</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>在日志中看到，main容器先结束，sidecar检测到main-terminated文件后，执行完post-stop-hook，sidecar主进程开始退出<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl  logs -f <span class="built_in">test</span>-stop main</span><br><span class="line">...</span><br><span class="line">Tue Sep  8 03:14:20 UTC 2020</span><br><span class="line">Tue Sep  8 03:14:21 UTC 2020</span><br><span class="line">Tue Sep  8 03:14:22 UTC 2020</span><br><span class="line"></span><br><span class="line">$ kubectl  logs -f <span class="built_in">test</span>-stop sidecar</span><br><span class="line">Tue Sep  8 03:14:22 UTC 2020</span><br><span class="line">Tue Sep  8 03:14:23 UTC 2020</span><br><span class="line"><span class="comment"># post stop hook 检测到main容器退出，记录日志</span></span><br><span class="line">main <span class="built_in">exit</span> at Tue Sep  8 03:14:23 UTC 2020</span><br><span class="line"><span class="comment"># sidecar主进程退出</span></span><br><span class="line">sidecar <span class="built_in">exit</span> at Tue Sep  8 03:14:29 UTC 2020</span><br></pre></td></tr></tbody></table></figure><p></p><p>通过测试，使用postStopHook可以达到sidecar延迟退出的目的，但这种方式也有一些缺点</p><ol><li>配置复杂，多个sidecar都需要配置postStop监听业务容器状态</li><li>业务容器需要有可观察性（提供特定形式的健康检测）</li><li>poststop执行异常，会等到最大优雅退出时间（默认30s）后才终止</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>目前对于sidecar生命周期的支持方案对比如下：</p><table><thead><tr><th>方案</th><th>启动顺序</th><th>退出顺序</th><th>job sidecar</th><th>是否需要用户修改代码</th><th>是否需要修改k8s代码</th><th>缺点</th><th>备注</th></tr></thead><tbody><tr><td>用户控制</td><td>支持</td><td>不支持</td><td>不支持</td><td>需要</td><td>不需要</td><td>需要用户更改启动脚本;退出支持难度大，需要同时修改业务容器与sidecar启动脚本；大部分情况不支持</td><td>启动时需要检测sidecar服务状态</td></tr><tr><td>Lifecycle Hooks</td><td>支持</td><td>支持</td><td>不支持</td><td>不需要</td><td>不需要</td><td>配置hook复杂度高;在hook执行异常情况下不能确保顺序</td><td></td></tr><tr><td>富容器</td><td>支持</td><td>部分支持</td><td>部分支持</td><td>不需要</td><td>需要（更改镜像或启动命令）</td><td>所有功能集成在一个容器中，对于外部sidecar如istio envoy等，不可控;</td><td></td></tr><tr><td>修改源码</td><td>支持</td><td>支持</td><td>支持</td><td>不需要</td><td>需要</td><td>需要满足各种情况，实现难度较大</td><td>社区有计划支持</td></tr></tbody></table><p>在k8s提供此类功能前，目前没有完善的方案。Lifecycle Hooks不需要更改用户启动代码以及k8s相关代码，相对于其他方式不失为一种解决思路。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> container </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开启shareProcessNamespace后容器异常</title>
      <link href="/cotainer-init/"/>
      <url>/cotainer-init/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>目前k8s不支持容器启动顺序，部分业务通过开启<code>shareProcessNamespace</code>监控某些进程状态。当开启共享pid后，有用户反馈某个容器主进程退出，但是容器并没有重启，执行<code>exec</code>会卡住，现象参考<a href="3">issue</a></p><h2 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h2><ol><li><p>创建deployment</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      shareProcessNamespace:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - image:</span> <span class="attr">nginx:alpine</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">nginx</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>查看进程信息<br>由于开启了<code>shareProcessNamespace</code>, <code>pause</code>变为<code>pid 1</code>, <code>nginx daemon</code>pid为<code>6</code>, ppid为<code>containerd-shim</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看容器内进程</span></span><br><span class="line">/ <span class="comment"># ps -efo "pid,ppid,comm,args"</span></span><br><span class="line">PID   PPID  COMMAND          COMMAND</span><br><span class="line">    1     0 pause            /pause</span><br><span class="line">    6     0 nginx            nginx: master process nginx -g daemon off;</span><br><span class="line">   11     6 nginx            nginx: worker process</span><br><span class="line">   12     6 nginx            nginx: worker process</span><br><span class="line">   13     6 nginx            nginx: worker process</span><br><span class="line">   14     6 nginx            nginx: worker process</span><br><span class="line">   15     0 sh               sh</span><br><span class="line">   47    15 ps               ps -efo pid,ppid,comm,args</span><br></pre></td></tr></tbody></table></figure></li><li><p>删除主进程<br>子进程被<code>pid 1</code>回收, 有时也会被<code>containerd-shim</code>回收</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># kill -9 6</span></span><br><span class="line">/ <span class="comment"># </span></span><br><span class="line">/ <span class="comment"># ps -efo "pid,ppid,comm,args"</span></span><br><span class="line">PID   PPID  COMMAND          COMMAND</span><br><span class="line">    1     0 pause            /pause</span><br><span class="line">   11     1 nginx            nginx: worker process</span><br><span class="line">   12     1 nginx            nginx: worker process</span><br><span class="line">   13     1 nginx            nginx: worker process</span><br><span class="line">   14     1 nginx            nginx: worker process</span><br><span class="line">   15     0 sh               sh</span><br><span class="line">   48    15 ps               ps -efo pid,ppid,comm,args</span><br></pre></td></tr></tbody></table></figure></li><li><p>docker hang<br>此时对此容器执行docker命令(<code>inspect, logs, exec</code>)将卡住， 同样通过<code>kubectl</code>执行会超时。</p></li></ol><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>在未开启<code>shareProcessNamespace</code>的容器中，主进程退出<code>pid 1</code>, 此pid namespace销毁，系统会<code>kill</code>其下的所有进程。开启后，<code>pid 1</code>为<code>pause</code>进程，容器主进程退出，由于共享pid namespace，其他进程没有退出变成孤儿进程。此时调用docker相关接口去操作容器，docker首先去找主进程，但主进程已经不存在了，导致异常(待确认)。</p><p>清理掉这些孤儿进程容器便会正常退出，可以<code>kill</code>掉这些进程或者<code>kill</code>pause进程，即可恢复。</p><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>有没有优雅的方式解决此种问题，如果主进程退出子进程也一起退出便符合预期，这就需要进程管理工具来实现，在宿主机中有<code>systemd</code>、<code>god</code>，容器中也有类似的工具即<code>init进程</code>(传递信息，回收子进程)，常见的有</p><ol><li><code>docker init</code>, docker自带的init进程(即<code>tini</code>)</li><li><a href="https://github.com/krallin/tini" target="_blank" rel="noopener"><code>tini</code></a>, 可回收孤儿进程/僵尸进程，<code>kill</code>进程组等</li><li><a href="https://github.com/Yelp/dumb-init" target="_blank" rel="noopener"><code>dumb-init</code></a>, 可管理进程，重写信号等</li></ol><p>经过测试，<code>tini</code>进程只能回收前台程序，对于后台程序则无能为力(例如<code>nohup</code>, <code>&amp;</code>启动的程序)，<code>dumb-init</code>在主进程退出时，会传递信号给子进程，符合预期。</p><p>开启<code>dumb-init</code>进程的<code>dockerfile</code>如下，<code>tini</code>也类似<br></p><figure class="highlight dockerfile"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> nginx:alpine</span><br><span class="line"></span><br><span class="line"><span class="comment"># tini</span></span><br><span class="line"><span class="comment"># RUN apk add --no-cache tini</span></span><br><span class="line"><span class="comment"># ENTRYPOINT ["/sbin/tini", "-s", "-g", "--"]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># dumb-init</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> wget -O /usr/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.2/dumb-init_1.2.2_amd64</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> chmod +x /usr/bin/dumb-init</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="bash"> [<span class="string">"/usr/bin/dumb-init"</span>, <span class="string">"-v"</span>, <span class="string">"--"</span>]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">"nginx"</span>, <span class="string">"-g"</span>, <span class="string">"daemon off;"</span>]</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>init方式对于此问题是一种临时的解决方案，需要docker从根本上解决此种情况。容器推荐单进程运行，但某些情况必须要运行多进程，如果不想处理处理传递回收进程等，可以通过<code>init</code>进程，无需更改代码即可实现。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://github.com/Yelp/dumb-init" target="_blank" rel="noopener">https://github.com/Yelp/dumb-init</a><br>[2] <a href="https://github.com/krallin/tini" target="_blank" rel="noopener">https://github.com/krallin/tini</a><br>[3] <a href="https://github.com/kubernetes/kubernetes/issues/92214" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/92214</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prometheus最佳实践-聚合函数</title>
      <link href="/prometheus-best-practice-operation/"/>
      <url>/prometheus-best-practice-operation/</url>
      
        <content type="html"><![CDATA[<h2 id="rate"><a href="#rate" class="headerlink" title="rate"></a>rate</h2><p>prometheus中<code>rate</code>只能用于<code>counter</code>类型，对于需要聚合的数据需要先<code>rate</code>再<code>sum</code>，而不是<code>rate(sum)</code></p><h2 id="数据准确性"><a href="#数据准确性" class="headerlink" title="数据准确性"></a>数据准确性</h2><p><code>rate/increase/delta</code>等操作对于原始值进行了外推（类似线性插件），得到的不是准确值</p><p>如<code>rate(http_requests_total[2m])</code>指两分钟内每秒平均请求量，通过<code>2m</code>内首尾两个数据外推得到差值，比120s得到；<br>同理<code>increase(http_requests_total[2m])</code>指的不是首尾两个值的增长量，而是外推后计算出<code>2m</code>内的增长量。</p><h2 id="absent"><a href="#absent" class="headerlink" title="absent"></a>absent</h2><p>通常报警中，我们需要对某个对象是不是有数据进行监控（即<code>nodata</code>监控），<code>absent</code>用来验证指标是不是有数据很有用</p><h2 id="predict-linear"><a href="#predict-linear" class="headerlink" title="predict_linear"></a>predict_linear</h2><p>线性回归预测，适合线性数据的预测，如预测etcd的未来4小时文件描述符使用量<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_linear(cluster:etcd:fd_utilization[1h], 3600 * 4)</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="quantile-over-time"><a href="#quantile-over-time" class="headerlink" title="quantile_over_time"></a>quantile_over_time</h2><p>一段时间内统计分位数<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quantile_over_time(0.9, http_requests_total[1d]) # 一天内请求量的90分位</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="bool"><a href="#bool" class="headerlink" title="bool"></a>bool</h2><p>某些情况的需要比较两个标量（通常用来报警），可以使用bool<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http_requests_total &gt; bool 100</span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
            <tag> monitor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubernetes相关开源项目</title>
      <link href="/kubernetes-opensource-project/"/>
      <url>/kubernetes-opensource-project/</url>
      
        <content type="html"><![CDATA[<p>总结下项目中可参考k8s相关开源项目，不断更新中…</p><h2 id="cncf"><a href="#cncf" class="headerlink" title="cncf"></a>cncf</h2><ul><li><a href="https://www.cncf.io/projects/" target="_blank" rel="noopener">project</a></li><li><a href="https://www.cncf.io/sandbox-projects/" target="_blank" rel="noopener">sandbox</a></li></ul><h2 id="阿里"><a href="#阿里" class="headerlink" title="阿里"></a>阿里</h2><ul><li><a href="https://github.com/openkruise/kruise" target="_blank" rel="noopener">kruise</a>: 各种自定义app，包括增强deployment/statefulset等</li><li><a href="https://github.com/AliyunContainerService/kubernetes-cronhpa-controller" target="_blank" rel="noopener">kubernetes-cronhpa-controller</a>: 定时扩缩</li><li><a href="https://github.com/AliyunContainerService/kube-eventer" target="_blank" rel="noopener">kube-eventer</a>: event收集</li><li><a href="https://github.com/AliyunContainerService/gpushare-scheduler-extender" target="_blank" rel="noopener">gpushare-scheduler-extender</a>: 共享GPU</li><li><a href="https://github.com/AliyunContainerService/log-pilot" target="_blank" rel="noopener">log-pilot</a>: docker日志收集工具</li></ul><h2 id="腾讯"><a href="#腾讯" class="headerlink" title="腾讯"></a>腾讯</h2><ul><li><a href="https://github.com/tkestack/tapp" target="_blank" rel="noopener">tapp</a>: 增强版deployment/statefulset</li><li><a href="https://github.com/tkestack/cron-hpa" target="_blank" rel="noopener">cron-hpa</a>: 定时扩缩</li><li><a href="https://github.com/tkestack/lb-controlling-framework" target="_blank" rel="noopener">lb-controlling-framework</a>: lb扩展，可自定义接口</li><li><a href="https://github.com/Tencent/tke-kms-plugin" target="_blank" rel="noopener">tke-kms-plugin</a>: 实现kms,可参考</li></ul><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul><li><a href="https://github.com/stakater" target="_blank" rel="noopener">stakater</a>: 提供多种controller, 白名单、reloader等工具</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> crd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s如何优雅升级应用</title>
      <link href="/k8s-graceful-update-app/"/>
      <url>/k8s-graceful-update-app/</url>
      
        <content type="html"><![CDATA[<p>在k8s中通常用户通过<code>ingress</code>接入流量，转发到后端实例(<code>ingress → pod</code>)，在后端应用更新过程中，<code>ingress</code>是否能做到优雅升级，本文将通过分析升级流程与实验验证，说明在k8s中如何实现优化升级。</p><h2 id="Ingress原理"><a href="#Ingress原理" class="headerlink" title="Ingress原理"></a>Ingress原理</h2><p>用户创建ingress资源后，<code>ingress-nginx</code>通过<code>service</code>获取到对应的<code>endpoint</code>，监听到<code>endpoint</code>变化后将动态更新<code>upstream</code>。</p><p><code>endpoint</code>每次变化后会通过<code>selector</code>匹配的<code>pod</code>列表中<code>ready pod</code>（不包括待删除的<code>pod</code>, 及<code>DeletionTimestamp</code>不为空）<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pod ready = 所有container ready(启动成功,&nbsp;健康检查通过) + 所有rediness gateway执行成功</span><br></pre></td></tr></tbody></table></figure><p></p><p>那么<code>endpoint</code>在什么状况下会发生变化：</p><ul><li>service变化（一般不会）</li><li>扩缩容</li><li>升级</li><li>删除pod</li></ul><p>不管是什么操作，可归结于启动、删除、退出</p><ul><li><strong>启动</strong>，只要确保<code>pod ready</code>时能服务能正常接受流量，不会影响影响服务</li><li><strong>退出</strong>, 如果是应用异常退出，不能处理已接受的流量，此种状况是应用本身行为，不在讨论范围</li><li><strong>删除</strong>, 由于k8s所有组件都采用监听机制，无法保证<code>pod</code>删除时<code>ingress-nginx</code>的后端已经更新</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 大约在2s内</span></span><br><span class="line">ingress-nginx 生效时间 = endpoint 生效时间 + upstream更新时间</span><br></pre></td></tr></tbody></table></figure><p>如果要保证pod删除时不丢流量，需要做到</p><ul><li>已接受的请求需要处理完，可监听TERM信号，处理完再退出， 可参考<a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods" target="_blank" rel="noopener">https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods</a></li><li>删除时不接受新的请求，这部分无法保证，只能保证#1</li></ul><h2 id="ingress-nginx-重试机制"><a href="#ingress-nginx-重试机制" class="headerlink" title="ingress-nginx 重试机制"></a>ingress-nginx 重试机制</h2><p>ingress-nginx默认开启了proxy_next_upstream，配置如下<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># In case of errors try the next upstream server before returning an error</span></span><br><span class="line">proxy_next_upstream error timeout;</span><br><span class="line">proxy_next_upstream_timeout 0;</span><br><span class="line">proxy_next_upstream_tries 3;</span><br></pre></td></tr></tbody></table></figure><p></p><p>如果一次请求中，<code>upstream server</code> 出错或超时将通过rr算法重试下一个server，最多尝试三次。如果后端大于三个实例，一个实例异常不会影响服务。</p><h2 id="升级策略"><a href="#升级策略" class="headerlink" title="升级策略"></a>升级策略</h2><p>对于<code>Deployment</code>有两种升级策略， <code>Recreate</code>与<code>RollingUpdate</code></p><ul><li><strong>Recreate</strong>, 先将旧版缩到0再将新版扩到期望值，不建议使用</li><li><strong>RollingUpdate</strong>，默认策略，滚动更新</li></ul><p>在滚动升级时主要依据<code>maxSurge</code>与<code>maxUnavailable</code>对新旧版本进行扩缩</p><ul><li><strong>maxSurge</strong>， 升级中最多有多少pod超过期望值</li><li><strong>maxUnavailable</strong>， 此值用来计算升级中最小可用的实例数，最大不可用的实例数表示不准确</li></ul><p>举个例子，比如10个副本的Deployment， 采用默认值<code>maxSurge</code>与<code>maxUnavaiable</code>都为25%<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 向上取整为 3 </span><br><span class="line">maxSurge = replicas * deployment.spec.strategy.rollingUpdate.maxSurge(25%)= 2.5</span><br><span class="line"> </span><br><span class="line">// 向下取整为 2 </span><br><span class="line">maxUnavailable = replicas * deployment.spec.strategy.rollingUpdate.maxUnavailable(25%)= 2.5</span><br><span class="line"> </span><br><span class="line">maxAvailable = replicas(10) + MaxSurge（3） = 13</span><br><span class="line"> </span><br><span class="line">minAvailable := *(deployment.Spec.Replicas)（10） - maxUnavailable（2）= 8</span><br></pre></td></tr></tbody></table></figure><p></p><p>在升级过程中，首先创建 newRS，然后为其设定 replicas，此时计算出 replicas 结果为 3。等到下一个 syncLoop 时，所有 rs 的 replicas 已经达到最大值 10 + 3 = 13，此时需要 scale down oldRSs 了，scale down 的数量是通过以下公式得到的：<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 13 = 10 + 3 </span><br><span class="line">allPodsCount := newRS(10) + oldRS(3)</span><br><span class="line"> </span><br><span class="line">// ??? </span><br><span class="line">newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas</span><br><span class="line"> </span><br><span class="line">// 13 - 8 - ??? </span><br><span class="line">maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount</span><br><span class="line">newRSUnavailablePodCount 此时不确定，但是值在 [0,3] 中，此时假设 newRS 的三个 pod 还处于 containerCreating 状态，则newRSUnavailablePodCount 为 3，根据以上公式计算所知 maxScaledDown 为 2。如果有个新版本pod已经ready，则maxScaledDown 为 4。</span><br></pre></td></tr></tbody></table></figure><p></p><p>特殊情况，当只有一个副本，<code>maxSurge</code>与<code>maxUnavaiable</code>都为1时，按照以上公式，先扩容1个新版pod，再缩一个旧版的，如果旧版已经删除了而新版还没有起来可能会丟流量，可以将<code>maxUnavaiable</code>设置为0可避免以上情况。</p><h2 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h2><p>滚动升级终于也是通过扩缩新旧版本来实现的，我们只需要分析扩缩容过程中会不会丢流量即可。</p><h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><p>image: nginx<br>tool:  <code>wrk -c 2 -d 120 -H "Connection:Close" http://my.nginx.svc</code></p><h3 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h3><p>1) 从1扩到10个</p><p>不丢流量，nginx启动很快不需要额外的初始化工作，正常情况需要配置健康检查</p><h3 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h3><p><strong>1) 10 → 1</strong></p><p>缩容时会有502错误<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Running 2m <span class="built_in">test</span> @ http://my.nginx.svc</span><br><span class="line">  2 threads and 2 connections</span><br><span class="line">  Thread Stats   Avg      Stdev     Max   +/- Stdev</span><br><span class="line">    Latency    11.73ms   27.02ms 229.17ms   95.14%</span><br><span class="line">    Req/Sec   162.91     45.77   232.00     74.13%</span><br><span class="line">  8969 requests <span class="keyword">in</span> 28.24s, 2.40MB <span class="built_in">read</span></span><br><span class="line">  Non-2xx or 3xx responses: 366</span><br><span class="line">Requests/sec:    317.62</span><br><span class="line">Transfer/sec:     86.93KB</span><br></pre></td></tr></tbody></table></figure><p></p><p>查看ingress日志<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2020/06/19 08:12:28 [error] 9533<span class="comment">#9533: *197916788 connect() failed (111: Connection refused) while connecting to upstream, client: 10.232.41.102, server: my.nginx.svc, request: "GET / HTTP/1.1", upstream: "http://10.126.110.3:80/", host: "my.nginx.svc"</span></span><br><span class="line">2020/06/19 08:12:33 [error] 8935<span class="comment">#8935: *197916707 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.232.41.102, server: my.nginx.svc, request: "GET / HTTP/1.1", upstream: "http://10.126.69.136:80/", host: "my.nginx.svc"</span></span><br><span class="line">2020/06/19 08:12:33 [error] 9533<span class="comment">#9533: *197916788 upstream timed out (110: Operation timed out) while connecting to upstream, client: 10.232.41.102, server: my.nginx.svc, request: "GET / HTTP/1.1", upstream: "http://10.126.69.136:80/", host: "my.nginx.svc</span></span><br><span class="line">10.232.41.102 - - [18/Jun/2020:09:14:35 +0000] <span class="string">"GET / HTTP/1.1"</span> 502 157 <span class="string">"-"</span> <span class="string">"-"</span> 38 0.001 [default-my-nginx-80] [] 10.46.12.80:80, 10.46.12.79:80, 10.46.12.80:80 0, 0, 0 0.000, 0.000, 0.000 502, 502, 502 5cfc063dbe7daf1db953a0e16891f100</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>2) 4→1</strong></p><p>会丟流量</p><p><strong>3）3→1</strong></p><p>测试多次，偶现过丢流量的情况，这与ingress重试算法有关系</p><p><strong>4） 10→1</strong>, 忽略term信号, 不丢流量<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Running 2m <span class="built_in">test</span> @ http://my.nginx.svc</span><br><span class="line">  2 threads and 2 connections</span><br><span class="line">Thread Stats   Avg      Stdev     Max   +/- Stdev</span><br><span class="line">    Latency    12.12ms   16.66ms 214.89ms   88.39%</span><br><span class="line">    Req/Sec   129.75     74.05   250.00     62.35%</span><br><span class="line">  8811 requests <span class="keyword">in</span> 34.24s, 2.35MB <span class="built_in">read</span></span><br><span class="line">Requests/sec:    257.35</span><br><span class="line">Transfer/sec:     70.41KB</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过分析及实验，在pod启动时可配置健康检查避免请求异常；同一时刻大于2个pod终止可能会丢失流量，通过监听退出信号可避免此种情况。综上，应用的优化升级需要做到以下几点：</p><ul><li>健康检测，<code>pod ready</code>时能够正常接受流量</li><li>优雅停止，保证处理完请求再退出，在这段时间内实例ip可从ingress后端摘除</li><li>滚动升级配置，若只有1个实例需设置maxsurge=0，更建议副本数设置多个</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> ingress </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ingress获取真实IP</title>
      <link href="/ingress-real-ip/"/>
      <url>/ingress-real-ip/</url>
      
        <content type="html"><![CDATA[<p>一般情况下，经过ingress的请求会携带header<code>X-Real-IP</code>，用户可根据header解析出真实访问IP。</p><p>特殊情况，用户请求可能经过多个nginx才达到ingress, 通过上述方法得到的并不是用户的真实IP。</p><blockquote><p>request -&gt; nginx -&gt; … -&gt; ingress-nginx -&gt; backend</p></blockquote><h2 id="方案1-use-forwarded-headers"><a href="#方案1-use-forwarded-headers" class="headerlink" title="方案1 use-forwarded-headers"></a>方案1 use-forwarded-headers</h2><p>nginx-ingress官方的建议是开启<a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#forwarded-for-header" target="_blank" rel="noopener">use-forwarded-headers</a>, 配置如下：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-configuration</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  compute-full-forwarded-for:</span> <span class="string">'true'</span></span><br><span class="line"><span class="attr">  use-forwarded-headers:</span> <span class="string">'true'</span></span><br></pre></td></tr></tbody></table></figure><h2 id="方案2-real-ip-header"><a href="#方案2-real-ip-header" class="headerlink" title="方案2 real_ip_header"></a>方案2 real_ip_header</h2><p>这种方式确实可以起作用，但是有用户反馈开启后访问ingres后端服务一直报<code>308</code>，检查了ingress的代码开启<code>use-forwarded-headers</code>后会同时开启<code>ssl-redirect</code>导致308。</p><p>那么我们只需要开启nginx配置中的相关real-ip的配置，如下在<code>http-snippet</code>添加<code>real_ip_header X-Forwarded-For;</code></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-configuration</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  http-snippet:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    real_ip_header X-Forwarded-For;</span></span><br></pre></td></tr></tbody></table></figure><h2 id="golang中获取真实ip"><a href="#golang中获取真实ip" class="headerlink" title="golang中获取真实ip"></a>golang中获取真实ip</h2><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RemoteIP</span><span class="params">(r *http.Request)</span> <span class="title">string</span></span> {</span><br><span class="line">  <span class="comment">// ingress 行为，将真实ip放到header `X-Original-Forwarded-For`, 普通nginx可去掉此条</span></span><br><span class="line">ip := strings.TrimSpace(strings.Split(r.Header.Get(<span class="string">"X-Original-Forwarded-For"</span>), <span class="string">","</span>)[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span> ip != <span class="string">""</span> {</span><br><span class="line"><span class="keyword">return</span> ip</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">ip = strings.TrimSpace(strings.Split(r.Header.Get(<span class="string">"X-Forwarded-For"</span>), <span class="string">","</span>)[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">if</span> ip != <span class="string">""</span> {</span><br><span class="line"><span class="keyword">return</span> ip</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">ip = strings.TrimSpace(r.Header.Get(<span class="string">"X-Real-Ip"</span>))</span><br><span class="line"><span class="keyword">if</span> ip != <span class="string">""</span> {</span><br><span class="line"><span class="keyword">return</span> ip</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> ip, _, err := net.SplitHostPort(strings.TrimSpace(r.RemoteAddr)); err == <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> ip</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>nginx-ingress configmap中的配置会是全局生效的，上线前需要严格测试。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> ingress </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ingress Header Too Large</title>
      <link href="/ingress-header-too-large/"/>
      <url>/ingress-header-too-large/</url>
      
        <content type="html"><![CDATA[<p>线上遇到多次由ingress header过大引起的请求失败, 可能返回502/400，解决方案如下。</p><h2 id="502-–-too-big-header"><a href="#502-–-too-big-header" class="headerlink" title="502 – too big header"></a>502 – too big header</h2><p>502错误一般是后端服务不可用，但这里是nginx-ingress返回的，在nginx-ingress可看到如下日志：<br><code>upstream sent too big header while reading response header from upstream, client...</code></p><p>需要在ingress配置如下参数<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-buffer-size:</span> <span class="number">128</span><span class="string">k</span> <span class="comment">#根据实际情况配置</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-buffering:</span> <span class="string">"on"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/server-snippet:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      large_client_header_buffers 16 128K;</span></span><br><span class="line"><span class="string">      client_header_buffer_size 128k;</span></span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="431-400-–-too-big-header"><a href="#431-400-–-too-big-header" class="headerlink" title="431/400 – too big header"></a>431/400 – too big header</h2><p>http header过大也有可能返回400/431, 可按照上述调整，如果还是有问题需要检查后端服务的header设置，比如golang http header默认是<code>1M</code>;<br>springboot应用需要在<code>application.properties</code>加上<code>server.max-http-header-size=32KB</code>等</p><h2 id="413-–-too-large-body"><a href="#413-–-too-large-body" class="headerlink" title="413 – too large body"></a>413 – too large body</h2><p>如果返回413，则超过了body size的限制（默认<code>1M</code>）, 可在ingress annotation添加<br><code>nginx.ingress.kubernetes.io/proxy-body-size: 8m</code></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> ingress </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ingress nginx benchmark</title>
      <link href="/ingress-benchmark/"/>
      <url>/ingress-benchmark/</url>
      
        <content type="html"><![CDATA[<p>Ingress是目前Kubernetes集群流量接入的重要入口，了解其性能指标有助于用户选用合适的网络方案。</p><h2 id="测试方案"><a href="#测试方案" class="headerlink" title="测试方案"></a>测试方案</h2><p>通过wrk压测后端nginx服务，对比ingress-nginx, 原生nginx，以及直连后端性能的差异，如下图:<br><img src="/img/blogImg/ingress-benchmark1.png" alt=""></p><ul><li>方案1，经过ingress</li><li>方案2，经过nginx</li><li>方案3，直连ip</li></ul><h3 id="硬件环境"><a href="#硬件环境" class="headerlink" title="硬件环境"></a>硬件环境</h3><ul><li>CPU： 2x  Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, 32 cores</li><li>Network： 10-Gigabit</li><li>Memory： 128 GB</li></ul><h3 id="测试工具"><a href="#测试工具" class="headerlink" title="测试工具"></a>测试工具</h3><ul><li>wrk, 4.1.0, 在k8s master测试，减少网络影响</li><li>ingress-nginx, 0.30.0, <a href="https://github.com/kubernetes/ingress-nginx" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx</a></li><li>nginx, 1.13.5 </li><li>k8s, v1.14.9 </li><li>centos, 7.3.1611(Linux 4.9.2)</li></ul><h3 id="测试方法"><a href="#测试方法" class="headerlink" title="测试方法"></a>测试方法</h3><p>ingress-nginx主要工作是转发请求到后端pod, 我们着重对其RPS（每秒请求量）进行测试</p><p>通过以下命令<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">wrk</span> <span class="bullet">-t4</span> <span class="bullet">-c1000</span> <span class="bullet">-d120s</span> <span class="bullet">--latency</span> <span class="attr">http://my.nginx.svc/1kb.bin</span></span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><h3 id="不同cpu下的性能"><a href="#不同cpu下的性能" class="headerlink" title="不同cpu下的性能"></a>不同cpu下的性能</h3><p>对比不同ingress-nginx启动不同worker数量的性能差异，以下测试ingress-nginx开启了keepalive等特性</p><table><thead><tr><th>CPU</th><th>RPS</th></tr></thead><tbody><tr><td>1</td><td>5534</td></tr><tr><td>2</td><td>11203</td></tr><tr><td>4</td><td>22890</td></tr><tr><td>8</td><td>47025</td></tr><tr><td>16</td><td>93644</td></tr><tr><td>24</td><td>125990</td></tr><tr><td>32</td><td>153473</td></tr></tbody></table><p><img src="/img/blogImg/ingress-benchmark2.png" alt=""></p><p>如图所示，不同cpu下，ingress的rps与cpu成正比，cpu在16核之后增长趋势放缓。</p><h3 id="不同方案的性能对比"><a href="#不同方案的性能对比" class="headerlink" title="不同方案的性能对比"></a>不同方案的性能对比</h3><table><thead><tr><th>方案</th><th>RPS</th><th>备注</th></tr></thead><tbody><tr><td>ingress-nginx(原始)</td><td>69171</td><td></td></tr><tr><td>ingress-nginx(配置优化)</td><td>153473</td><td>调整worker，access-log, keepalive等</td></tr><tr><td>nginx</td><td>336769</td><td>开启keepalive, 关闭log</td></tr><tr><td>直连ip</td><td>340748</td><td>测试中的pod ip为真实ip</td></tr></tbody></table><p>通过实验可以看到，使用nginx代理和直连ip，rps相差不大；原始ingress-nginx rps很低，优化后rps提升一倍，但对比nginx还是有较大的性能差异。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>默认ingress-nginx性能较差，配置优化后也只有15w RPS，对比原生nginx（33W) 差距较大。经过分析主要瓶颈在于ingress-nginx的lua过滤脚本，具体原因需要进一步分析。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#upstream-keepalive-connections" target="_blank" rel="noopener">https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#upstream-keepalive-connections</a></li><li><a href="https://www.nginx.com/blog/testing-performance-nginx-ingress-controller-kubernetes/" target="_blank" rel="noopener">https://www.nginx.com/blog/testing-performance-nginx-ingress-controller-kubernetes/</a></li></ol><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>本测试所有配置见<a href="https://github.com/qingwave/ingress-nginx-benchmark" target="_blank" rel="noopener">qingwave/ingress-nginx-benchmark</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> ingress </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可能是史上最全的Kubernetes证书解析</title>
      <link href="/k8s-tls/"/>
      <url>/k8s-tls/</url>
      
        <content type="html"><![CDATA[<p>为了避免广告法，题目还是加个可能吧。</p><p>想要安全就必须复杂起来，证书是少不了的。在Kubernetes中提供了非常丰富的证书类型，满足各种不同场景的需求，今天我们就来看一看Kubernetes中的证书。</p><h2 id="k8s证书分类"><a href="#k8s证书分类" class="headerlink" title="k8s证书分类"></a>k8s证书分类</h2><p>在说证书之前，先想想作为集群的入口apiserver需要提供那些服务，与那些组件通信，通信的两方可能需要配置证书。<br>与apiserver通信的组件大体可以分为以下几类：</p><ul><li>client(kubectl，restapi等)：普通用户与apiserver之间的通信，对各类资源进行操作</li><li>kubelet，kubeproxy：master与node之间的通信</li><li>etcd：k8s的存储库</li><li>webhook：这里指apiserver提供的admission-webhook，在数据持久化前调用webhook</li><li>aggregation layer：扩展apiserver, 需要将自定义的api注册到k8s中，相比CRD性能更新</li><li>pod: 在pod中调用apiserver(一般调用为10.254.0.1:433)</li></ul><p>居然有这么多种，除了在pod中通过serviceacount认证（当然pod需要认证apiserver的证书），其他几种都需要配置证书。</p><p>其他集群内组件与apiserver通信的，kubelet/etcd/kube-proxy对应的也可以配置证书。</p><h2 id="apiserver证书"><a href="#apiserver证书" class="headerlink" title="apiserver证书"></a>apiserver证书</h2><p>简单列举下apiserver证书相关的启动参数<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">--cert-dir string                           The directory where the TLS certs are located. If --tls-cert-file and --tls-private-key-file are provided, this flag will be ignored. (default "/var/run/kubernetes")</span><br><span class="line">--client-ca-file string                     If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</span><br><span class="line">--etcd-certfile string                      SSL certification file used to secure etcd communication.</span><br><span class="line">--etcd-keyfile string                       SSL key file used to secure etcd communication.</span><br><span class="line">--kubelet-certificate-authority string      Path to a cert file for the certificate authority.</span><br><span class="line">--kubelet-client-certificate string         Path to a client cert file for TLS.</span><br><span class="line">--kubelet-client-key string                 Path to a client key file for TLS.</span><br><span class="line">--proxy-client-cert-file string             Client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. It is expected that this cert includes a signature from the CA in the --requestheader-client-ca-file flag. That CA is published in the 'extension-apiserver-authentication' configmap in the kube-system namespace. Components recieving calls from kube-aggregator should use that CA to perform their half of the mutual TLS verification.</span><br><span class="line">--proxy-client-key-file string              Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins.</span><br><span class="line">--requestheader-allowed-names stringSlice   List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed.</span><br><span class="line">--requestheader-client-ca-file string       Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers</span><br><span class="line">--service-account-key-file stringArray      File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. If unspecified, --tls-private-key-file is used. The specified file can contain multiple keys, and the flag can be specified multiple times with different files.</span><br><span class="line">--ssh-keyfile string                        If non-empty, use secure SSH proxy to the nodes, using this user keyfile</span><br><span class="line">--tls-ca-file string                        If set, this certificate authority will used for secure access from Admission Controllers. This must be a valid PEM-encoded CA bundle. Alternatively, the certificate authority can be appended to the certificate provided by --tls-cert-file.</span><br><span class="line">--tls-cert-file string                      File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to /var/run/kubernetes.</span><br><span class="line">--tls-private-key-file string               File containing the default x509 private key matching --tls-cert-file.</span><br><span class="line">--tls-sni-cert-key namedCertKey             A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: "example.crt,example.key" or "foo.crt,foo.key:*.foo.com,foo.com". (default [])</span><br><span class="line">--oidc-ca-file string                       If set, the OpenID server's certificate will be verified by one of the authorities in the oidc-ca-file, otherwise the host's root CA set will be used.</span><br><span class="line">--tls-sni-cert-key namedCertKey             A pair of x509 certificate and private key file paths, optionally suffixed with a list of domain patterns which are fully qualified domain names, possibly with prefixed wildcard segments. If no domain patterns are provided, the names of the certificate are extracted. Non-wildcard matches trump over wildcard matches, explicit domain patterns trump over extracted names. For multiple key/certificate pairs, use the --tls-sni-cert-key multiple times. Examples: "example.crt,example.key" or "foo.crt,foo.key:*.foo.com,foo.com". (default [])</span><br></pre></td></tr></tbody></table></figure><p></p><p>不要害怕，咱们一个个看。</p><h3 id="tls证书"><a href="#tls证书" class="headerlink" title="tls证书"></a>tls证书</h3><p>首先，apiserver本身是一个http服务器，需要tls证书<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--tls-cert-file string</span><br><span class="line">    File containing the default x509 Certificate for HTTPS. (CA cert, if any, concatenated after server cert). If HTTPS serving is enabled, and --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory specified by --cert-dir.</span><br><span class="line"></span><br><span class="line">--tls-private-key-file string</span><br><span class="line">    File containing the default x509 private key matching --tls-cert-file.</span><br><span class="line">其他client验证apiserver时可以通过签署这两个证书的CA，我们称为`tls-ca`</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="client证书"><a href="#client证书" class="headerlink" title="client证书"></a>client证书</h3><p>apiserver提供了tls证书，同样也需要验证client的配置，但是client太多了(kubectl,各种restapi调用的), 这些client需要统一用一个CA签发，我们称为<code>client-ca</code>。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--client-ca-file string</span><br><span class="line">    If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</span><br></pre></td></tr></tbody></table></figure><p></p><p>需要注意的是，在apiserver认证中，通过<code>CN</code>和<code>O</code>来识别用户，开启RBAC的用户要配置<code>CN</code>和<code>O</code>做一些授权：</p><ul><li>CN：Common Name，kube-apiserver 从证书中提取作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；</li><li>O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)</li></ul><p>如kube-proxy的证书申请, User为<code>system:kube-proxy</code>, Group为<code>k8s</code><br></p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="attr">"CN"</span>: <span class="string">"system:kube-proxy"</span>,</span><br><span class="line">  <span class="attr">"hosts"</span>: [],</span><br><span class="line">  <span class="attr">"key"</span>: {</span><br><span class="line">    <span class="attr">"algo"</span>: <span class="string">"rsa"</span>,</span><br><span class="line">    <span class="attr">"size"</span>: <span class="number">2048</span></span><br><span class="line">  },</span><br><span class="line">  <span class="attr">"names"</span>: [</span><br><span class="line">    {</span><br><span class="line">      <span class="attr">"C"</span>: <span class="string">"CN"</span>,</span><br><span class="line">      <span class="attr">"ST"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="attr">"L"</span>: <span class="string">"BeiJing"</span>,</span><br><span class="line">      <span class="attr">"O"</span>: <span class="string">"k8s"</span>,</span><br><span class="line">      <span class="attr">"OU"</span>: <span class="string">"System"</span></span><br><span class="line">    }</span><br><span class="line">  ]</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="requestheader证书"><a href="#requestheader证书" class="headerlink" title="requestheader证书"></a>requestheader证书</h3><p>apiserver可以使用HTTP请求头中的指定字段来进行认证，相关配置如下:<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--requestheader-allowed-names stringSlice</span><br><span class="line">    List of client certificate common names to allow to provide usernames in headers specified by --requestheader-username-headers. If empty, any client certificate validated by the authorities in --requestheader-client-ca-file is allowed.</span><br><span class="line">--requestheader-client-ca-file string</span><br><span class="line">    Root certificate bundle to use to verify client certificates on incoming requests before trusting usernames in headers specified by --requestheader-username-headers. WARNING: generally do not depend on authorization being already done for incoming requests.</span><br><span class="line">--requestheader-extra-headers-prefix strings        </span><br><span class="line">    List of request header prefixes to inspect. X-Remote-Extra- is suggested.</span><br><span class="line">--requestheader-group-headers strings               </span><br><span class="line">    List of request headers to inspect for groups. X-Remote-Group is suggested.</span><br><span class="line">--requestheader-username-headers strings            </span><br><span class="line">    List of request headers to inspect for usernames. X-Remote-User is common.</span><br></pre></td></tr></tbody></table></figure><p></p><p>收到请求时，apiserver会首先认证<code>requsetheader-ca</code>，验证成功并且<code>CN</code>在<code>requestheader-allowed-names</code>（默认全部需求）中，然后通过Http header中的<code>X-Remote-User, X-Remote-Group</code>去得到用户；如果匹配不成功回去验证<code>client-ca</code>。</p><p>如上，<code>requestheader</code>证书与<code>client-ca</code>不能是同一个。</p><h3 id="proxy证书"><a href="#proxy证书" class="headerlink" title="proxy证书"></a>proxy证书</h3><p>k8s提供了丰富的扩展机制，CRD与[API Aggregation][<a href="https://kubernetes.io/zh/docs/tasks/access-kubernetes-api/configure-aggregation-layer/]。" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tasks/access-kubernetes-api/configure-aggregation-layer/]。</a><br>对于API Aggregation(例如metrics-server提供了metrics.k8s.io api), apiserver接受到请求后经过一系列验证过滤，会将请求转发到扩展API，这里apisever作为代理服务器，需要配置配置证书。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--proxy-client-cert-file string             </span><br><span class="line">    Client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins. It is expected that this cert includes a signature from the CA in the --requestheader-client-ca-file flag. That CA is published in the 'extension-apiserver-authentication' configmap in the kube-system namespace. Components recieving calls from kube-aggregator should use that CA to perform their half of the mutual TLS verification.</span><br><span class="line">--proxy-client-key-file string              </span><br><span class="line">    Private key for the client certificate used to prove the identity of the aggregator or kube-apiserver when it must call out during a request. This includes proxying requests to a user api-server and calling out to webhook admission plugins.</span><br></pre></td></tr></tbody></table></figure><p></p><p>需要注意的是对证书需要通过<code>requestheader-ca</code>签发，扩展api会通过requestheader证书去验证，具体流程后面会写一篇，下图为官方提供的流程<br><img src="https://d33wubrfki0l68.cloudfront.net/3c5428678a95c3715894011d8dd4812d2cf229b9/e745c/images/docs/aggregation-api-auth-flow.png" alt="aggregation-api"></p><h3 id="kubelet证书"><a href="#kubelet证书" class="headerlink" title="kubelet证书"></a>kubelet证书</h3><p>对于kubelet，apiserver单独提供了证书配置选项，同时kubelet组件也提供了反向设置的相关选项:<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># API Server</span><br><span class="line">--kubelet-certificate-authority string</span><br><span class="line">    Path to a cert file for the certificate authority.</span><br><span class="line">--kubelet-client-certificate string</span><br><span class="line">    Path to a client cert file for TLS.</span><br><span class="line">--kubelet-client-key string</span><br><span class="line">    Path to a client key file for TLS.</span><br><span class="line"></span><br><span class="line"># kubelet</span><br><span class="line">--client-ca-file string</span><br><span class="line">    If set, any request presenting a client certificate signed by one of the authorities in the client-ca-file is authenticated with an identity corresponding to the CommonName of the client certificate.</span><br><span class="line">--tls-cert-file string </span><br><span class="line">    File containing x509 Certificate used for serving HTTPS (with intermediate certs, if any, concatenated after server cert). If --tls-cert-file and --tls-private-key-file are not provided, a self-signed certificate and key are generated for the public address and saved to the directory passed to --cert-dir.</span><br><span class="line">--tls-private-key-file string</span><br><span class="line">    File containing x509 private key matching --tls-cert-file.</span><br></pre></td></tr></tbody></table></figure><p></p><p>kubelet也是即作为server也作为client, 需要提供tls证书和client-ca, 我们称这个CA为<code>kubelet-ca</code>, 可以是单独的CA。</p><h3 id="etcd证书"><a href="#etcd证书" class="headerlink" title="etcd证书"></a>etcd证书</h3><p>这个也不用多说，用来连接etcd，由<code>etcd-ca</code>签发<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--etcd-certfile string                      SSL certification file used to secure etcd communication.</span><br><span class="line">--etcd-keyfile string                       SSL key file used to secure etcd communication.</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="serviceaccount证书"><a href="#serviceaccount证书" class="headerlink" title="serviceaccount证书"></a>serviceaccount证书</h3><p>在k8s中，通过<code>JWT</code>认证<code>serviecaccount</code>，同样有两个证书配置:<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># apiserver</span><br><span class="line">--service-account-key-file stringArray # 用于验证sa</span><br><span class="line">    File containing PEM-encoded x509 RSA or ECDSA private or public keys, used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If unspecified, --tls-private-key-file is used. Must be specified when --service-account-signing-key is provided</span><br><span class="line">--service-account-signing-key-file string</span><br><span class="line">    Path to the file that contains the current private key of the service account token issuer. The issuer will sign issued ID tokens with this private key. (Requires the 'TokenRequest' feature gate.)</span><br><span class="line"></span><br><span class="line"># controller-manager</span><br><span class="line">–service-account-private-key-file #用于签署sa</span><br></pre></td></tr></tbody></table></figure><p></p><p>这两个配置描述了对<code>serviceaccount</code>进行签名验证时所使用的证书；可以是单独的生成，我们称为<code>sa-key</code>。</p><h2 id="其他证书"><a href="#其他证书" class="headerlink" title="其他证书"></a>其他证书</h2><p>其他还有<code>oidc</code>证书，用于OpenID认证；<code>ssh</code>证书，用来连接node，目前以及废弃。</p><p>etcd与kubelet证书上面已经提过了，需要双方都配置。</p><p>k8s中也支持证书申请，用户可以创建<code>CertificateSigningRequest</code>来申请证书，需要在controller-manager配置下面的证书，用于签发证书称为<code>sing-ca</code>，多用于webhook的证书配置。<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--cluster-signing-cert-file string          Filename containing a PEM-encoded X509 CA certificate used to issue cluster-scoped certificates (default "/etc/kubernetes/ca/ca.pem")</span><br><span class="line">--cluster-signing-key-file string           Filename containing a PEM-encoded RSA or ECDSA private key used to sign cluster-scoped certificates (default "/etc/kubernetes/ca/ca.key")</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>k8s提供了强大的功能，需要考虑到各个场景的安全问题，上面我们梳理了遍目前常用的证书</p><ul><li>tls-ca</li><li>client-ca</li><li>requestheader-ca</li><li>proxy-ca</li><li>kubelet-ca</li><li>etcd-ca</li><li>sa-key</li><li>sign-ca</li></ul><p>上面除了<code>proxy-ca</code>必须使用<code>requestheader-ca</code>签发，其他所有的都可以是单独的CA，可以根据安全性评估是使用一个CA还是多个CA，我们建议下面的CA尽量是独立的</p><ul><li>client-ca</li><li>requestheader-ca</li><li>etcd-ca</li><li>kubelet-ca</li><li>sign-ca</li></ul><p>终于理完了，可以起床啦。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> apiserver </tag>
            
            <tag> tls </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubernetes扩展apiserver实现分析</title>
      <link href="/kube-apiserver-aggretation-api/"/>
      <url>/kube-apiserver-aggretation-api/</url>
      
        <content type="html"><![CDATA[<p>Kubernetes提供了丰富的扩展功能，实现自定义资源有两种方式<code>CRD</code>与<code>Aggregation API</code>。相对于<code>CRD</code>，扩展API功能更丰富，可以实现单独的存储。今天来聊一聊，k8s是如是实现扩展api的，它与apiserver之间又是如何协作的</p><h2 id="AggregationApiserver介绍"><a href="#AggregationApiserver介绍" class="headerlink" title="AggregationApiserver介绍"></a>AggregationApiserver介绍</h2><p><code>Aggregator</code>类似于一个七层负载均衡，将来自用户的请求拦截转发给其他服务器，并且负责整个 APIServer 的 Discovery 功能。</p><p>通过<code>APIServices</code>对象关联到某个<code>Service</code>来进行请求的转发，其关联的<code>Service</code>类型进一步决定了请求转发形式。<code>Aggregator</code>包括一个<code>GenericAPIServer</code>和维护自身状态的<code>Controller</code>。其中 <code>GenericAPIServer</code>主要处理<code>apiregistration.k8s.io</code>组下的<code>APIService</code>资源请求。</p><p>主要controller包括：</p><ol><li>apiserviceRegistrationController：负责<code>APIServices</code>中资源的注册与删除；</li><li>availableConditionController：维护<code>APIServices</code>的可用状态，包括其引用<code>Service</code>是否可用等；</li><li>autoRegistrationController：用于保持API中存在的一组特定的<code>APIServices</code>；</li><li>crdRegistrationController：负责将<code>CRD GroupVersions</code>自动注册到<code>APIServices</code>中；</li><li>openAPIAggregationController：将<code>APIServices</code>资源的变化同步至提供的<code>OpenAPI</code>文档；</li></ol><p>在 kube-apiserver 中需要增加以下配置来开启 API Aggregation：<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt</span><br><span class="line">--proxy-client-key-file=/etc/kubernetes/certs/proxy.key</span><br><span class="line">--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt</span><br><span class="line">--requestheader-extra-headers-prefix=X-Remote-Extra-</span><br><span class="line">--requestheader-group-headers=X-Remote-Group</span><br><span class="line">--requestheader-username-headers=X-Remote-User</span><br></pre></td></tr></tbody></table></figure><p></p><p>如果 kube-proxy 没有和 API server 运行在同一台主机上，那么需要确保启用了如下 apiserver 标记：<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--enable-aggregator-routing=true</span><br></pre></td></tr></tbody></table></figure><p></p><p>在<a href="./kube-apiserver-start.md">apiserver启动流程</a>中，分析了<code>AggregationApiserver</code>的初始化流程, 需要了解的可以回去看下。</p><h2 id="AggregationApiserver认证流程"><a href="#AggregationApiserver认证流程" class="headerlink" title="AggregationApiserver认证流程"></a>AggregationApiserver认证流程</h2><p>与自定义资源定义（CRD）不同，除标准的 Kubernetes apiserver 外，Aggregation API 还涉及另一个服务器：扩展 apiserver。Kubernetes apiserver 将需要与您的扩展 apiserver 通信，并且您的扩展 apiserver 也需要与 Kubernetes apiserver 通信。为了确保此通信的安全，Kubernetes apiserver 使用 x509 证书向扩展 apiserver 认证。</p><p>AggregationApi的请求链路如下：<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaultHandlerChain-&gt;aggregator-&gt;aggregation-apiserver-&gt;aggregator-&gt;user</span><br></pre></td></tr></tbody></table></figure><p></p><p>大致流程如下：</p><ol><li>Kubernetes apiserver：对发出请求的用户身份认证，并对请求的 API 路径执行鉴权。</li><li>Kubernetes apiserver：将请求转发到扩展 apiserver</li><li>扩展 apiserver：认证来自 Kubernetes apiserver 的请求</li><li>扩展 apiserver：对来自原始用户的请求鉴权</li><li>扩展 apiserver：执行对应操作返回</li></ol><p>如图所示：<br><a href="https://d33wubrfki0l68.cloudfront.net/3c5428678a95c3715894011d8dd4812d2cf229b9/e745c/images/docs/aggregation-api-auth-flow.png" target="_blank" rel="noopener">aggregation-apiserver-auth</a></p><p>apiserver与扩展apiserver通过证书认证,</p><ul><li>apiserver配置<code>porxy-client</code>证书(使用requestheader根证书签发)，扩展apiserver配置<code>reqeustheader</code>根证书，如果没配置，会默认从configmap <code>kube-system/extension-apiserver-authentication</code> 去找</li><li>扩展apiserver通过<code>extension-apiserver-authentication</code>获取apiserver的<code>client-ca</code>，生成证书对，apiserver可以使用<code>client-ca</code>验证它</li><li>由于apiserver-&gt;扩展apiserver通过<code>reqeustheader</code>方式认证，apiserver会将接受到的请求经过认证，转换为header，扩展apiserver通过header获取用户，再通过apiserver接口做权限校验。</li></ul><p>有同学有疑问，为什么这里需要做两次认证，两次鉴权。这是由于扩展apiserveer是一个单独的服务器，如果接受非apiserver的请求也是需要做认证鉴权的。那能不能认证是apiserver后就不做鉴权了呢，这得需要apiserver在转发请求时加入鉴权信息就行。</p><h2 id="AggregationApiserver处理流程"><a href="#AggregationApiserver处理流程" class="headerlink" title="AggregationApiserver处理流程"></a>AggregationApiserver处理流程</h2><h3 id="apiserver处理逻辑"><a href="#apiserver处理逻辑" class="headerlink" title="apiserver处理逻辑"></a>apiserver处理逻辑</h3><p>在apiserver认证时，认证接受会将认证信息删除, 可参考前面的[apiserver认证源码分析]</p><p>处理逻辑如下：</p><ol><li>通过<code>context</code>获取user信息</li><li>构造请求，删除reqeustheader信息，通过user重新填充</li><li>通过<code>proxyRoundTripper</code>转发请求</li></ol><p>(kube-apiserver-authentication-code.md)<br>aggregation的<a href="https://github.com/kubernetes/kubernetes/blob/df9b4e92e84849e2b9fdb5b4849c9c4ebfae8040/staging/src/k8s.io/kube-aggregator/pkg/apiserver/handler_proxy.go#L109" target="_blank" rel="noopener">hander</a>的实现：<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 通过context获取user</span></span><br><span class="line">user, ok := genericapirequest.UserFrom(req.Context())</span><br><span class="line"><span class="keyword">if</span> !ok {</span><br><span class="line">proxyError(w, req, <span class="string">"missing user"</span>, http.StatusInternalServerError)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">  }</span><br><span class="line">  <span class="comment">// 构造请求url,通过apiservice配置的service/namespace随机得到某个endpoint后端</span></span><br><span class="line">  location := &amp;url.URL{}</span><br><span class="line">location.Scheme = <span class="string">"https"</span></span><br><span class="line">rloc, err := r.serviceResolver.ResolveEndpoint(handlingInfo.serviceNamespace, handlingInfo.serviceName, handlingInfo.servicePort)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">klog.Errorf(<span class="string">"error resolving %s/%s: %v"</span>, handlingInfo.serviceNamespace, handlingInfo.serviceName, err)</span><br><span class="line">proxyError(w, req, <span class="string">"service unavailable"</span>, http.StatusServiceUnavailable)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">location.Host = rloc.Host</span><br><span class="line">location.Path = req.URL.Path</span><br><span class="line">  location.RawQuery = req.URL.Query().Encode()</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// we need to wrap the roundtripper in another roundtripper which will apply the front proxy headers</span></span><br><span class="line">  <span class="comment">// 包裹请求信息，将user信息放到header中</span></span><br><span class="line">proxyRoundTripper, upgrade, err := maybeWrapForConnectionUpgrades(handlingInfo.restConfig, handlingInfo.proxyRoundTripper, req)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">proxyError(w, req, err.Error(), http.StatusInternalServerError)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">  proxyRoundTripper = transport.NewAuthProxyRoundTripper(user.GetName(), user.GetGroups(), user.GetExtra(), proxyRoundTripper)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 调用后端</span></span><br><span class="line">  handler := proxy.NewUpgradeAwareHandler(location, proxyRoundTripper, <span class="literal">true</span>, upgrade, &amp;responder{w: w})</span><br><span class="line">handler.ServeHTTP(w, newReq)</span><br></pre></td></tr></tbody></table></figure><p></p><p>根据扩展apiserver找到后端时通过service获取对应endpoint列表，随机选择某个endpoint、<br>实现如下：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ResourceLocation returns a URL to which one can send traffic for the specified service.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ResolveEndpoint</span><span class="params">(services listersv1.ServiceLister, endpoints listersv1.EndpointsLister, namespace, id <span class="keyword">string</span>, port <span class="keyword">int32</span>)</span> <span class="params">(*url.URL, error)</span></span> {</span><br><span class="line">svc, err := services.Services(namespace).Get(id)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">svcPort, err := findServicePort(svc, port)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> {</span><br><span class="line"><span class="keyword">case</span> svc.Spec.Type == v1.ServiceTypeClusterIP, svc.Spec.Type == v1.ServiceTypeLoadBalancer, svc.Spec.Type == v1.ServiceTypeNodePort:</span><br><span class="line"><span class="comment">// these are fine</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"unsupported service type %q"</span>, svc.Spec.Type)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">eps, err := endpoints.Endpoints(namespace).Get(svc.Name)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(eps.Subsets) == <span class="number">0</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, errors.NewServiceUnavailable(fmt.Sprintf(<span class="string">"no endpoints available for service %q"</span>, svc.Name))</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Pick a random Subset to start searching from.</span></span><br><span class="line">ssSeed := rand.Intn(<span class="built_in">len</span>(eps.Subsets))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Find a Subset that has the port.</span></span><br><span class="line"><span class="keyword">for</span> ssi := <span class="number">0</span>; ssi &lt; <span class="built_in">len</span>(eps.Subsets); ssi++ {</span><br><span class="line">ss := &amp;eps.Subsets[(ssSeed+ssi)%<span class="built_in">len</span>(eps.Subsets)]</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(ss.Addresses) == <span class="number">0</span> {</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> ss.Ports {</span><br><span class="line"><span class="keyword">if</span> ss.Ports[i].Name == svcPort.Name {</span><br><span class="line"><span class="comment">// Pick a random address.</span></span><br><span class="line"><span class="comment">// 核心，随机选择endpoint</span></span><br><span class="line">ip := ss.Addresses[rand.Intn(<span class="built_in">len</span>(ss.Addresses))].IP</span><br><span class="line">port := <span class="keyword">int</span>(ss.Ports[i].Port)</span><br><span class="line"><span class="keyword">return</span> &amp;url.URL{</span><br><span class="line">Scheme: <span class="string">"https"</span>,</span><br><span class="line">Host:   net.JoinHostPort(ip, strconv.Itoa(port)),</span><br><span class="line">}, <span class="literal">nil</span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, errors.NewServiceUnavailable(fmt.Sprintf(<span class="string">"no endpoints available for service %q"</span>, id))</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>ProxyRoundTripper创建在<a href="https://github.com/kubernetes/kubernetes/blob/a42e029e6905bee5b9d5489610c4fbe5988eeac6/staging/src/k8s.io/client-go/transport/round_trippers.go#L101" target="_blank" rel="noopener">round_trippers.go</a><br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewAuthProxyRoundTripper</span><span class="params">(username <span class="keyword">string</span>, groups []<span class="keyword">string</span>, extra <span class="keyword">map</span>[<span class="keyword">string</span>][]<span class="keyword">string</span>, rt http.RoundTripper)</span> <span class="title">http</span>.<span class="title">RoundTripper</span></span> {</span><br><span class="line"><span class="keyword">return</span> &amp;authProxyRoundTripper{</span><br><span class="line">username: username,</span><br><span class="line">groups:   groups,</span><br><span class="line">extra:    extra,</span><br><span class="line">rt:       rt,</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rt *authProxyRoundTripper)</span> <span class="title">RoundTrip</span><span class="params">(req *http.Request)</span> <span class="params">(*http.Response, error)</span></span> {</span><br><span class="line">  req = utilnet.CloneRequest(req)</span><br><span class="line">  <span class="comment">// 包裹user信息</span></span><br><span class="line">SetAuthProxyHeaders(req, rt.username, rt.groups, rt.extra)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> rt.rt.RoundTrip(req)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// SetAuthProxyHeaders stomps the auth proxy header fields.  It mutates its argument.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">SetAuthProxyHeaders</span><span class="params">(req *http.Request, username <span class="keyword">string</span>, groups []<span class="keyword">string</span>, extra <span class="keyword">map</span>[<span class="keyword">string</span>][]<span class="keyword">string</span>)</span></span> {</span><br><span class="line">  <span class="comment">// 清楚原始url的requestheader信息</span></span><br><span class="line">req.Header.Del(<span class="string">"X-Remote-User"</span>)</span><br><span class="line">req.Header.Del(<span class="string">"X-Remote-Group"</span>)</span><br><span class="line"><span class="keyword">for</span> key := <span class="keyword">range</span> req.Header {</span><br><span class="line"><span class="keyword">if</span> strings.HasPrefix(strings.ToLower(key), strings.ToLower(<span class="string">"X-Remote-Extra-"</span>)) {</span><br><span class="line">req.Header.Del(key)</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 通过user重新填充信息</span></span><br><span class="line">req.Header.Set(<span class="string">"X-Remote-User"</span>, username)</span><br><span class="line"><span class="keyword">for</span> _, group := <span class="keyword">range</span> groups {</span><br><span class="line">req.Header.Add(<span class="string">"X-Remote-Group"</span>, group)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">for</span> key, values := <span class="keyword">range</span> extra {</span><br><span class="line"><span class="keyword">for</span> _, value := <span class="keyword">range</span> values {</span><br><span class="line">req.Header.Add(<span class="string">"X-Remote-Extra-"</span>+headerKeyEscape(key), value)</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="扩展apiserver处理逻辑"><a href="#扩展apiserver处理逻辑" class="headerlink" title="扩展apiserver处理逻辑"></a>扩展apiserver处理逻辑</h3><p>下以metrics-server为例说明扩展apiserver在收到apiserver请求后的处理</p><p>与apiserver初始化相同，metrics-server也需要初始化生成<code>genericServer</code>, 然后注册apigroup<br><code>pkg/metrics-server/config.go</code><br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c Config)</span> <span class="title">Complete</span><span class="params">()</span> <span class="params">(*MetricsServer, error)</span></span> {</span><br><span class="line">informer, err := c.informer()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">kubeletClient, err := c.kubeletClient()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">addressResolver := c.addressResolver()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建scraper，负责抓取监控数据</span></span><br><span class="line">scrape := scraper.NewScraper(informer.Core().V1().Nodes().Lister(), kubeletClient, addressResolver, c.ScrapeTimeout)</span><br><span class="line"></span><br><span class="line">scraper.RegisterScraperMetrics(c.ScrapeTimeout)</span><br><span class="line">RegisterServerMetrics(c.MetricResolution)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 生成genericServer, 包裹有 DefaultBuildHandlerChain</span></span><br><span class="line">genericServer, err := c.Apiserver.Complete(informer).New(<span class="string">"metrics-server"</span>, genericapiserver.NewEmptyDelegate())</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">store := storage.NewStorage()</span><br><span class="line"><span class="comment">// 注册api</span></span><br><span class="line"><span class="keyword">if</span> err := api.Install(store, informer.Core().V1(), genericServer); err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> &amp;MetricsServer{</span><br><span class="line">GenericAPIServer: genericServer,</span><br><span class="line">storage:          store,</span><br><span class="line">scraper:          scrape,</span><br><span class="line">resolution:       c.MetricResolution,</span><br><span class="line">}, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>api注册代码，通过<code>Build</code>生成apigroup，调用<code>InstallAPIGroup</code>进行注册<br><code>pkg/api/install.go</code><br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// InstallStorage builds the metrics for the metrics.k8s.io API, and then installs it into the given API metrics-server.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Install</span><span class="params">(metrics MetricsGetter, informers coreinf.Interface, server *genericapiserver.GenericAPIServer)</span> <span class="title">error</span></span> {</span><br><span class="line">info := Build(metrics, informers)</span><br><span class="line"><span class="comment">// 注册apigroup</span></span><br><span class="line"><span class="keyword">return</span> server.InstallAPIGroup(&amp;info)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Build constructs APIGroupInfo the metrics.k8s.io API group using the given getters.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Build</span><span class="params">(m MetricsGetter, informers coreinf.Interface)</span> <span class="title">genericapiserver</span>.<span class="title">APIGroupInfo</span></span> {</span><br><span class="line">apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(metrics.GroupName, Scheme, metav1.ParameterCodec, Codecs)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册metrics相关api</span></span><br><span class="line">node := newNodeMetrics(metrics.Resource(<span class="string">"nodemetrics"</span>), m, informers.Nodes().Lister())</span><br><span class="line">pod := newPodMetrics(metrics.Resource(<span class="string">"podmetrics"</span>), m, informers.Pods().Lister())</span><br><span class="line">metricsServerResources := <span class="keyword">map</span>[<span class="keyword">string</span>]rest.Storage{</span><br><span class="line"><span class="string">"nodes"</span>: node,</span><br><span class="line"><span class="string">"pods"</span>:  pod,</span><br><span class="line">}</span><br><span class="line">apiGroupInfo.VersionedResourcesStorageMap[v1beta1.SchemeGroupVersion.Version] = metricsServerResources</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> apiGroupInfo</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>同apiserver，metrics-server收到请求后会经过<code>DefaultBuildHandlerChain</code></p><ul><li>认证，从apiserver转发来的请求是<code>reqeustheader</code>形式，metrics-server会使用<code>requestheader-ca</code>验证证书</li><li>鉴权，同apiserver一样</li></ul><blockquote><p>注意, 如果apiserver未配置<code>proxy-client</code>证书，metrics-server认证不通过，即使apiserver认证通过，metrics-server也会认为是匿名用户<code>system:anonymous</code></p></blockquote><p>最后，metrics-server执行具体逻辑，返回结果。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>扩容apiserver的创建，处理流程与apiserver完全一样，可以直接调用apiserver的库，扩展apiserver直接处理请求，不需要经过webhook，性能更好，更强大的是完全不使用etcd，替换成时序数据库或者其他数据库。后续可以分析下CRD与扩展apiserver的区别以及使用场景。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> apiserver </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kube-apiserver启动流程分析</title>
      <link href="/kube-apiserver-start/"/>
      <url>/kube-apiserver-start/</url>
      
        <content type="html"><![CDATA[<p>kube-apiserver 共由 3 个组件构成（Aggregator. KubeAPIServer. APIExtensionServer），这些组件依次通过 Delegation 处理请求：</p><ul><li>Aggregator：暴露的功能类似于一个七层负载均衡，将来自用户的请求拦截转发给其他服务器，并且负责整个 APIServer 的 Discovery 功能；也负责处理ApiService，注册对应的扩展api。</li><li>KubeAPIServer ：负责对请求的一些通用处理，认证. 鉴权等，以及处理各个内建资源的 REST 服务；</li><li>APIExtensionServer：主要处理 CustomResourceDefinition（CRD）和 CustomResource（CR）的 REST 请求，也是 Delegation 的最后一环，如果对应 CR 不能被处理的话则会返回 404。</li></ul><h2 id="kube-apiserver启动流程"><a href="#kube-apiserver启动流程" class="headerlink" title="kube-apiserver启动流程"></a>kube-apiserver启动流程</h2><p>Apiserver通过<code>Run</code>方法启动, 主要逻辑为：</p><ol><li>调用<code>CreateServerChain</code>构建服务调用链并判断是否启动非安全的<code>httpserver</code>，<code>httpserver</code>链中包含 apiserver要启动的三个server，以及为每个server注册对应资源的路由；</li><li>调用<code>server.PrepareRun</code>进行服务运行前的准备，该方法主要完成了健康检查. 存活检查和OpenAPI路由的注册工作；</li><li>调用<code>prepared.Run</code>启动server；<figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Run runs the specified APIServer.  This should never exit.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Run</span><span class="params">(completeOptions completedServerRunOptions, stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span> <span class="title">error</span></span> {</span><br><span class="line"><span class="comment">// To help debugging, immediately log version</span></span><br><span class="line">klog.Infof(<span class="string">"Version: %+v"</span>, version.Get())</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 创建调用链</span></span><br><span class="line">server, err := CreateServerChain(completeOptions, stopCh)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 进行一些准备工作， 注册一些hander，执行hook等</span></span><br><span class="line">prepared, err := server.PrepareRun()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 开始执行</span></span><br><span class="line"><span class="keyword">return</span> prepared.Run(stopCh)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></li></ol><p>执行具体的<code>Run</code>方法<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Run spawns the secure http server. It only returns if stopCh is closed</span></span><br><span class="line"><span class="comment">// or the secure port cannot be listened on initially.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s preparedGenericAPIServer)</span> <span class="title">Run</span><span class="params">(stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span> <span class="title">error</span></span> {</span><br><span class="line">delayedStopCh := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line"><span class="keyword">defer</span> <span class="built_in">close</span>(delayedStopCh)</span><br><span class="line"></span><br><span class="line">&lt;-stopCh</span><br><span class="line"></span><br><span class="line"><span class="comment">// As soon as shutdown is initiated, /readyz should start returning failure.</span></span><br><span class="line"><span class="comment">// This gives the load balancer a window defined by ShutdownDelayDuration to detect that /readyz is red</span></span><br><span class="line"><span class="comment">// and stop sending traffic to this server.</span></span><br><span class="line"><span class="comment">// 当终止时，关闭readiness</span></span><br><span class="line"><span class="built_in">close</span>(s.readinessStopCh)</span><br><span class="line"></span><br><span class="line">time.Sleep(s.ShutdownDelayDuration)</span><br><span class="line">}()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 执行非阻塞Run</span></span><br><span class="line"><span class="comment">// close socket after delayed stopCh</span></span><br><span class="line">err := s.NonBlockingRun(delayedStopCh)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">&lt;-stopCh</span><br><span class="line"></span><br><span class="line"><span class="comment">// run shutdown hooks directly. This includes deregistering from the kubernetes endpoint in case of kube-apiserver.</span></span><br><span class="line"><span class="comment">// 关闭前执行一些hook操作</span></span><br><span class="line">err = s.RunPreShutdownHooks()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// wait for the delayed stopCh before closing the handler chain (it rejects everything after Wait has been called).</span></span><br><span class="line">&lt;-delayedStopCh</span><br><span class="line"></span><br><span class="line"><span class="comment">// 等待所有请求执行完</span></span><br><span class="line"><span class="comment">// Wait for all requests to finish, which are bounded by the RequestTimeout variable.</span></span><br><span class="line">s.HandlerChainWaitGroup.Wait()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>执行<code>NonBlockingRun</code><br><code>k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:351</code><br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s preparedGenericAPIServer)</span> <span class="title">NonBlockingRun</span><span class="params">(stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span> <span class="title">error</span></span> {</span><br><span class="line">    auditStopCh := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 判断是否要启动审计日志</span></span><br><span class="line">    <span class="keyword">if</span> s.AuditBackend != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">if</span> err := s.AuditBackend.Run(auditStopCh); err != <span class="literal">nil</span> {</span><br><span class="line">            <span class="keyword">return</span> fmt.Errorf(<span class="string">"failed to run the audit backend: %v"</span>, err)</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 启动 https server</span></span><br><span class="line">    internalStopCh := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span><br><span class="line">    <span class="keyword">var</span> stoppedCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>{}</span><br><span class="line">    <span class="keyword">if</span> s.SecureServingInfo != <span class="literal">nil</span> &amp;&amp; s.Handler != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">var</span> err error</span><br><span class="line">        stoppedCh, err = s.SecureServingInfo.Serve(s.Handler, s.ShutdownTimeout, internalStopCh)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">            <span class="built_in">close</span>(internalStopCh)</span><br><span class="line">            <span class="built_in">close</span>(auditStopCh)</span><br><span class="line">            <span class="keyword">return</span> err</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line">        &lt;-stopCh</span><br><span class="line">        <span class="built_in">close</span>(s.readinessStopCh)</span><br><span class="line">        <span class="built_in">close</span>(internalStopCh)</span><br><span class="line">        <span class="keyword">if</span> stoppedCh != <span class="literal">nil</span> {</span><br><span class="line">            &lt;-stoppedCh</span><br><span class="line">        }</span><br><span class="line">        s.HandlerChainWaitGroup.Wait()</span><br><span class="line">        <span class="built_in">close</span>(auditStopCh)</span><br><span class="line">    }()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 执行 postStartHooks</span></span><br><span class="line">    s.RunPostStartHooks(stopCh)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 向 systemd 发送 ready 信号</span></span><br><span class="line">    <span class="keyword">if</span> _, err := systemd.SdNotify(<span class="literal">true</span>, <span class="string">"READY=1\n"</span>); err != <span class="literal">nil</span> {</span><br><span class="line">        klog.Errorf(<span class="string">"Unable to send systemd daemon successful start message: %v\n"</span>, err)</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="调用链分析"><a href="#调用链分析" class="headerlink" title="调用链分析"></a>调用链分析</h2><p>上一节简单分析了Apiserver的启动流程，通过初始化各种配置，封装调用链，启动Server。这节主要分析调用链。</p><p>初始化阶段, 通过<code>CreateServerChain</code>创建调用链， 代码在<a href="https://github.com/kubernetes/kubernetes/blob/1d057da2f73118893b5cc27c15d59ff03beb271e/cmd/kube-apiserver/app/server.go#L169" target="_blank" rel="noopener">server.go</a><br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CreateServerChain creates the apiservers connected via delegation.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">CreateServerChain</span><span class="params">(completedOptions completedServerRunOptions, stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span> <span class="params">(*aggregatorapiserver.APIAggregator, error)</span></span> {</span><br><span class="line">  <span class="comment">// nodetunneler与node通信，proxy实现代理功能，转发请求给其他apiservice</span></span><br><span class="line">  <span class="comment">// apiserver到cluster的通信可以通过三种方法</span></span><br><span class="line">  <span class="comment">// apiserver到kubelet的endpoint，用于logs功能，exec功能，port-forward功能</span></span><br><span class="line">  <span class="comment">// HTTP连接，即使可以用HTTPS也不做任何其他校验，并不安全</span></span><br><span class="line">  <span class="comment">// ssh tunnel，不推荐使用</span></span><br><span class="line"></span><br><span class="line">  nodeTunneler, proxyTransport, err := CreateNodeDialer(completedOptions)</span><br><span class="line">    <span class="comment">// 1. 为 kubeAPIServer 创建配置</span></span><br><span class="line">    kubeAPIServerConfig, insecureServingInfo, serviceResolver, pluginInitializer, admissionPostStartHook, err :=                                         CreateKubeAPIServerConfig(completedOptions, nodeTunneler, proxyTransport)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 判断是否配置了 APIExtensionsServer，创建 apiExtensionsConfig </span></span><br><span class="line">    apiExtensionsConfig, err := createAPIExtensionsConfig(*kubeAPIServerConfig.GenericConfig, kubeAPIServerConfig.ExtraConfig.VersionedInformers,        pluginInitializer, completedOptions.ServerRunOptions, completedOptions.MasterCount,vc</span><br><span class="line">        serviceResolver, webhook.NewDefaultAuthenticationInfoResolverWrapper(proxyTransport, kubeAPIServerConfig.GenericConfig.LoopbackClientConfig))</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 3. 初始化 APIExtensionsServer, 通过一个空的delegate初始化</span></span><br><span class="line">    apiExtensionsServer, err := createAPIExtensionsServer(apiExtensionsConfig, genericapiserver.NewEmptyDelegate())</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 初始化 KubeAPIServer</span></span><br><span class="line">    kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, admissionPostStartHook)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 5. 创建 AggregatorConfig</span></span><br><span class="line">    aggregatorConfig, err := createAggregatorConfig(*kubeAPIServerConfig.GenericConfig, completedOptions.ServerRunOptions, kubeAPIServerConfig.          ExtraConfig.VersionedInformers, serviceResolver, proxyTransport, pluginInitializer)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 6. 初始化 AggregatorServer</span></span><br><span class="line">    aggregatorServer, err := createAggregatorServer(aggregatorConfig, kubeAPIServer.GenericAPIServer, apiExtensionsServer.Informers)</span><br><span class="line">    <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7. 判断是否启动非安全端口的 http server</span></span><br><span class="line">    <span class="keyword">if</span> insecureServingInfo != <span class="literal">nil</span> {</span><br><span class="line">        insecureHandlerChain := kubeserver.BuildInsecureHandlerChain(aggregatorServer.GenericAPIServer.UnprotectedHandler(), kubeAPIServerConfig.GenericConfig)</span><br><span class="line">        <span class="keyword">if</span> err := insecureServingInfo.Serve(insecureHandlerChain, kubeAPIServerConfig.GenericConfig.RequestTimeout, stopCh); err != <span class="literal">nil</span> {</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> aggregatorServer, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>创建过程主要有以下步骤：</p><ol><li>根据配置构造apiserver的配置，调用方法<code>CreateKubeAPIServerConfig</code></li><li>根据配置构造扩展的apiserver的配置，调用方法为<code>createAPIExtensionsConfig</code></li><li>创建server，包括扩展的apiserver和原生的apiserver，调用方法为<code>createAPIExtensionsServer</code>和<code>CreateKubeAPIServer</code>。主要就是将各个handler的路由方法注册到Container中去，完全遵循go-restful的设计模式，即将处理方法注册到Route中去，同一个根路径下的Route注册到WebService中去，WebService注册到Container中，Container负责分发。访问的过程为<code>Container--&gt;WebService--&gt;Route</code></li><li>聚合server的配置和和创建。主要就是将原生的apiserver和扩展的apiserver的访问进行整合，添加后续的一些处理接口。调用方法为<code>createAggregatorConfig</code>和<code>createAggregatorServer</code></li><li>创建完成，返回配置的server信息</li></ol><p>以上几个步骤，最核心的就是apiserver如何创建，即如何按照go-restful的模式，添加路由和相应的处理方法。</p><h3 id="配置初始化"><a href="#配置初始化" class="headerlink" title="配置初始化"></a>配置初始化</h3><p>先看apiserver配置的创建<code>CreateKubeAPIServerConfig-&gt;buildGenericConfig-&gt;genericapiserver.NewConfig</code></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BuildGenericConfig takes the master server options and produces the genericapiserver.Config associated with it</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">buildGenericConfig</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">s *options.ServerRunOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">proxyTransport *http.Transport,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> <span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">genericConfig *genericapiserver.Config,</span></span></span><br><span class="line"><span class="function"><span class="params">versionedInformers clientgoinformers.SharedInformerFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">insecureServingInfo *genericapiserver.DeprecatedInsecureServingInfo,</span></span></span><br><span class="line"><span class="function"><span class="params">serviceResolver aggregatorapiserver.ServiceResolver,</span></span></span><br><span class="line"><span class="function"><span class="params">pluginInitializers []admission.PluginInitializer,</span></span></span><br><span class="line"><span class="function"><span class="params">admissionPostStartHook genericapiserver.PostStartHookFunc,</span></span></span><br><span class="line"><span class="function"><span class="params">storageFactory *serverstorage.DefaultStorageFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">lastErr error,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span> {</span><br><span class="line"><span class="comment">// 创建genericConfig,其中包括DefaultBuildHandlerChain，一系列认证授权的中间件</span></span><br><span class="line">genericConfig = genericapiserver.NewConfig(legacyscheme.Codecs)</span><br><span class="line">genericConfig.MergedResourceConfig = master.DefaultAPIResourceConfigSource()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化各种配置</span></span><br><span class="line"><span class="keyword">if</span> lastErr = s.GenericServerRunOptions.ApplyTo(genericConfig); lastErr != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">genericConfig.OpenAPIConfig = genericapiserver.DefaultOpenAPIConfig(generatedopenapi.GetOpenAPIDefinitions, openapinamer.NewDefinitionNamer(legacyscheme.Scheme, extensionsapiserver.Scheme, aggregatorscheme.Scheme))</span><br><span class="line">genericConfig.OpenAPIConfig.Info.Title = <span class="string">"Kubernetes"</span></span><br><span class="line"><span class="comment">// 长连接请求</span></span><br><span class="line">genericConfig.LongRunningFunc = filters.BasicLongRunningRequestCheck(</span><br><span class="line">sets.NewString(<span class="string">"watch"</span>, <span class="string">"proxy"</span>),</span><br><span class="line">sets.NewString(<span class="string">"attach"</span>, <span class="string">"exec"</span>, <span class="string">"proxy"</span>, <span class="string">"log"</span>, <span class="string">"portforward"</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">kubeVersion := version.Get()</span><br><span class="line">genericConfig.Version = &amp;kubeVersion</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化storageFactory， 用来连接etcd</span></span><br><span class="line">storageFactoryConfig := kubeapiserver.NewStorageFactoryConfig()</span><br><span class="line">storageFactoryConfig.APIResourceConfig = genericConfig.MergedResourceConfig</span><br><span class="line">completedStorageFactoryConfig, err := storageFactoryConfig.Complete(s.Etcd)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">lastErr = err</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">storageFactory, lastErr = completedStorageFactoryConfig.New()</span><br><span class="line"><span class="keyword">if</span> lastErr != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> genericConfig.EgressSelector != <span class="literal">nil</span> {</span><br><span class="line">storageFactory.StorageConfig.Transport.EgressLookup = genericConfig.EgressSelector.Lookup</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> lastErr = s.Etcd.ApplyWithStorageFactoryTo(storageFactory, genericConfig); lastErr != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use protobufs for self-communication.</span></span><br><span class="line"><span class="comment">// Since not every generic apiserver has to support protobufs, we</span></span><br><span class="line"><span class="comment">// cannot default to it in generic apiserver and need to explicitly</span></span><br><span class="line"><span class="comment">// set it in kube-apiserver.</span></span><br><span class="line"><span class="comment">// 内部使用protobufs通信</span></span><br><span class="line">genericConfig.LoopbackClientConfig.ContentConfig.ContentType = <span class="string">"application/vnd.kubernetes.protobuf"</span></span><br><span class="line"><span class="comment">// Disable compression for self-communication, since we are going to be</span></span><br><span class="line"><span class="comment">// on a fast local network</span></span><br><span class="line">genericConfig.LoopbackClientConfig.DisableCompression = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// clientset初始化</span></span><br><span class="line">kubeClientConfig := genericConfig.LoopbackClientConfig</span><br><span class="line">clientgoExternalClient, err := clientgoclientset.NewForConfig(kubeClientConfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">lastErr = fmt.Errorf(<span class="string">"failed to create real external clientset: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">versionedInformers = clientgoinformers.NewSharedInformerFactory(clientgoExternalClient, <span class="number">10</span>*time.Minute)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化认证实例，支持多种认证方式：requestheader,token, tls等</span></span><br><span class="line">genericConfig.Authentication.Authenticator, genericConfig.OpenAPIConfig.SecurityDefinitions, err = BuildAuthenticator(s, genericConfig.EgressSelector, clientgoExternalClient, versionedInformers)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">lastErr = fmt.Errorf(<span class="string">"invalid authentication config: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化鉴权配置</span></span><br><span class="line">genericConfig.Authorization.Authorizer, genericConfig.RuleResolver, err = BuildAuthorizer(s, genericConfig.EgressSelector, versionedInformers)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">lastErr = fmt.Errorf(<span class="string">"invalid authorization config: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> !sets.NewString(s.Authorization.Modes...).Has(modes.ModeRBAC) {</span><br><span class="line">genericConfig.DisabledPostStartHooks.Insert(rbacrest.PostStartHookName)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化admission webhook的配置</span></span><br><span class="line">admissionConfig := &amp;kubeapiserveradmission.Config{</span><br><span class="line">ExternalInformers:    versionedInformers,</span><br><span class="line">LoopbackClientConfig: genericConfig.LoopbackClientConfig,</span><br><span class="line">CloudConfigFile:      s.CloudProvider.CloudConfigFile,</span><br><span class="line">}</span><br><span class="line">serviceResolver = buildServiceResolver(s.EnableAggregatorRouting, genericConfig.LoopbackClientConfig.Host, versionedInformers)</span><br><span class="line"></span><br><span class="line">authInfoResolverWrapper := webhook.NewDefaultAuthenticationInfoResolverWrapper(proxyTransport, genericConfig.EgressSelector, genericConfig.LoopbackClientConfig)</span><br><span class="line"></span><br><span class="line">lastErr = s.Audit.ApplyTo(</span><br><span class="line">genericConfig,</span><br><span class="line">genericConfig.LoopbackClientConfig,</span><br><span class="line">versionedInformers,</span><br><span class="line">serveroptions.NewProcessInfo(<span class="string">"kube-apiserver"</span>, <span class="string">"kube-system"</span>),</span><br><span class="line">&amp;serveroptions.WebhookOptions{</span><br><span class="line">AuthInfoResolverWrapper: authInfoResolverWrapper,</span><br><span class="line">ServiceResolver:         serviceResolver,</span><br><span class="line">},</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> lastErr != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化注入插件</span></span><br><span class="line">pluginInitializers, admissionPostStartHook, err = admissionConfig.New(proxyTransport, genericConfig.EgressSelector, serviceResolver)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">lastErr = fmt.Errorf(<span class="string">"failed to create admission plugin initializer: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">err = s.Admission.ApplyTo(</span><br><span class="line">genericConfig,</span><br><span class="line">versionedInformers,</span><br><span class="line">kubeClientConfig,</span><br><span class="line">feature.DefaultFeatureGate,</span><br><span class="line">pluginInitializers...)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">lastErr = fmt.Errorf(<span class="string">"failed to initialize admission: %v"</span>, err)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> utilfeature.DefaultFeatureGate.Enabled(genericfeatures.APIPriorityAndFairness) &amp;&amp; s.GenericServerRunOptions.EnablePriorityAndFairness {</span><br><span class="line">genericConfig.FlowControl = BuildPriorityAndFairness(s, clientgoExternalClient, versionedInformers)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="APIExtensionsServer初始化"><a href="#APIExtensionsServer初始化" class="headerlink" title="APIExtensionsServer初始化"></a>APIExtensionsServer初始化</h3><p><code>APIExtensionsServer</code>最先初始化，在调用链的末尾, 处理CR、CRD相关资源.</p><p>其中包含的 controller 以及功能如下所示：</p><ol><li>openapiController：将 crd 资源的变化同步至提供的 OpenAPI 文档，可通过访问 /openapi/v2 进行查看；</li><li>crdController：负责将 crd 信息注册到 apiVersions 和 apiResources 中，两者的信息可通过 $ kubectl api-versions 和 $ kubectl api-resources 查看；</li><li>namingController：检查 crd obj 中是否有命名冲突，可在 crd .status.conditions 中查看；</li><li>establishingController：检查 crd 是否处于正常状态，可在 crd .status.conditions 中查看；</li><li>nonStructuralSchemaController：检查 crd obj 结构是否正常，可在 crd .status.conditions 中查看；</li><li>apiApprovalController：检查 crd 是否遵循 kubernetes API 声明策略，可在 crd .status.conditions 中查看；</li><li>finalizingController：类似于 finalizes 的功能，与 CRs 的删除有关；</li></ol><p><code>createAPIExtensionsServer</code>调用<code>apiextensionsConfig.Complete().New(delegateAPIServer)</code></p><p><code>k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/apiserver.go:132</code><br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">/ New returns a <span class="built_in">new</span> instance of CustomResourceDefinitions from the given config.</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c completedConfig)</span> <span class="title">New</span><span class="params">(delegationTarget genericapiserver.DelegationTarget)</span> <span class="params">(*CustomResourceDefinitions, error)</span></span> {</span><br><span class="line"><span class="comment">// 初始化 genericServer</span></span><br><span class="line">genericServer, err := c.GenericConfig.New(<span class="string">"apiextensions-apiserver"</span>, delegationTarget)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">s := &amp;CustomResourceDefinitions{</span><br><span class="line">GenericAPIServer: genericServer,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化apigroup, 即需要暴露的api，这里extension apiserver只注册了cr于crd相关的</span></span><br><span class="line">apiResourceConfig := c.GenericConfig.MergedResourceConfig</span><br><span class="line">apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(apiextensions.GroupName, Scheme, metav1.ParameterCodec, Codecs)</span><br><span class="line"><span class="keyword">if</span> apiResourceConfig.VersionEnabled(v1beta1.SchemeGroupVersion) {</span><br><span class="line">storage := <span class="keyword">map</span>[<span class="keyword">string</span>]rest.Storage{}</span><br><span class="line"><span class="comment">// customresourcedefinitions</span></span><br><span class="line">customResourceDefintionStorage := customresourcedefinition.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)</span><br><span class="line">storage[<span class="string">"customresourcedefinitions"</span>] = customResourceDefintionStorage</span><br><span class="line">storage[<span class="string">"customresourcedefinitions/status"</span>] = customresourcedefinition.NewStatusREST(Scheme, customResourceDefintionStorage)</span><br><span class="line"></span><br><span class="line">apiGroupInfo.VersionedResourcesStorageMap[v1beta1.SchemeGroupVersion.Version] = storage</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> apiResourceConfig.VersionEnabled(v1.SchemeGroupVersion) {</span><br><span class="line">storage := <span class="keyword">map</span>[<span class="keyword">string</span>]rest.Storage{}</span><br><span class="line"><span class="comment">// customresourcedefinitions</span></span><br><span class="line">customResourceDefintionStorage := customresourcedefinition.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter)</span><br><span class="line">storage[<span class="string">"customresourcedefinitions"</span>] = customResourceDefintionStorage</span><br><span class="line">storage[<span class="string">"customresourcedefinitions/status"</span>] = customresourcedefinition.NewStatusREST(Scheme, customResourceDefintionStorage)</span><br><span class="line"></span><br><span class="line">apiGroupInfo.VersionedResourcesStorageMap[v1.SchemeGroupVersion.Version] = storage</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册apigroup</span></span><br><span class="line"><span class="keyword">if</span> err := s.GenericAPIServer.InstallAPIGroup(&amp;apiGroupInfo); err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// clientset创建</span></span><br><span class="line">crdClient, err := clientset.NewForConfig(s.GenericAPIServer.LoopbackClientConfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="comment">// it's really bad that this is leaking here, but until we can fix the test (which I'm pretty sure isn't even testing what it wants to test),</span></span><br><span class="line"><span class="comment">// we need to be able to move forward</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"failed to create clientset: %v"</span>, err)</span><br><span class="line">}</span><br><span class="line">s.Informers = externalinformers.NewSharedInformerFactory(crdClient, <span class="number">5</span>*time.Minute)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建各种handler</span></span><br><span class="line">delegateHandler := delegationTarget.UnprotectedHandler()</span><br><span class="line"><span class="keyword">if</span> delegateHandler == <span class="literal">nil</span> {</span><br><span class="line">delegateHandler = http.NotFoundHandler()</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">versionDiscoveryHandler := &amp;versionDiscoveryHandler{</span><br><span class="line">discovery: <span class="keyword">map</span>[schema.GroupVersion]*discovery.APIVersionHandler{},</span><br><span class="line">delegate:  delegateHandler,</span><br><span class="line">}</span><br><span class="line">groupDiscoveryHandler := &amp;groupDiscoveryHandler{</span><br><span class="line">discovery: <span class="keyword">map</span>[<span class="keyword">string</span>]*discovery.APIGroupHandler{},</span><br><span class="line">delegate:  delegateHandler,</span><br><span class="line">}</span><br><span class="line">establishingController := establish.NewEstablishingController(s.Informers.Apiextensions().V1().CustomResourceDefinitions(), crdClient.ApiextensionsV1())</span><br><span class="line">crdHandler, err := NewCustomResourceDefinitionHandler(</span><br><span class="line">versionDiscoveryHandler,</span><br><span class="line">groupDiscoveryHandler,</span><br><span class="line">s.Informers.Apiextensions().V1().CustomResourceDefinitions(),</span><br><span class="line">delegateHandler,</span><br><span class="line">c.ExtraConfig.CRDRESTOptionsGetter,</span><br><span class="line">c.GenericConfig.AdmissionControl,</span><br><span class="line">establishingController,</span><br><span class="line">c.ExtraConfig.ServiceResolver,</span><br><span class="line">c.ExtraConfig.AuthResolverWrapper,</span><br><span class="line">c.ExtraConfig.MasterCount,</span><br><span class="line">s.GenericAPIServer.Authorizer,</span><br><span class="line">c.GenericConfig.RequestTimeout,</span><br><span class="line">time.Duration(c.GenericConfig.MinRequestTimeout)*time.Second,</span><br><span class="line">apiGroupInfo.StaticOpenAPISpec,</span><br><span class="line">c.GenericConfig.MaxRequestBodyBytes,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(<span class="string">"/apis"</span>, crdHandler)</span><br><span class="line">s.GenericAPIServer.Handler.NonGoRestfulMux.HandlePrefix(<span class="string">"/apis/"</span>, crdHandler)</span><br><span class="line"></span><br><span class="line">discoveryController := NewDiscoveryController(s.Informers.Apiextensions().V1().CustomResourceDefinitions(), versionDiscoveryHandler, groupDiscoveryHandler)</span><br><span class="line">namingController := status.NewNamingConditionController(s.Informers.Apiextensions().V1().CustomResourceDefinitions(), crdClient.ApiextensionsV1())</span><br><span class="line">nonStructuralSchemaController := nonstructuralschema.NewConditionController(s.Informers.Apiextensions().V1().CustomResourceDefinitions(), crdClient.ApiextensionsV1())</span><br><span class="line">apiApprovalController := apiapproval.NewKubernetesAPIApprovalPolicyConformantConditionController(s.Informers.Apiextensions().V1().CustomResourceDefinitions(), crdClient.ApiextensionsV1())</span><br><span class="line">finalizingController := finalizer.NewCRDFinalizer(</span><br><span class="line">s.Informers.Apiextensions().V1().CustomResourceDefinitions(),</span><br><span class="line">crdClient.ApiextensionsV1(),</span><br><span class="line">crdHandler,</span><br><span class="line">)</span><br><span class="line">openapiController := openapicontroller.NewController(s.Informers.Apiextensions().V1().CustomResourceDefinitions())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 加入到启动hook中</span></span><br><span class="line">s.GenericAPIServer.AddPostStartHookOrDie(<span class="string">"start-apiextensions-informers"</span>, <span class="function"><span class="keyword">func</span><span class="params">(context genericapiserver.PostStartHookContext)</span> <span class="title">error</span></span> {</span><br><span class="line">s.Informers.Start(context.StopCh)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">})</span><br><span class="line">s.GenericAPIServer.AddPostStartHookOrDie(<span class="string">"start-apiextensions-controllers"</span>, <span class="function"><span class="keyword">func</span><span class="params">(context genericapiserver.PostStartHookContext)</span> <span class="title">error</span></span> {</span><br><span class="line"><span class="comment">// OpenAPIVersionedService and StaticOpenAPISpec are populated in generic apiserver PrepareRun().</span></span><br><span class="line"><span class="comment">// Together they serve the /openapi/v2 endpoint on a generic apiserver. A generic apiserver may</span></span><br><span class="line"><span class="comment">// choose to not enable OpenAPI by having null openAPIConfig, and thus OpenAPIVersionedService</span></span><br><span class="line"><span class="comment">// and StaticOpenAPISpec are both null. In that case we don't run the CRD OpenAPI controller.</span></span><br><span class="line"><span class="keyword">if</span> s.GenericAPIServer.OpenAPIVersionedService != <span class="literal">nil</span> &amp;&amp; s.GenericAPIServer.StaticOpenAPISpec != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">go</span> openapiController.Run(s.GenericAPIServer.StaticOpenAPISpec, s.GenericAPIServer.OpenAPIVersionedService, context.StopCh)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">go</span> namingController.Run(context.StopCh)</span><br><span class="line"><span class="keyword">go</span> establishingController.Run(context.StopCh)</span><br><span class="line"><span class="keyword">go</span> nonStructuralSchemaController.Run(<span class="number">5</span>, context.StopCh)</span><br><span class="line"><span class="keyword">go</span> apiApprovalController.Run(<span class="number">5</span>, context.StopCh)</span><br><span class="line"><span class="keyword">go</span> finalizingController.Run(<span class="number">5</span>, context.StopCh)</span><br><span class="line"></span><br><span class="line">discoverySyncedCh := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">struct</span>{})</span><br><span class="line"><span class="keyword">go</span> discoveryController.Run(context.StopCh, discoverySyncedCh)</span><br><span class="line"><span class="keyword">select</span> {</span><br><span class="line"><span class="keyword">case</span> &lt;-context.StopCh:</span><br><span class="line"><span class="keyword">case</span> &lt;-discoverySyncedCh:</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">})</span><br><span class="line"><span class="comment">// we don't want to report healthy until we can handle all CRDs that have already been registered.  Waiting for the informer</span></span><br><span class="line"><span class="comment">// to sync makes sure that the lister will be valid before we begin.  There may still be races for CRDs added after startup,</span></span><br><span class="line"><span class="comment">// but we won't go healthy until we can handle the ones already present.</span></span><br><span class="line">s.GenericAPIServer.AddPostStartHookOrDie(<span class="string">"crd-informer-synced"</span>, <span class="function"><span class="keyword">func</span><span class="params">(context genericapiserver.PostStartHookContext)</span> <span class="title">error</span></span> {</span><br><span class="line"><span class="keyword">return</span> wait.PollImmediateUntil(<span class="number">100</span>*time.Millisecond, <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="params">(<span class="keyword">bool</span>, error)</span></span> {</span><br><span class="line"><span class="keyword">return</span> s.Informers.Apiextensions().V1().CustomResourceDefinitions().Informer().HasSynced(), <span class="literal">nil</span></span><br><span class="line">}, context.StopCh)</span><br><span class="line">})</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p><code>c.GenericConfig.New</code>来初始化<code>genericapiserver</code>,包裹一些默认链，创建handler<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c completedConfig)</span> <span class="title">New</span><span class="params">(name <span class="keyword">string</span>, delegationTarget DelegationTarget)</span> <span class="params">(*GenericAPIServer, error)</span></span> {</span><br><span class="line"><span class="keyword">if</span> c.Serializer == <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"Genericapiserver.New() called with config.Serializer == nil"</span>)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> c.LoopbackClientConfig == <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"Genericapiserver.New() called with config.LoopbackClientConfig == nil"</span>)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> c.EquivalentResourceRegistry == <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"Genericapiserver.New() called with config.EquivalentResourceRegistry == nil"</span>)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 包裹了DefaultBuildHandlerChain</span></span><br><span class="line">handlerChainBuilder := <span class="function"><span class="keyword">func</span><span class="params">(handler http.Handler)</span> <span class="title">http</span>.<span class="title">Handler</span></span> {</span><br><span class="line"><span class="keyword">return</span> c.BuildHandlerChainFunc(handler, c.Config)</span><br><span class="line">}</span><br><span class="line"><span class="comment">// 创建apiserverhandler</span></span><br><span class="line">apiServerHandler := NewAPIServerHandler(name, c.Serializer, handlerChainBuilder, delegationTarget.UnprotectedHandler())</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> s, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p><code>APIServerHandler</code>包含多种<code>http.Handler</code>类型，包括<code>go-restful</code>以及<code>non-go-restful</code>，以及在以上两者之间选择的<code>Director</code>对象，<code>go-restful</code>用于处理已经注册的handler，<code>non-go-restful用来处理不存在的handler，API URI处理的选择过程为：</code>FullHandlerChain-&gt; Director -&gt;{GoRestfulContainer， NonGoRestfulMux}<code>。</code>NewAPIServerHandler`<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">func NewAPIServerHandler(name string, s runtime.NegotiatedSerializer, handlerChainBuilder HandlerChainBuilderFn, notFoundHandler http.Handler) *APIServerHandler {</span><br><span class="line">// non-go-restful路由</span><br><span class="line">nonGoRestfulMux := mux.NewPathRecorderMux(name)</span><br><span class="line">if notFoundHandler != nil {</span><br><span class="line">nonGoRestfulMux.NotFoundHandler(notFoundHandler)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">// go-resetful路由</span><br><span class="line">gorestfulContainer := restful.NewContainer()</span><br><span class="line">gorestfulContainer.ServeMux = http.NewServeMux()</span><br><span class="line">gorestfulContainer.Router(restful.CurlyRouter{}) // e.g. for proxy/{kind}/{name}/{*}</span><br><span class="line">gorestfulContainer.RecoverHandler(func(panicReason interface{}, httpWriter http.ResponseWriter) {</span><br><span class="line">logStackOnRecover(s, panicReason, httpWriter)</span><br><span class="line">})</span><br><span class="line">gorestfulContainer.ServiceErrorHandler(func(serviceErr restful.ServiceError, request *restful.Request, response *restful.Response) {</span><br><span class="line">serviceErrorHandler(s, serviceErr, request, response)</span><br><span class="line">})</span><br><span class="line"></span><br><span class="line">// 选择器, 根据path选择是否执行go-restful，注册过的path执行go-restful</span><br><span class="line">director := director{</span><br><span class="line">name:               name,</span><br><span class="line">goRestfulContainer: gorestfulContainer,</span><br><span class="line">nonGoRestfulMux:    nonGoRestfulMux,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">return &amp;APIServerHandler{</span><br><span class="line">FullHandlerChain:   handlerChainBuilder(director),</span><br><span class="line">GoRestfulContainer: gorestfulContainer,</span><br><span class="line">NonGoRestfulMux:    nonGoRestfulMux,</span><br><span class="line">Director:           director,</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>以上是<code>APIExtensionsServer</code>的初始化流程，初始化Server, 调用<code>s.GenericAPIServer.InstallAPIGroup</code>注册api。此方法的调用链非常深，主要是为了将需要暴露的<code>API Resource</code>注册到 server 中，以便能通过 http 接口进行 resource 的 REST 操作，其他几种 server 在初始化时也都会执行对应的 <code>InstallAPI</code>方法。</p><h3 id="KubeAPIServer初始化"><a href="#KubeAPIServer初始化" class="headerlink" title="KubeAPIServer初始化"></a>KubeAPIServer初始化</h3><p>KubeAPIServer 主要是提供对 API Resource 的操作请求，为 kubernetes 中众多 API 注册路由信息，暴露 RESTful API 并且对外提供 kubernetes service，使集群中以及集群外的服务都可以通过 RESTful API 操作 kubernetes 中的资源。</p><p>与<code>APIExtensionsServer</code>，<code>KubeAPIServer</code>初始化流程如下</p><ol><li><code>CreateKubeAPIServer</code>调用<code>kubeAPIServerConfig.Complete().New</code>来初始化</li><li><code>New</code>函数创建默认的<code>apigroup</code>(pod,deployment等内部资源), 调用<code>InstallAPIs</code>注册</li><li>启动相关controller, 加入到<code>poststarthook</code></li></ol><h3 id="AggregatorServer初始化"><a href="#AggregatorServer初始化" class="headerlink" title="AggregatorServer初始化"></a>AggregatorServer初始化</h3><p><code>Aggregator</code>通过<code>APIServices</code>对象关联到某个<code>Service</code>来进行请求的转发，其关联的<code>Service</code>类型进一步决定了请求转发形式。<code>Aggregator</code>包括一个<code>GenericAPIServer</code>和维护自身状态的<code>Controller</code>。其中 <code>GenericAPIServer</code>主要处理<code>apiregistration.k8s.io</code>组下的<code>APIService</code>资源请求。</p><p><code>Aggregator</code>除了处理资源请求外还包含几个controller：</p><ol><li>apiserviceRegistrationController：负责<code>APIServices</code>中资源的注册与删除；</li><li>availableConditionController：维护<code>APIServices</code>的可用状态，包括其引用<code>Service</code>是否可用等；</li><li>autoRegistrationController：用于保持API中存在的一组特定的<code>APIServices</code>；</li><li>crdRegistrationController：负责将<code>CRD GroupVersions</code>自动注册到<code>APIServices</code>中；</li><li>openAPIAggregationController：将<code>APIServices</code>资源的变化同步至提供的<code>OpenAPI</code>文档；<br>kubernetes中的一些附加组件，比如metrics-server就是通过Aggregator的方式进行扩展的，实际环境中可以通过使用apiserver-builder工具轻松以Aggregator的扩展方式创建自定义资源。</li></ol><p>初始化AggregatorServer的主要逻辑为：</p><ol><li>调用<code>aggregatorConfig.Complete().NewWithDelegate</code>创建<code>aggregatorServer</code></li><li>初始化<code>crdRegistrationController</code>和<code>autoRegistrationController</code>，<code>crdRegistrationController</code>负责注册CRD，<code>autoRegistrationController</code>负责将 CRD 对应的 APIServices自动注册到apiserver中，CRD 创建后可通过<code>$ kubectl get apiservices</code>查看是否注册到 apiservices中</li><li>将<code>autoRegistrationController</code>和<code>crdRegistrationController</code>加入到PostStartHook中</li></ol><p>首先，初始化配置<code>createAggregatorConfig</code><br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createAggregatorConfig</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">kubeAPIServerConfig genericapiserver.Config,</span></span></span><br><span class="line"><span class="function"><span class="params">commandOptions *options.ServerRunOptions,</span></span></span><br><span class="line"><span class="function"><span class="params">externalInformers kubeexternalinformers.SharedInformerFactory,</span></span></span><br><span class="line"><span class="function"><span class="params">serviceResolver aggregatorapiserver.ServiceResolver,</span></span></span><br><span class="line"><span class="function"><span class="params">proxyTransport *http.Transport,</span></span></span><br><span class="line"><span class="function"><span class="params">pluginInitializers []admission.PluginInitializer,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> <span class="params">(*aggregatorapiserver.Config, error)</span></span> {</span><br><span class="line"><span class="comment">// make a shallow copy to let us twiddle a few things</span></span><br><span class="line"><span class="comment">// most of the config actually remains the same.  We only need to mess with a couple items related to the particulars of the aggregator</span></span><br><span class="line">genericConfig := kubeAPIServerConfig</span><br><span class="line">genericConfig.PostStartHooks = <span class="keyword">map</span>[<span class="keyword">string</span>]genericapiserver.PostStartHookConfigEntry{}</span><br><span class="line">genericConfig.RESTOptionsGetter = <span class="literal">nil</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// override genericConfig.AdmissionControl with kube-aggregator's scheme,</span></span><br><span class="line"><span class="comment">// because aggregator apiserver should use its own scheme to convert its own resources.</span></span><br><span class="line"><span class="comment">// 取消admission的配置，aggregator自行处理请求，不需要admissions</span></span><br><span class="line">err := commandOptions.Admission.ApplyTo(</span><br><span class="line">&amp;genericConfig,</span><br><span class="line">externalInformers,</span><br><span class="line">genericConfig.LoopbackClientConfig,</span><br><span class="line">feature.DefaultFeatureGate,</span><br><span class="line">pluginInitializers...)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// copy the etcd options so we don't mutate originals.</span></span><br><span class="line">etcdOptions := *commandOptions.Etcd</span><br><span class="line">etcdOptions.StorageConfig.Paging = utilfeature.DefaultFeatureGate.Enabled(features.APIListChunking)</span><br><span class="line">etcdOptions.StorageConfig.Codec = aggregatorscheme.Codecs.LegacyCodec(v1beta1.SchemeGroupVersion, v1.SchemeGroupVersion)</span><br><span class="line">etcdOptions.StorageConfig.EncodeVersioner = runtime.NewMultiGroupVersioner(v1beta1.SchemeGroupVersion, schema.GroupKind{Group: v1beta1.GroupName})</span><br><span class="line">genericConfig.RESTOptionsGetter = &amp;genericoptions.SimpleRestOptionsFactory{Options: etcdOptions}</span><br><span class="line"></span><br><span class="line"><span class="comment">// override MergedResourceConfig with aggregator defaults and registry</span></span><br><span class="line"><span class="keyword">if</span> err := commandOptions.APIEnablement.ApplyTo(</span><br><span class="line">&amp;genericConfig,</span><br><span class="line">aggregatorapiserver.DefaultAPIResourceConfigSource(),</span><br><span class="line">aggregatorscheme.Scheme); err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 配置proxy证书，用于apiserver与扩展服务的通信，使用requestheader证书签发</span></span><br><span class="line"><span class="keyword">var</span> certBytes, keyBytes []<span class="keyword">byte</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(commandOptions.ProxyClientCertFile) &gt; <span class="number">0</span> &amp;&amp; <span class="built_in">len</span>(commandOptions.ProxyClientKeyFile) &gt; <span class="number">0</span> {</span><br><span class="line">certBytes, err = ioutil.ReadFile(commandOptions.ProxyClientCertFile)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">keyBytes, err = ioutil.ReadFile(commandOptions.ProxyClientKeyFile)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">aggregatorConfig := &amp;aggregatorapiserver.Config{</span><br><span class="line">GenericConfig: &amp;genericapiserver.RecommendedConfig{</span><br><span class="line">Config:                genericConfig,</span><br><span class="line">SharedInformerFactory: externalInformers,</span><br><span class="line">},</span><br><span class="line">ExtraConfig: aggregatorapiserver.ExtraConfig{</span><br><span class="line">ProxyClientCert: certBytes,</span><br><span class="line">ProxyClientKey:  keyBytes,</span><br><span class="line">ServiceResolver: serviceResolver,</span><br><span class="line"><span class="comment">// 代理请求的具体实现</span></span><br><span class="line">ProxyTransport:  proxyTransport,</span><br><span class="line">},</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// we need to clear the poststarthooks so we don't add them multiple times to all the servers (that fails)</span></span><br><span class="line"><span class="comment">// 加入PostStartHook</span></span><br><span class="line">aggregatorConfig.GenericConfig.PostStartHooks = <span class="keyword">map</span>[<span class="keyword">string</span>]genericapiserver.PostStartHookConfigEntry{}</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> aggregatorConfig, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p><code>createAggregatorServer</code>初始化<code>Aggregator</code><br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">createAggregatorServer</span><span class="params">(aggregatorConfig *aggregatorapiserver.Config, delegateAPIServer genericapiserver.DelegationTarget, apiExtensionInformers apiextensionsinformers.SharedInformerFactory)</span> <span class="params">(*aggregatorapiserver.APIAggregator, error)</span></span> {</span><br><span class="line"><span class="comment">// 初始化配置，与前面流程相同</span></span><br><span class="line">aggregatorServer, err := aggregatorConfig.Complete().NewWithDelegate(delegateAPIServer)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建auto-registration controller</span></span><br><span class="line">apiRegistrationClient, err := apiregistrationclient.NewForConfig(aggregatorConfig.GenericConfig.LoopbackClientConfig)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">autoRegistrationController := autoregister.NewAutoRegisterController(aggregatorServer.APIRegistrationInformers.Apiregistration().V1().APIServices(), apiRegistrationClient)</span><br><span class="line">apiServices := apiServicesToRegister(delegateAPIServer, autoRegistrationController)</span><br><span class="line">crdRegistrationController := crdregistration.NewCRDRegistrationController(</span><br><span class="line">apiExtensionInformers.Apiextensions().V1().CustomResourceDefinitions(),</span><br><span class="line">autoRegistrationController)</span><br><span class="line"></span><br><span class="line">err = aggregatorServer.GenericAPIServer.AddPostStartHook(<span class="string">"kube-apiserver-autoregistration"</span>, <span class="function"><span class="keyword">func</span><span class="params">(context genericapiserver.PostStartHookContext)</span> <span class="title">error</span></span> {</span><br><span class="line"><span class="comment">// 启动controller</span></span><br><span class="line"><span class="keyword">go</span> crdRegistrationController.Run(<span class="number">5</span>, context.StopCh)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line"><span class="comment">// let the CRD controller process the initial set of CRDs before starting the autoregistration controller.</span></span><br><span class="line"><span class="comment">// this prevents the autoregistration controller's initial sync from deleting APIServices for CRDs that still exist.</span></span><br><span class="line"><span class="comment">// we only need to do this if CRDs are enabled on this server.  We can't use discovery because we are the source for discovery.</span></span><br><span class="line"><span class="keyword">if</span> aggregatorConfig.GenericConfig.MergedResourceConfig.AnyVersionForGroupEnabled(<span class="string">"apiextensions.k8s.io"</span>) {</span><br><span class="line">crdRegistrationController.WaitForInitialSync()</span><br><span class="line">}</span><br><span class="line">autoRegistrationController.Run(<span class="number">5</span>, context.StopCh)</span><br><span class="line">}()</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">})</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">err = aggregatorServer.GenericAPIServer.AddBootSequenceHealthChecks(</span><br><span class="line">makeAPIServiceAvailableHealthCheck(</span><br><span class="line"><span class="string">"autoregister-completion"</span>,</span><br><span class="line">apiServices,</span><br><span class="line">aggregatorServer.APIRegistrationInformers.Apiregistration().V1().APIServices(),</span><br><span class="line">),</span><br><span class="line">)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> aggregatorServer, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>至此，启动步骤以前分析完了，三个组件的流量大体时一样的，通过<code>Complete().New()</code>初始化配置，创建所需的controller, 调用<code>InstallAPIGroup</code>注册<code>apigroup</code>。</p><h2 id="请求分析"><a href="#请求分析" class="headerlink" title="请求分析"></a>请求分析</h2><p>上面我们分析了apiserver的调用链，大体如下<br><code>DefaultHandlerChain-&gt;{handler/crdhandler/proxy}-&gt;admission-&gt;validation-&gt;etcd</code></p><ol><li>请求进入时，会经过<code>defaultchain</code>做一些认证鉴权工作</li><li>然后通过<code>route</code>执行对应的handler，如果为aggration api, 将直接转发请求到对应service</li><li>handler处理完，经过admission与validation，做一些修改和检查，用户在这部分可以自定义webhook</li><li>最后存入etcd</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文大体对apiserver的启动流程，以及初始化过程做了分析，由于apiserver实现复杂，中间一些细节没涉及到，还需要对着代码研究研究。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://juejin.im/post/5c934e5a5188252d7c216981" target="_blank" rel="noopener">https://juejin.im/post/5c934e5a5188252d7c216981</a><br>[2] <a href="https://blog.tianfeiyu.com/2020/02/24/kube_apiserver/" target="_blank" rel="noopener">https://blog.tianfeiyu.com/2020/02/24/kube_apiserver/</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> apiserver </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kube-apiserver鉴权源码分析</title>
      <link href="/kube-apiserver-authorization-code/"/>
      <url>/kube-apiserver-authorization-code/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>kube-apiserver中与权限相关的主要有三种机制，即认证、鉴权和准入控制。上节讲到<a href="./kube-apiserver-authentication-code.md">认证流程</a>。</p><p>认证与授权很容易混淆：</p><ul><li>认证(Authentication), 负责检查你是谁，识别user</li><li>授权(Authorization), 你能做什么，是否允许User对资源的操作</li><li>审计(Audit), 负责记录操作信息，方便后续审查</li></ul><p>本文主要分析apiserver的rbac授权流程。</p><h2 id="认证流程分析"><a href="#认证流程分析" class="headerlink" title="认证流程分析"></a>认证流程分析</h2><p>权限相关代码从<code>k8s.io/apiserver/pkg/server/config.go</code>中<code>DefaultBuildHandlerChain</code>函数开始执行</p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DefaultBuildHandlerChain</span><span class="params">(apiHandler http.Handler, c *Config)</span> <span class="title">http</span>.<span class="title">Handler</span></span> {</span><br><span class="line">handler := genericapifilters.WithAuthorization(apiHandler, c.Authorization.Authorizer, c.Serializer)</span><br><span class="line">handler = genericfilters.WithMaxInFlightLimit(handler, c.MaxRequestsInFlight, c.MaxMutatingRequestsInFlight, c.LongRunningFunc)</span><br><span class="line">handler = genericapifilters.WithImpersonation(handler, c.Authorization.Authorizer, c.Serializer)</span><br><span class="line">handler = genericapifilters.WithAudit(handler, c.AuditBackend, c.AuditPolicyChecker, c.LongRunningFunc)</span><br><span class="line">failedHandler := genericapifilters.Unauthorized(c.Serializer, c.Authentication.SupportsBasicAuth)</span><br><span class="line">failedHandler = genericapifilters.WithFailedAuthenticationAudit(failedHandler, c.AuditBackend, c.AuditPolicyChecker)</span><br><span class="line">handler = genericapifilters.WithAuthentication(handler, c.Authentication.Authenticator, failedHandler, c.Authentication.APIAudiences)</span><br><span class="line">handler = genericfilters.WithCORS(handler, c.CorsAllowedOriginList, <span class="literal">nil</span>, <span class="literal">nil</span>, <span class="literal">nil</span>, <span class="string">"true"</span>)</span><br><span class="line">handler = genericfilters.WithTimeoutForNonLongRunningRequests(handler, c.LongRunningFunc, c.RequestTimeout)</span><br><span class="line">handler = genericfilters.WithWaitGroup(handler, c.LongRunningFunc, c.HandlerChainWaitGroup)</span><br><span class="line">handler = genericapifilters.WithRequestInfo(handler, c.RequestInfoResolver)</span><br><span class="line">handler = genericfilters.WithPanicRecovery(handler)</span><br><span class="line"><span class="keyword">return</span> handler</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><code>DefaultBuildHandlerChain</code>中包含了多种filter（如认证，链接数检验，RBAC权限检验等），授权步骤在<code>WithAuthorization</code>中，如下：</p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WithAuthorizationCheck passes all authorized requests on to handler, and returns a forbidden error otherwise.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithAuthorization</span><span class="params">(handler http.Handler, a authorizer.Authorizer, s runtime.NegotiatedSerializer)</span> <span class="title">http</span>.<span class="title">Handler</span></span> {</span><br><span class="line"><span class="comment">// 检查是否需要权限校验</span></span><br><span class="line"><span class="keyword">if</span> a == <span class="literal">nil</span> {</span><br><span class="line">klog.Warningf(<span class="string">"Authorization is disabled"</span>)</span><br><span class="line"><span class="keyword">return</span> handler</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> http.HandlerFunc(<span class="function"><span class="keyword">func</span><span class="params">(w http.ResponseWriter, req *http.Request)</span></span> {</span><br><span class="line">ctx := req.Context()</span><br><span class="line"><span class="comment">// 用作审计</span></span><br><span class="line">ae := request.AuditEventFrom(ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取Attribute, 通过reqeust获取到请求的user, resource, verb, 是否为namespace级别的等</span></span><br><span class="line">attributes, err := GetAuthorizerAttributes(ctx)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">responsewriters.InternalError(w, req, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"><span class="comment">// 执行认证流程</span></span><br><span class="line">authorized, reason, err := a.Authorize(ctx, attributes)</span><br><span class="line"><span class="comment">// an authorizer like RBAC could encounter evaluation errors and still allow the request, so authorizer decision is checked before error here.</span></span><br><span class="line"><span class="keyword">if</span> authorized == authorizer.DecisionAllow {</span><br><span class="line">audit.LogAnnotation(ae, decisionAnnotationKey, decisionAllow)</span><br><span class="line">audit.LogAnnotation(ae, reasonAnnotationKey, reason)</span><br><span class="line"><span class="comment">// 校验成功，记录信息，转到下一个handler</span></span><br><span class="line">handler.ServeHTTP(w, req)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">audit.LogAnnotation(ae, reasonAnnotationKey, reasonError)</span><br><span class="line">responsewriters.InternalError(w, req, err)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 校验失败返回403，注意认证失败返回的是401</span></span><br><span class="line">klog.V(<span class="number">4</span>).Infof(<span class="string">"Forbidden: %#v, Reason: %q"</span>, req.RequestURI, reason)</span><br><span class="line">audit.LogAnnotation(ae, decisionAnnotationKey, decisionForbid)</span><br><span class="line">audit.LogAnnotation(ae, reasonAnnotationKey, reason)</span><br><span class="line">responsewriters.Forbidden(ctx, attributes, w, req, reason, s)</span><br><span class="line">})</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>授权流程比较清晰，从request获取请求信息，进行鉴权，成功进入后续handler，失败返回403。</p><p><code>Authorize</code>接口有多种实现，通过在apiserver配置<code>--authorization-mode</code>选择鉴权模式，包括：</p><ul><li>ABAC</li><li>RBAC</li><li>Node, 用于kubelet鉴权exec/logs等</li><li>AlwaysAllow</li><li>AlwaysDeny</li><li>Webhook， 用于扩展权限，用户可实现Webhook与其他权限系统集成</li></ul><p>如果选择<code>AlwaysAllow</code>,即不做鉴权, 开启后强制不允许匿名用户<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ApplyAuthorization will conditionally modify the authentication options based on the authorization options</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(o *BuiltInAuthenticationOptions)</span> <span class="title">ApplyAuthorization</span><span class="params">(authorization *BuiltInAuthorizationOptions)</span></span> {</span><br><span class="line"><span class="keyword">if</span> o == <span class="literal">nil</span> || authorization == <span class="literal">nil</span> || o.Anonymous == <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// authorization ModeAlwaysAllow cannot be combined with AnonymousAuth.</span></span><br><span class="line"><span class="comment">// in such a case the AnonymousAuth is stomped to false and you get a message</span></span><br><span class="line"><span class="keyword">if</span> o.Anonymous.Allow &amp;&amp; sets.NewString(authorization.Modes...).Has(authzmodes.ModeAlwaysAllow) {</span><br><span class="line">klog.Warningf(<span class="string">"AnonymousAuth is not allowed with the AlwaysAllow authorizer. Resetting AnonymousAuth to false. You should use a different authorizer"</span>)</span><br><span class="line">o.Anonymous.Allow = <span class="literal">false</span></span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="rbac鉴权"><a href="#rbac鉴权" class="headerlink" title="rbac鉴权"></a>rbac鉴权</h2><p>rbac是常用的鉴权方式，实现<code>Authorize</code>接口, 代码在<a href="https://github.com/kubernetes/kubernetes/blob/92eb072989eba22236d034b56cc2bf159dfb4915/plugin/pkg/auth/authorizer/rbac/rbac.go#L75" target="_blank" rel="noopener">rbac.go</a><br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *RBACAuthorizer)</span> <span class="title">Authorize</span><span class="params">(ctx context.Context, requestAttributes authorizer.Attributes)</span> <span class="params">(authorizer.Decision, <span class="keyword">string</span>, error)</span></span> {</span><br><span class="line">ruleCheckingVisitor := &amp;authorizingVisitor{requestAttributes: requestAttributes}</span><br><span class="line"><span class="comment">// 调用VisitRulesFor来检查是否用权限</span></span><br><span class="line">r.authorizationRuleResolver.VisitRulesFor(requestAttributes.GetUser(), requestAttributes.GetNamespace(), ruleCheckingVisitor.visit)</span><br><span class="line"><span class="keyword">if</span> ruleCheckingVisitor.allowed {</span><br><span class="line"><span class="comment">// 成功直接返回</span></span><br><span class="line"><span class="keyword">return</span> authorizer.DecisionAllow, ruleCheckingVisitor.reason, <span class="literal">nil</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 失败，打印日志返回失败原因</span></span><br><span class="line"><span class="comment">// Build a detailed log of the denial.</span></span><br><span class="line"><span class="comment">// Make the whole block conditional so we don't do a lot of string-building we won't use.</span></span><br><span class="line"><span class="keyword">if</span> klog.V(<span class="number">5</span>) {</span><br><span class="line"><span class="keyword">var</span> operation <span class="keyword">string</span></span><br><span class="line"><span class="keyword">if</span> requestAttributes.IsResourceRequest() {</span><br><span class="line">b := &amp;bytes.Buffer{}</span><br><span class="line">b.WriteString(<span class="string">`"`</span>)</span><br><span class="line">b.WriteString(requestAttributes.GetVerb())</span><br><span class="line">b.WriteString(<span class="string">`" resource "`</span>)</span><br><span class="line">b.WriteString(requestAttributes.GetResource())</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(requestAttributes.GetAPIGroup()) &gt; <span class="number">0</span> {</span><br><span class="line">b.WriteString(<span class="string">`.`</span>)</span><br><span class="line">b.WriteString(requestAttributes.GetAPIGroup())</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(requestAttributes.GetSubresource()) &gt; <span class="number">0</span> {</span><br><span class="line">b.WriteString(<span class="string">`/`</span>)</span><br><span class="line">b.WriteString(requestAttributes.GetSubresource())</span><br><span class="line">}</span><br><span class="line">b.WriteString(<span class="string">`"`</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(requestAttributes.GetName()) &gt; <span class="number">0</span> {</span><br><span class="line">b.WriteString(<span class="string">` named "`</span>)</span><br><span class="line">b.WriteString(requestAttributes.GetName())</span><br><span class="line">b.WriteString(<span class="string">`"`</span>)</span><br><span class="line">}</span><br><span class="line">operation = b.String()</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">operation = fmt.Sprintf(<span class="string">"%q nonResourceURL %q"</span>, requestAttributes.GetVerb(), requestAttributes.GetPath())</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> scope <span class="keyword">string</span></span><br><span class="line"><span class="keyword">if</span> ns := requestAttributes.GetNamespace(); <span class="built_in">len</span>(ns) &gt; <span class="number">0</span> {</span><br><span class="line">scope = fmt.Sprintf(<span class="string">"in namespace %q"</span>, ns)</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">scope = <span class="string">"cluster-wide"</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">klog.Infof(<span class="string">"RBAC DENY: user %q groups %q cannot %s %s"</span>, requestAttributes.GetUser().GetName(), requestAttributes.GetUser().GetGroups(), operation, scope)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">reason := <span class="string">""</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(ruleCheckingVisitor.errors) &gt; <span class="number">0</span> {</span><br><span class="line">reason = fmt.Sprintf(<span class="string">"RBAC: %v"</span>, utilerrors.NewAggregate(ruleCheckingVisitor.errors))</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> authorizer.DecisionNoOpinion, reason, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p><code>Authorize</code>调用了<code>VisitRulesFor</code>来处理具体鉴权操作, 代码在<a href="https://github.com/kubernetes/kubernetes/blob/81e9f21f832f88422f1ccf5b8aa90de7cf822132/pkg/registry/rbac/validation/rule.go#L178" target="_blank" rel="noopener">rule.go</a><br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *DefaultRuleResolver)</span> <span class="title">VisitRulesFor</span><span class="params">(user user.Info, namespace <span class="keyword">string</span>, visitor <span class="keyword">func</span>(source fmt.Stringer, rule *rbacv1.PolicyRule, err error)</span> <span class="title">bool</span>)</span> {</span><br><span class="line"><span class="comment">// 获取所有clusterrolebinding</span></span><br><span class="line"><span class="keyword">if</span> clusterRoleBindings, err := r.clusterRoleBindingLister.ListClusterRoleBindings(); err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">if</span> !visitor(<span class="literal">nil</span>, <span class="literal">nil</span>, err) {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">sourceDescriber := &amp;clusterRoleBindingDescriber{}</span><br><span class="line"><span class="comment">// 遍历clusterrolebing</span></span><br><span class="line"><span class="keyword">for</span> _, clusterRoleBinding := <span class="keyword">range</span> clusterRoleBindings {</span><br><span class="line"><span class="comment">// 检查是否有对应的user</span></span><br><span class="line">subjectIndex, applies := appliesTo(user, clusterRoleBinding.Subjects, <span class="string">""</span>)</span><br><span class="line"><span class="keyword">if</span> !applies {</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">}</span><br><span class="line"><span class="comment">// 如果user存在于subject, 获取对应的rules即clusterrole</span></span><br><span class="line">rules, err := r.GetRoleReferenceRules(clusterRoleBinding.RoleRef, <span class="string">""</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">if</span> !visitor(<span class="literal">nil</span>, <span class="literal">nil</span>, err) {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">}</span><br><span class="line">sourceDescriber.binding = clusterRoleBinding</span><br><span class="line">sourceDescriber.subject = &amp;clusterRoleBinding.Subjects[subjectIndex]</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> rules {</span><br><span class="line"><span class="comment">// 调用visitor判断是否需要进入下一步鉴权</span></span><br><span class="line"><span class="keyword">if</span> !visitor(sourceDescriber, &amp;rules[i], <span class="literal">nil</span>) {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// clusterrole遍历完还没有鉴权成功，接着遍历所在namespace的role，流程同上</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(namespace) &gt; <span class="number">0</span> {</span><br><span class="line"><span class="keyword">if</span> roleBindings, err := r.roleBindingLister.ListRoleBindings(namespace); err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">if</span> !visitor(<span class="literal">nil</span>, <span class="literal">nil</span>, err) {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">sourceDescriber := &amp;roleBindingDescriber{}</span><br><span class="line"><span class="keyword">for</span> _, roleBinding := <span class="keyword">range</span> roleBindings {</span><br><span class="line">subjectIndex, applies := appliesTo(user, roleBinding.Subjects, namespace)</span><br><span class="line"><span class="keyword">if</span> !applies {</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">}</span><br><span class="line">rules, err := r.GetRoleReferenceRules(roleBinding.RoleRef, namespace)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">if</span> !visitor(<span class="literal">nil</span>, <span class="literal">nil</span>, err) {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">continue</span></span><br><span class="line">}</span><br><span class="line">sourceDescriber.binding = roleBinding</span><br><span class="line">sourceDescriber.subject = &amp;roleBinding.Subjects[subjectIndex]</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> rules {</span><br><span class="line"><span class="keyword">if</span> !visitor(sourceDescriber, &amp;rules[i], <span class="literal">nil</span>) {</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p><code>visit</code>函数, 用来判断是否认证成功，成功返回<code>false</code>, 不需要进行下一步鉴权<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v *authorizingVisitor)</span> <span class="title">visit</span><span class="params">(source fmt.Stringer, rule *rbacv1.PolicyRule, err error)</span> <span class="title">bool</span></span> {</span><br><span class="line"><span class="keyword">if</span> rule != <span class="literal">nil</span> &amp;&amp; RuleAllows(v.requestAttributes, rule) {</span><br><span class="line"><span class="comment">// allowed用来表示是否认证成功</span></span><br><span class="line">v.allowed = <span class="literal">true</span></span><br><span class="line">v.reason = fmt.Sprintf(<span class="string">"RBAC: allowed by %s"</span>, source.String())</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">v.errors = <span class="built_in">append</span>(v.errors, err)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>rbac的鉴权流程如下:</p><ol><li>通过<code>Request</code>获取<code>Attribute</code>包括用户，资源和对应的操作</li><li><code>Authorize</code>调用<code>VisitRulesFor</code>进行具体的鉴权</li><li>获取所有的ClusterRoleBindings，并对其进行遍历操作</li><li>根据请求User信息，判断该是否被绑定在该ClusterRoleBinding中</li><li>若在将通过函数<code>GetRoleReferenceRules()</code>获取绑定的Role所控制的访问的资源</li><li>将Role所控制的访问的资源，与从API请求中提取出的资源进行比对，若比对成功，即为API请求的调用者有权访问相关资源</li><li>遍历ClusterRoleBinding中，都没有获得鉴权成功的操作，将会判断提取出的信息中是否包括了namespace的信息，若包括了，将会获取该namespace下的所有RoleBindings，类似ClusterRoleBindings</li><li>若在遍历了所有CluterRoleBindings，及该namespace下的所有RoleBingdings之后，仍没有对资源比对成功，则可判断该API请求的调用者没有权限访问相关资源, 鉴权失败</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文结合RBAC分析了Kubernetes的鉴权流程，整体这部分比较代码清晰。RBAC是Kubernetes比较推荐的鉴权方式，了解完整个流程后，居然所有请求都会先遍历一遍ClusterRoleBindings，这样实现起来比较简单，但随着规模和用户的扩大，这部分是否会有性能问题，需不需要实现能够快速鉴权的方式。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> rbac </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多端口服务的Ingress IP-hash问题</title>
      <link href="/ingress-ip-hash/"/>
      <url>/ingress-ip-hash/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>业务反馈使用Ingress的ip-hash, 同一个服务开启了http和websocket分别是两个端口, 但是配置ip-hash后, 同一个client的请求http和websocket不在同一个后端.</p><h2 id="探究"><a href="#探究" class="headerlink" title="探究"></a>探究</h2><p>根据业务Ingress配置,配置如下实例:<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/cors-allow-origin:</span> <span class="string">'*'</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/enable-cors:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-body-size:</span> <span class="number">200</span><span class="string">m</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/proxy-read-timeout:</span> <span class="string">"300"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/upstream-hash-by:</span> <span class="string">$binary_remote_addr</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">hellogo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">hellogo.d.xiaomi.net</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">hellogo</span> <span class="comment">#http1, 8080</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/8080</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">hellogo</span> <span class="comment">#http2, 9090</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">9090</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/9090</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">hellogo</span> <span class="comment">#websocket, 8081</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8081</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/ws</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>创建多个副本<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get po -l app=hellogo</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE</span><br><span class="line">hellogo-699f997454-b5vs4   1/1     Running   0          66m</span><br><span class="line">hellogo-699f997454-hm924   1/1     Running   0          66m</span><br><span class="line">hellogo-699f997454-mfbqv   1/1     Running   0          66m</span><br><span class="line">hellogo-699f997454-qdrwn   1/1     Running   0          66m</span><br><span class="line">hellogo-699f997454-srh9b   1/1     Running   0          66m</span><br><span class="line">hellogo-699f997454-wlwfh   1/1     Running   0          66m</span><br></pre></td></tr></tbody></table></figure><p></p><p>测试http 8080端口, 请求到pod hellogo-699f997454-qdrwn<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://hellogo.d.xiaomi.net/8080</span><br><span class="line">hello 8080!</span><br><span class="line">host hellogo.d.xiaomi.net</span><br><span class="line">remoteaddr 10.46.23.1:15340</span><br><span class="line">realip 10.232.41.102</span><br><span class="line">hostname hellogo-699f997454-qdrwn </span><br><span class="line"></span><br><span class="line">$ curl http://hellogo.d.xiaomi.net/8080</span><br><span class="line">hello 8080!</span><br><span class="line">host hellogo.d.xiaomi.net</span><br><span class="line">remoteaddr 10.46.23.1:15866</span><br><span class="line">realip 10.232.41.102</span><br><span class="line">hostname hellogo-699f997454-qdrwn</span><br></pre></td></tr></tbody></table></figure><p></p><p>测试http 8080端口, 请求到pod hellogo-699f997454-b5vs4<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://hellogo.d.xiaomi.net/9090</span><br><span class="line">hello 9090!</span><br><span class="line">host hellogo.d.xiaomi.net</span><br><span class="line">remoteaddr 10.38.200.195:23706</span><br><span class="line">realip 10.232.41.102</span><br><span class="line">hostname hellogo-699f997454-b5vs4</span><br><span class="line"></span><br><span class="line">$ curl http://hellogo.d.xiaomi.net/9090</span><br><span class="line">hello 9090!</span><br><span class="line">host hellogo.d.xiaomi.net</span><br><span class="line">remoteaddr 10.38.200.195:23706</span><br><span class="line">realip 10.232.41.102</span><br><span class="line">hostname hellogo-699f997454-b5vs4</span><br></pre></td></tr></tbody></table></figure><p></p><p>猜想是由于获取的nginx server列表顺序不一致导致的, 但是看源码ip list是直接从endpoint获取的, 进入nginx-ingress查看<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">exec</span> -it -n kube-system nginx-ingress-controller-m496n sh</span><br><span class="line"><span class="comment"># dbg工具查看nginx后端列表</span></span><br><span class="line">/etc/nginx $ /dbg backends list | grep hellogo</span><br><span class="line">default-hellogo-8080</span><br><span class="line">default-hellogo-8081</span><br><span class="line">default-hellogo-9090</span><br><span class="line"><span class="comment"># 8080端口的列表</span></span><br><span class="line">/etc/nginx $ /dbg backends get default-hellogo-8080</span><br><span class="line">{</span><br><span class="line">  <span class="string">"endpoints"</span>: [</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.12.107"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"8080"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.12.108"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"8080"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.12.109"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"8080"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.23.23"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"8080"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.23.25"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"8080"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.23.29"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"8080"</span></span><br><span class="line">    }</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"default-hellogo-8080"</span>,</span><br><span class="line">  <span class="string">"noServer"</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"port"</span>: 8080,</span><br><span class="line">  ...</span><br><span class="line">}</span><br><span class="line"><span class="comment"># 9090端口的列表</span></span><br><span class="line">/etc/nginx $ /dbg backends get default-hellogo-9090</span><br><span class="line">{</span><br><span class="line">  <span class="string">"endpoints"</span>: [</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.12.107"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"9090"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.12.108"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"9090"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.12.109"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"9090"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.23.23"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"9090"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.23.25"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"9090"</span></span><br><span class="line">    },</span><br><span class="line">    {</span><br><span class="line">      <span class="string">"address"</span>: <span class="string">"10.46.23.29"</span>,</span><br><span class="line">      <span class="string">"port"</span>: <span class="string">"9090"</span></span><br><span class="line">    }</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"name"</span>: <span class="string">"default-hellogo-9090"</span>,</span><br><span class="line">  <span class="string">"noServer"</span>: <span class="literal">false</span>,</span><br><span class="line">  <span class="string">"port"</span>: 9090,</span><br><span class="line">  ...</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>对比发现两个端口的列表是一样的,只能看看代码.</p><p>ip-hash代码在<a href="https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/balancer/chash.lua" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/balancer/chash.lua</a><br></p><figure class="highlight lua"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> _M.new<span class="params">(self, backend)</span></span></span><br><span class="line">  <span class="keyword">local</span> nodes = util.get_nodes(backend.endpoints)</span><br><span class="line">  <span class="keyword">local</span> o = {</span><br><span class="line">    instance = self.factory:new(nodes),  <span class="comment">--获取后端pod ip列表</span></span><br><span class="line">    hash_by = backend[<span class="string">"upstreamHashByConfig"</span>][<span class="string">"upstream-hash-by"</span>],</span><br><span class="line">    traffic_shaping_policy = backend.trafficShapingPolicy,</span><br><span class="line">    alternative_backends = backend.alternativeBackends,</span><br><span class="line">  }</span><br><span class="line">  <span class="built_in">setmetatable</span>(o, self)</span><br><span class="line">  self.<span class="built_in">__index</span> = self</span><br><span class="line">  <span class="keyword">return</span> o</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> _M.balance<span class="params">(self)</span></span></span><br><span class="line">  <span class="keyword">local</span> key = util.lua_ngx_var(self.hash_by) <span class="comment">--获取需要hash的变量</span></span><br><span class="line">  <span class="keyword">return</span> self.instance:<span class="built_in">find</span>(key)  <span class="comment">--计算hash值</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> _M</span><br></pre></td></tr></tbody></table></figure><p></p><p>关键是在<code>get_nodes</code>函数,位于<a href="https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util.lua" target="_blank" rel="noopener">https://github.com/kubernetes/ingress-nginx/blob/master/rootfs/etc/nginx/lua/util.lua</a><br></p><figure class="highlight lua"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> _M.get_nodes<span class="params">(endpoints)</span></span></span><br><span class="line">  <span class="keyword">local</span> nodes = {}</span><br><span class="line">  <span class="keyword">local</span> weight = <span class="number">1</span> <span class="comment">--所有后端weight相同都为1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> _, endpoint <span class="keyword">in</span> <span class="built_in">pairs</span>(endpoints) <span class="keyword">do</span></span><br><span class="line">    <span class="keyword">local</span> endpoint_string = endpoint.address .. <span class="string">":"</span> .. endpoint.port <span class="comment">--endpoint为ip+port</span></span><br><span class="line">    nodes[endpoint_string] = weight</span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> nodes</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>通过代码可以看到在<code>ingress-nginx</code>中,实际的后端(upstream)是包含端口的,通过hash计算得到的值也不一样。</p><h2 id="解决建议"><a href="#解决建议" class="headerlink" title="解决建议"></a>解决建议</h2><p>首先确认系统的架构是不是合理，不同的端口提供不同的服务，一般是相互独立的。<br>如果确实有类似需求：</p><ul><li>通过同一个端口提供服务，使用path来区分不同功能</li><li>修改代码，也比较简单</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> ingress </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何做一个优雅的Pod</title>
      <link href="/pod-graceful-lifecycle/"/>
      <url>/pod-graceful-lifecycle/</url>
      
        <content type="html"><![CDATA[<p>没有人不想优雅的活着，在这喧闹的生活中过得优雅从容并不容易。但在k8s的世界中，如何做个优雅的Pod还是有套路可循的。</p><h2 id="Pod的生命周期"><a href="#Pod的生命周期" class="headerlink" title="Pod的生命周期"></a>Pod的生命周期</h2><p>在优雅之前，我们先谈谈Pod的一生，大体分为以下几个阶段</p><ol><li>创建，通过kubectl或者api创建pod, apiserver收到请求后存储到etcd</li><li>调度，scheduler检测到pod创建后，通过预选优选为pod选取合适的人家(node)</li><li>启动，kubelet检测到有pod调度到当前节点，开始启动pod</li><li>终止，不同的pod有不同的谢幕方式，有的正常运行结束没有restart就completed，有的被kill就入土为安了，有的被驱逐换种方式重新开始</li></ol><p>今天我们主要讨论3-4阶段，前面部分更多是deployment/daemonset这些pod的父母所决定的。</p><h2 id="优雅的启动"><a href="#优雅的启动" class="headerlink" title="优雅的启动"></a>优雅的启动</h2><h3 id="init-container"><a href="#init-container" class="headerlink" title="init container"></a>init container</h3><p>通常pod有一些初始化操作，创建文件夹，初始化磁盘，检查某些依赖服务是不是正常，这些操作放在代码中会污染代码，写在启动命令中不方便管理，出问题也不方便排查，更优雅的方式是使用k8s的[init container][1]。</p><p><strong>理解 Init 容器</strong><br>Pod 可以包含多个容器，应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。</p><p>Init 容器与普通的容器非常像，除了如下两点：</p><ul><li>它们总是运行到完成。</li><li>每个都必须在下一个启动之前成功完成。<br>如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 值为 Never，它不会重新启动。</li></ul><p>如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。</p><p><strong>Init 容器能做什么？</strong><br>因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：</p><ul><li>Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。例如，没有必要仅为了在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具而去FROM 一个镜像来生成一个新的镜像。</li><li>Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。<br>应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。<br>Init 容器能以不同于Pod内应用容器的文件系统视图运行。因此，Init容器可具有访问 Secrets 的权限，而应用容器不能够访问。</li><li>由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。一旦前置条件满足，Pod内的所有的应用容器会并行启动。</li></ul><p><strong>示例</strong><br>下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 myservice 启动，第二个等待 mydb 启动。 一旦这两个 Init容器 都启动完成，Pod 将启动spec区域中的应用容器。<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">myapp-pod</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">myapp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">myapp-container</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">busybox:1.28</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">['sh',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">'echo The app is running! &amp;&amp; sleep 3600'</span><span class="string">]</span></span><br><span class="line"><span class="attr">  initContainers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">init-myservice</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">busybox:1.28</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">['sh',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"</span><span class="string">]</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">init-mydb</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">busybox:1.28</span></span><br><span class="line"><span class="attr">    command:</span> <span class="string">['sh',</span> <span class="string">'-c'</span><span class="string">,</span> <span class="string">"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"</span><span class="string">]</span></span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="readinessProbe"><a href="#readinessProbe" class="headerlink" title="readinessProbe"></a>readinessProbe</h3><p>pod启动后，如果直接加入endpoint，有可能服务还没初始化完成，端口没有就绪，这时候接收流量肯定无法正常处理。如果能判断pod是否ready就好了，当当当，readiness来了，可以通过http，tcp以及执行命令的方式来检查服务情况，检查成功后再将pod状态设置为ready,ready后才会加入到endpoint中。</p><p>下为一个readiness探测，5秒执行一次命令，执行成功则pod变为ready<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">readinessProbe:</span></span><br><span class="line"><span class="attr">  exec:</span></span><br><span class="line"><span class="attr">    command:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">cat</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">/tmp/healthy</span></span><br><span class="line"><span class="attr">  initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">  periodSeconds:</span> <span class="number">5</span></span><br></pre></td></tr></tbody></table></figure><p></p><blockquote><p><strong>注</strong></p><ul><li>http, tcp探针是kubelet执行的，所以无法探测容器中localhost的端口，也无法解析service</li><li>exec则在容器内执行的</li></ul></blockquote><h3 id="ReadinessGates"><a href="#ReadinessGates" class="headerlink" title="ReadinessGates"></a>ReadinessGates</h3><p>ReadinessProbe机制可能无法满足某些复杂应用对容器内服务可用状态的判断，所以kubernetes从1.11版本开始引入了<code>Pod Ready++</code>特性对Readiness探测机制进行扩展，在1.14版本时达到GA稳定版本，称其为<code>Pod Readiness Gates</code>。</p><p>通过Pod Readiness Gates机制，用户可以将自定义的ReadinessProbe探测方式设置在Pod上，辅助kubernetes设置Pod何时达到服务可用状态Ready，为了使自定义的ReadinessProbe生效，用户需要提供一个外部的控制器Controller来设置相应的Condition状态。Pod的Readiness Gates在pod定义中的ReadinessGates字段进行设置，</p><p>如下示例设置了一个类型为<code>www.example.com/feature-1</code>的新Readiness Gates：<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  readinessGates:</span></span><br><span class="line"><span class="attr">    - conditionType:</span> <span class="string">"www.example.com/feature-1"</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">  conditions:</span></span><br><span class="line"><span class="attr">    - type:</span> <span class="string">Ready</span>  <span class="comment"># kubernetes系统内置的名为Ready的Condition</span></span><br><span class="line"><span class="attr">      status:</span> <span class="string">"True"</span></span><br><span class="line"><span class="attr">      lastProbeTime:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">      lastTransitionTime:</span> <span class="number">2018</span><span class="bullet">-01</span><span class="bullet">-01</span><span class="attr">T00:00:00Z</span></span><br><span class="line"><span class="attr">    - type:</span> <span class="string">"www.example.com/feature-1"</span>   <span class="comment"># 用户定义的Condition</span></span><br><span class="line"><span class="attr">      status:</span> <span class="string">"False"</span></span><br><span class="line"><span class="attr">      lastProbeTime:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">      lastTransitionTime:</span> <span class="number">2018</span><span class="bullet">-01</span><span class="bullet">-01</span><span class="attr">T00:00:00Z</span></span><br><span class="line"><span class="attr">  containerStatuses:</span></span><br><span class="line"><span class="attr">    - containerID:</span> <span class="attr">docker://abcd...</span></span><br><span class="line"><span class="attr">      ready:</span> <span class="literal">true</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>新增的自定义Condition的状态status将由用户自定义的外部控制器设置，默认值为False，kubernetes将在判断全部readinessGates条件都为True时，才设置pod为服务可用状态（Ready或True）。</p><h3 id="poststart"><a href="#poststart" class="headerlink" title="poststart"></a>poststart</h3><p>另外也可以通过<code>poststart</code>设置hook操作，做一些额外工作。k8s在容器创建后立即发送 postStart 事件。然而，postStart 处理函数的调用不保证早于容器的入口点（entrypoint） 的执行。postStart 处理函数与容器的代码是异步执行的，但 Kubernetes 的容器管理逻辑会一直阻塞等待 postStart 处理函数执行完毕。只有 postStart 处理函数执行完毕，容器的状态才会变成<code>RUNNING</code>。</p><h2 id="优雅的运行"><a href="#优雅的运行" class="headerlink" title="优雅的运行"></a>优雅的运行</h2><h3 id="livenessProbe"><a href="#livenessProbe" class="headerlink" title="livenessProbe"></a>livenessProbe</h3><p>同readinessProbe探针，livenessProbe是用来检查pod运行状态是否正常，如果探测失败，pod被kill掉，重启启动pod。</p><h3 id="restartpolicy"><a href="#restartpolicy" class="headerlink" title="restartpolicy"></a>restartpolicy</h3><p>如果pod运行时意外退出(程序故障)，kubelet会根据restart policy来判断是否重启pod，可能的值为 Always、OnFailure 和 Never。默认为 Always，如果容器退出会再再启动，pod启动次数加1。</p><h2 id="优雅的结束"><a href="#优雅的结束" class="headerlink" title="优雅的结束"></a>优雅的结束</h2><p>首先谈下pod的删除流程：</p><ol><li>用户发送命令删除 Pod，使用的是默认的宽限期（grace period 30秒）</li><li>apiserver中的 Pod 会随着宽限期规定的时间进行更新，过了这个时间 Pod 就会被认为已”dead”</li><li>当使用客户端命令查询 Pod 状态时，Pod 显示为 “Terminating”</li><li>（和第 3 步同步进行）当 Kubelet 看到 Pod 由于步骤 2 中设置的时间而被标记为 terminating 状态时，它就开始执行关闭 Pod 流程<ul><li>如果 Pod 定义了 preStop 钩子，就在 Pod 内部调用它。如果宽限期结束了，但是 preStop 钩子还在运行，那么就用小的（2 秒）扩展宽限期调用步骤 2。</li><li>给 Pod 内的进程发送 <code>TERM</code> 信号(即<code>kill</code>, <code>kill -15</code>)。请注意，并不是所有 Pod 中的容器都会同时收到 TERM 信号，如果它们关闭的顺序很重要，则每个容器可能都需要一个 preStop 钩子。</li></ul></li><li>（和第 3 步同步进行）从服务的<code>endpoint</code>列表中删除 Pod，Pod 也不再被视为副本控制器的运行状态的 Pod 集的一部分。因为负载均衡器（如服务代理）会将其从轮换中删除，所以缓慢关闭的 Pod 无法继续为流量提供服务。</li><li>当宽限期到期时，仍在 Pod 中运行的所有进程都会被<code>SIGKILL</code>(即<code>kill -9</code>)信号杀死。</li></ol><h3 id="捕捉SIGTERM"><a href="#捕捉SIGTERM" class="headerlink" title="捕捉SIGTERM"></a>捕捉SIGTERM</h3><p>如果pod没有捕捉<code>SIGTERM</code>信号就直接退出，有些请求还没处理完，这势必影响服务质量，所以需要优雅退出，很多库都提供了类似的功能，当接受到退出信号时，清理空闲链接，等待当前请求处理完后再退出。如果善后工作较长，比较适当增加<code>terminationGracePeriodSeconds</code>的时间。</p><h3 id="prestop"><a href="#prestop" class="headerlink" title="prestop"></a>prestop</h3><p>另外也可以通过<code>prestop</code>设置hook操作，做一些额外的清理工作，<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">lifecycle-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">lifecycle-demo-container</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">    lifecycle:</span></span><br><span class="line"><span class="attr">      preStop:</span></span><br><span class="line"><span class="attr">        exec:</span></span><br><span class="line"><span class="attr">          command:</span> <span class="string">["/bin/sh","-c","nginx</span> <span class="bullet">-s</span> <span class="string">quit;</span> <span class="string">while</span> <span class="string">killall</span> <span class="bullet">-0</span> <span class="string">nginx;</span> <span class="string">do</span> <span class="string">sleep</span> <span class="number">1</span><span class="string">;</span> <span class="string">done"]</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>命令 preStop 负责优雅地终止 nginx 服务。当因为失效而导致容器终止时，这一处理方式很有用。</p><blockquote><p><strong>注</strong><br>  Kubernetes 只有在 Pod 结束（Terminated） 的时候才会发送 preStop 事件，这意味着在 Pod 完成（Completed） 时 preStop 的事件处理逻辑不会被触发。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>优雅就不要怕麻烦，来我们总结下优雅的秘诀：</p><ol><li>需要初始化的操作使用initcontainer来做</li><li>就绪检查，探活检查少不了,必要时也可以配置ReadinessGates</li><li>优雅退出要处理<code>SIGTERM</code></li><li>需要时也可以设置下poststart, prestop</li><li>其他的，设置limit/reqeust也是必须的</li></ol><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1] <a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/init-containers/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/concepts/workloads/pods/init-containers/</a><br>[2] <a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx ingress controller 最后的倔强: admission webhook</title>
      <link href="/ingress-nginx-controller-admission-webhook/"/>
      <url>/ingress-nginx-controller-admission-webhook/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>k8s中大多使用nginx-ingress-controller来实现ingress, 但是脆弱的nginx-controller通过ingress解析出nginx配置, 对于某些annotation会reload nignx配置失败, 然后controller就卡死了, 不断重启, 除非删除对应的ingress.</p><h3 id="问题复现"><a href="#问题复现" class="headerlink" title="问题复现"></a>问题复现</h3><p>创建有问题的<code>ingress</code></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream:</span> <span class="string">"false"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/auth-tls-verify-client:</span> <span class="string">optional</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/auth-tls-verify-depth:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="string">nginx.ingress.kubernetes.io/configuration-snippet:</span> <span class="string">|</span></span><br><span class="line"><span class="string">      proxy_set_header Host $targethost;</span></span><br><span class="line"><span class="string">      proxy_buffering     off;</span></span><br><span class="line"><span class="string">      proxy_pass          http://$targetbackend;</span></span><br><span class="line"><span class="string">      proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504;</span></span><br><span class="line"><span class="string">      proxy_redirect      off;</span></span><br><span class="line"><span class="string">      proxy_set_header    X-SSL-Client-Verify $ssl_client_verify;</span></span><br><span class="line"><span class="string">      proxy_set_header    X-SSL-Client-DN $ssl_client_s_dn;</span></span><br><span class="line"><span class="string">      proxy_set_header    X-Real-IP       $remote_addr;</span></span><br><span class="line"><span class="string">      proxy_set_header    X-Forwarded-For $proxy_add_x_forwarded_for;</span></span><br><span class="line"><span class="string"></span><span class="attr">  creationTimestamp:</span> <span class="string">"2020-03-23T04:57:22Z"</span></span><br><span class="line"><span class="attr">  generation:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">example-ingress</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  resourceVersion:</span> <span class="string">"57681168"</span></span><br><span class="line"><span class="attr">  selfLink:</span> <span class="string">/apis/extensions/v1beta1/namespaces/kube-system/ingresses/example-ingress</span></span><br><span class="line"><span class="attr">  uid:</span> <span class="string">c7f66385-6cc2-11ea-b6a8-246e96d4b538</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">example.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">example-svc</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8008</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">  tls:</span></span><br><span class="line"><span class="attr">  - hosts:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">example.com</span></span><br><span class="line"><span class="attr">    secretName:</span> <span class="string">example-tls</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">  loadBalancer:</span> <span class="string">{}</span></span><br></pre></td></tr></tbody></table></figure><p>查看<code>nginx-ingress-controller</code>状态全部为<code>CrashLoopBackOff</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl get po -n kube-system -owide |grep ingress</span></span><br><span class="line">nginx-ingress-controller-ftfbg                        1/2     CrashLoopBackOff   6          8m27s</span><br><span class="line">nginx-ingress-controller-hp4pf                        1/2     CrashLoopBackOff   11         24m  </span><br><span class="line">nginx-ingress-controller-qlb4l                        1/2     CrashLoopBackOff   11         24m</span><br></pre></td></tr></tbody></table></figure><p>查看<code>nginx-ingress-controller</code>日志, 显示reload失败<code>"proxy_pass" directive is duplicate in /tmp/nginx-cfg911768424:822</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">W0403 10:26:14.716246       1 queue.go:130] requeuing kube-system/nginx-ingress-controller-4txfk, err </span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Error: <span class="built_in">exit</span> status 1</span><br><span class="line">2020/04/03 10:26:14 [notice] 137<span class="comment">#137: ModSecurity-nginx v1.0.0</span></span><br><span class="line">2020/04/03 10:26:14 [warn] 137<span class="comment">#137: duplicate value "error" in /tmp/nginx-cfg911768424:815</span></span><br><span class="line">nginx: [warn] duplicate value <span class="string">"error"</span> <span class="keyword">in</span> /tmp/nginx-cfg911768424:815</span><br><span class="line">2020/04/03 10:26:14 [warn] 137<span class="comment">#137: duplicate value "timeout" in /tmp/nginx-cfg911768424:815</span></span><br><span class="line">nginx: [warn] duplicate value <span class="string">"timeout"</span> <span class="keyword">in</span> /tmp/nginx-cfg911768424:815</span><br><span class="line">2020/04/03 10:26:14 [emerg] 137<span class="comment">#137: "proxy_pass" directive is duplicate in /tmp/nginx-cfg911768424:822</span></span><br><span class="line">nginx: [emerg] <span class="string">"proxy_pass"</span> directive is duplicate <span class="keyword">in</span> /tmp/nginx-cfg911768424:822</span><br><span class="line">nginx: configuration file /tmp/nginx-cfg911768424 <span class="built_in">test</span> failed</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">W0403 10:26:16.998897       1 nginx_status.go:207] unexpected error obtaining nginx status info: unexpected error scraping nginx status page: unexpected error scraping nginx : Get http://0.0.0.0:18080/nginx_status: dial tcp 0.0.0.0:18080: connect: connection refused</span><br><span class="line">I0403 10:26:17.526801       1 main.go:167] Received SIGTERM, shutting down</span><br><span class="line">I0403 10:26:17.526827       1 nginx.go:364] Shutting down controller queues</span><br><span class="line">I0403 10:26:17.526845       1 status.go:200] updating status of Ingress rules (remove)</span><br><span class="line">I0403 10:26:17.537511       1 status.go:219] removing address from ingress status ([])</span><br><span class="line">I0403 10:26:17.537593       1 nginx.go:372] Stopping NGINX process</span><br><span class="line">2020/04/03 10:26:17 [notice] 141<span class="comment">#141: signal process started</span></span><br><span class="line">I0403 10:26:20.547669       1 nginx.go:385] NGINX process has stopped</span><br><span class="line">I0403 10:26:20.547692       1 main.go:175] Handled quit, awaiting Pod deletion</span><br><span class="line">I0403 10:26:30.547824       1 main.go:178] Exiting with 0</span><br></pre></td></tr></tbody></table></figure><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>创建一个有问题的ingress, 会影响所有新创建的ingress规则, 又一个集群级别的Bug诞生了.那么有没有办法, 提前检验ingress配置, 有问题就不去reload. 那验证步骤肯定要在请求到达nginx-controller之前来做, 是不是想到了<a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">k8s-admission-webhook</a>, 可以在apiserver持久化对象前拦截请求, 去实现自定义的验证规则. 好在新版本的nginx-ingress-controller(v0.25.0+)已经实现了相关的功能, 只需开启对应配置就行.</p><h3 id="ApiServer配置"><a href="#ApiServer配置" class="headerlink" title="ApiServer配置"></a>ApiServer配置</h3><p>Apiserver开启webhook相关配置, 必须包含<code>MutatingAdmissionWebhook</code>与<code>ValidatingAdmissionWebhook</code><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--admission-control=MutatingAdmissionWebhook,ValidatingAdmissionWebhook</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="创建webhook相关配置"><a href="#创建webhook相关配置" class="headerlink" title="创建webhook相关配置"></a>创建webhook相关配置</h3><p>启用ValidatingAdmissionWebhook必须使用https, 需要配置对应证书</p><ul><li><p>手动生成:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl req -x509 -newkey rsa:2048 -keyout certificate.pem -out key.pem -days 365 -nodes -subj <span class="string">"/CN=ingress-validation-webhook.ingress-nginx.svc"</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>CertificateSigningRequest<br>通过k8s <code>CertificateSigningRequest</code>来创建(controller-manager需要开启<code>--cluster-signing-cert-file</code>与<code>--cluster-signing-key-file</code>)<br>可通过如下脚本创建, namespace与service替换成自己的</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">SERVICE_NAME=ingress-nginx</span><br><span class="line">NAMESPACE=ingress-nginx</span><br><span class="line"></span><br><span class="line">TEMP_DIRECTORY=$(mktemp -d)</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"creating certs in directory <span class="variable">${TEMP_DIRECTORY}</span>"</span></span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; <span class="variable">${TEMP_DIRECTORY}</span>/csr.conf</span><br><span class="line">[req]</span><br><span class="line">req_extensions = v3_req</span><br><span class="line">distinguished_name = req_distinguished_name</span><br><span class="line">[req_distinguished_name]</span><br><span class="line">[ v3_req ]</span><br><span class="line">basicConstraints = CA:FALSE</span><br><span class="line">keyUsage = nonRepudiation, digitalSignature, keyEncipherment</span><br><span class="line">extendedKeyUsage = serverAuth</span><br><span class="line">subjectAltName = @alt_names</span><br><span class="line">[alt_names]</span><br><span class="line">DNS.1 = <span class="variable">${SERVICE_NAME}</span></span><br><span class="line">DNS.2 = <span class="variable">${SERVICE_NAME}</span>.<span class="variable">${NAMESPACE}</span></span><br><span class="line">DNS.3 = <span class="variable">${SERVICE_NAME}</span>.<span class="variable">${NAMESPACE}</span>.svc</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">openssl genrsa -out <span class="variable">${TEMP_DIRECTORY}</span>/server-key.pem 2048</span><br><span class="line">openssl req -new -key <span class="variable">${TEMP_DIRECTORY}</span>/server-key.pem \</span><br><span class="line">    -subj <span class="string">"/CN=<span class="variable">${SERVICE_NAME}</span>.<span class="variable">${NAMESPACE}</span>.svc"</span> \</span><br><span class="line">    -out <span class="variable">${TEMP_DIRECTORY}</span>/server.csr \</span><br><span class="line">    -config <span class="variable">${TEMP_DIRECTORY}</span>/csr.conf</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF | kubectl create -f -</span><br><span class="line">apiVersion: certificates.k8s.io/v1beta1</span><br><span class="line">kind: CertificateSigningRequest</span><br><span class="line">metadata:</span><br><span class="line">  name: <span class="variable">${SERVICE_NAME}</span>.<span class="variable">${NAMESPACE}</span>.svc</span><br><span class="line">spec:</span><br><span class="line">  request: $(cat <span class="variable">${TEMP_DIRECTORY}</span>/server.csr | base64 | tr -d <span class="string">'\n'</span>)</span><br><span class="line">  usages:</span><br><span class="line">  - digital signature</span><br><span class="line">  - key encipherment</span><br><span class="line">  - server auth</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">kubectl certificate approve <span class="variable">${SERVICE_NAME}</span>.<span class="variable">${NAMESPACE}</span>.svc</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> $(seq 10); <span class="keyword">do</span></span><br><span class="line">    SERVER_CERT=$(kubectl get csr <span class="variable">${SERVICE_NAME}</span>.<span class="variable">${NAMESPACE}</span>.svc -o jsonpath=<span class="string">'{.status.certificate}'</span>)</span><br><span class="line">    <span class="keyword">if</span> [[ <span class="variable">${SERVER_CERT}</span> != <span class="string">''</span> ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">break</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    sleep 1</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="keyword">if</span> [[ <span class="variable">${SERVER_CERT}</span> == <span class="string">''</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"ERROR: After approving csr <span class="variable">${SERVICE_NAME}</span>.<span class="variable">${NAMESPACE}</span>.svc, the signed certificate did not appear on the resource. Giving up after 10 attempts."</span> &gt;&amp;2</span><br><span class="line">    <span class="built_in">exit</span> 1</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">${SERVER_CERT}</span> | openssl base64 -d -A -out <span class="variable">${TEMP_DIRECTORY}</span>/server-cert.pem</span><br><span class="line"></span><br><span class="line">kubectl create secret generic ingress-nginx.svc \</span><br><span class="line">    --from-file=key.pem=<span class="variable">${TEMP_DIRECTORY}</span>/server-key.pem \</span><br><span class="line">    --from-file=cert.pem=<span class="variable">${TEMP_DIRECTORY}</span>/server-cert.pem \</span><br><span class="line">    -n <span class="variable">${NAMESPACE}</span></span><br></pre></td></tr></tbody></table></figure></li></ul><h3 id="配置ingress-controller"><a href="#配置ingress-controller" class="headerlink" title="配置ingress controller"></a>配置ingress controller</h3><p>ingress controller需要启用如下参数, 挂载需要的tls证书</p><table><thead><tr><th>flag</th><th>description</th><th>example usage</th></tr></thead><tbody><tr><td><code>--validating-webhook</code></td><td>admission webhook的地址</td><td><code>:8080</code></td></tr><tr><td><code>--validating-webhook-certificate</code></td><td>webhook证书</td><td><code>/usr/local/certificates/validating-webhook.pem</code></td></tr><tr><td><code>--validating-webhook-key</code></td><td>webhook私钥</td><td><code>/usr/local/certificates/validating-webhook-key.pem</code></td></tr></tbody></table><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>更新后, 创建有问题的ingress则会拦截, 符合预期</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># kubectl apply -f ing.yaml</span></span><br><span class="line">Error from server: error when creating <span class="string">"ing.yaml"</span>: admission webhook <span class="string">"validate.nginx.ingress.kubernetes.io"</span> denied the request: </span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Error: <span class="built_in">exit</span> status 1</span><br><span class="line">2020/04/02 10:26:04 [emerg] 331<span class="comment">#331: directive "proxy_pass" is not terminated by ";" in /tmp/nginx-cfg461116913:2165</span></span><br><span class="line">nginx: [emerg] directive <span class="string">"proxy_pass"</span> is not terminated by <span class="string">";"</span> <span class="keyword">in</span> /tmp/nginx-cfg461116913:2165</span><br><span class="line">nginx: configuration file /tmp/nginx-cfg461116913 <span class="built_in">test</span> failed</span><br></pre></td></tr></tbody></table></figure><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ul><li><a href="https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/reference/access-authn-authz/extensible-admission-controllers/</a></li><li><a href="https://kubernetes.github.io/ingress-nginx/deploy/validating-webhook/" target="_blank" rel="noopener">https://kubernetes.github.io/ingress-nginx/deploy/validating-webhook/</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> ingress </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pod sandbox 创建失败</title>
      <link href="/pod-sandbox-recreated/"/>
      <url>/pod-sandbox-recreated/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>今天在k8s更新服务时,发现pod启动失败,报错<code>failed to start sandbox container</code>,如下所示:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason                  Age                     From                                           Message</span><br><span class="line">  ----     ------                  ----                    ----                                           -------</span><br><span class="line">  Normal   Scheduled               28m                     default-scheduler                              Successfully assigned kube-system/k8s-proxy-7wkt4 to tj1-staging-com-ocean007-201812.kscn</span><br><span class="line">  Warning  FailedCreatePodSandBox  28m (x13 over 28m)      kubelet, tj1-staging-com-ocean007-201812.kscn  Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container <span class="keyword">for</span> pod <span class="string">"k8s-proxy-7wkt4"</span>: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused <span class="string">"process_linux.go:297: getting the final child's pid from pipe caused \"EOF\""</span>: unknown</span><br><span class="line">  Normal   SandboxChanged          3m19s (x1364 over 28m)  kubelet, tj1-staging-com-ocean007-201812.kscn  Pod sandbox changed, it will be killed and re-created.</span><br></pre></td></tr></tbody></table></figure><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>sandbox 创建失败只是表象,是宿主机其他异常导致的,一般是(cpu,diskio,mem)导致的.</p><p>首先,上节点看kubelet,docker有无异常,日志没有明显错误,通过<code>top</code>看到docker cpu占用非常高</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@tj1-staging-com-ocean007-201812 ~]<span class="comment"># top</span></span><br><span class="line"></span><br><span class="line">top - 17:55:00 up 265 days,  3:41,  1 user,  load average: 10.71, 11.34, 10.76</span><br><span class="line">Tasks: 816 total,   5 running, 811 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s): 24.0 us, 34.5 sy,  0.0 ni, 41.4 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st</span><br><span class="line">KiB Mem : 65746380 total, 20407940 free, 11007040 used, 34331400 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used. 49134416 avail Mem </span><br><span class="line"></span><br><span class="line">    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                      </span><br><span class="line"> 115483 root      20   0 3965212 273188  34564 S 489.7  0.4 382260:40 dockerd                                                                                                                                      </span><br><span class="line">1367523 root      20   0   18376   2972   2716 R  66.9  0.0  20163:45 bash                                                                                                                                         </span><br><span class="line">1367487 root      20   0   11856   5616   4512 S  54.0  0.0  16748:26 containerd-shim                                                                                                                              </span><br><span class="line">3200169 root      20   0    1300      4      0 R  53.3  0.0  14913:49 sh                                                                                                                                           </span><br><span class="line">2429952 root      20   0    1300      4      0 S  49.3  0.0   9620:56 sh                                                                                                                                           </span><br><span class="line">3200130 root      20   0    9392   4756   3884 S  47.7  0.0  13417:30 containerd-shim                                                                                                                              </span><br><span class="line">3718475 root      20   0    1300      4      0 R  47.4  0.0   8600:20 sh                                                                                                                                           </span><br><span class="line">3718440 root      20   0   10736   5516   4512 S  42.1  0.0   7575:31 containerd-shim                                                                                                                              </span><br><span class="line">2429917 root      20   0   11856   5556   4512 S  40.1  0.0   8313:22 containerd-shim                                                                                                                              </span><br><span class="line">3205493 root      20   0 3775924 230996  66704 S  18.9  0.4   2559:07 kubelet                                                                                                                                      </span><br><span class="line">      1 root      20   0  195240 157000   3932 S   7.9  0.2   1417:46 systemd                                                                                                                                      </span><br><span class="line">    804 dbus      20   0   30308   6460   2464 S   1.7  0.0 462:18.84 dbus-daemon                                                                                                                                  </span><br><span class="line">1011737 root      20   0  277656 122788  18428 S   1.3  0.2 768:03.00 cadvisor                                                                                                                                     </span><br><span class="line"> 115508 root      20   0 7139200  32896  24288 S   1.0  0.1 662:25.27 containerd                                                                                                                                   </span><br><span class="line">    806 root      20   0   24572   3060   2480 S   0.7  0.0 171:22.52 systemd-logind                                                                                                                               </span><br><span class="line"> 511080 root       0 -20 2751348  52552  15744 S   0.7  0.1 178:27.51 sagent                                                                                                                                       </span><br><span class="line">1102507 root      20   0   11792   7292   4512 S   0.7  0.0  23:36.37 containerd-shim                                                                                                                              </span><br><span class="line">1272223 root      20   0  164800   5296   3824 R   0.7  0.0   0:00.38 top                                                                                                                                          </span><br><span class="line">2866292 root      20   0 5045000 1.983g   3080 S   0.7  3.2 230:09.47 redis</span><br></pre></td></tr></tbody></table></figure><p>同时, cpu system异常高.<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%Cpu(s): 24.0 us, 34.5 sy,  0.0 ni, 41.4 id,  0.0 wa,  0.0 hi,  0.1 si,  0.0 st</span><br></pre></td></tr></tbody></table></figure><p></p><p>按照以前的经验,一般是由某些容器引起的,通过<code>top</code>看到个别<code>sh</code>进程占用cpu较高.</p><p>通过<code>ps</code>看到进程居然是个死循环<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@tj1-staging-com-ocean007-201812 ~]<span class="comment"># ps -ef |grep 1367523</span></span><br><span class="line">root     1287628 1247781  0 17:55 pts/1    00:00:00 grep --color=auto 1367523</span><br><span class="line">root     1367523 1367504 72 Feb28 ?        14-00:04:17 /bin/bash -c <span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> <span class="built_in">echo</span> hello; <span class="keyword">done</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>通过<code>/proc/pid/cgroup</code>找到对应容器<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /proc/1367523/cgroup</span></span><br><span class="line">11:freezer:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">10:devices:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">9:hugetlb:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">8:blkio:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">7:memory:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">6:perf_event:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">5:cpuset:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">4:pids:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">3:net_cls,net_prio:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">2:cpu,cpuacct:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br><span class="line">1:name=systemd:/kubepods/besteffort/pod55d3adf2-67f7-11ea-93f2-246e968203b8/29842d5544b701dbb5ff647dba19bb4ebec821edc6ee1ffbd7aeee58fa5038fd</span><br></pre></td></tr></tbody></table></figure><p></p><p>找到对应容器<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps | grep 29842d554</span><br></pre></td></tr></tbody></table></figure><p></p><p>清理完相关pod后,系统恢复正常</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">top - 18:25:57 up 265 days,  4:12,  1 user,  load average: 1.05, 1.24, 4.02</span><br><span class="line">Tasks: 769 total,   1 running, 768 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  1.7 us,  0.9 sy,  0.0 ni, 97.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem : 65746380 total, 22106960 free, 10759860 used, 32879560 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used. 49401576 avail Mem </span><br><span class="line"></span><br><span class="line">    PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                      </span><br><span class="line">3205493 root      20   0 3775924 229844  66704 S   9.9  0.3   2563:18 kubelet                                                                                                                                      </span><br><span class="line"> 115483 root      20   0 3965468 249124  34564 S   7.9  0.4 382323:36 dockerd                                                                                                                                      </span><br><span class="line">      1 root      20   0  195240 157000   3932 S   6.3  0.2   1419:48 systemd                                                                                                                                      </span><br><span class="line">    804 dbus      20   0   30308   6460   2464 S   2.0  0.0 462:51.51 dbus-daemon                                                                                                                                  </span><br><span class="line">3085322 root      20   0 12.045g 1.578g  19028 S   1.3  2.5 767:51.19 java                                                                                                                                         </span><br><span class="line"> 115508 root      20   0 7139200  32264  24288 S   1.0  0.0 662:42.18 containerd                                                                                                                                   </span><br><span class="line"> 511080 root       0 -20 2751348  42116  15744 S   1.0  0.1 178:44.79 sagent                                                                                                                                       </span><br><span class="line">1011737 root      20   0  277656 111836  18428 S   1.0  0.2 768:49.01 cadvisor                                                                                                                                     </span><br><span class="line">1523167 root      20   0  164800   5436   4012 R   0.7  0.0   0:00.04 top                                                                                                                                          </span><br><span class="line">3199459 root      20   0 1554708  43668   9496 S   0.7  0.1  28:50.60 falcon-agent                                                                                                                                 </span><br><span class="line">      7 root      20   0       0      0      0 S   0.3  0.0 619:07.64 rcu_sched                                                                                                                                    </span><br><span class="line">    806 root      20   0   24572   3060   2480 S   0.3  0.0 171:33.69 systemd-logind                                                                                                                               </span><br><span class="line">  11921 root      20   0   94820  20480   5840 S   0.3  0.0   1402:42 consul                                                                                                                                       </span><br><span class="line"> 575838 root      20   0  411464  17092   7364 S   0.3  0.0  15:16.25 python                                                                                                                                       </span><br><span class="line"> 856593 root      20   0 1562392  37912   9612 S   0.3  0.1  21:34.23 falcon-agent                                                                                                                                 </span><br><span class="line"> 931957 33        20   0   90728   3392   1976 S   0.3  0.0   0:51.23 nginx                                                                                                                                        </span><br><span class="line">1212186 root      20   0       0      0      0 S   0.3  0.0   0:01.12 kworker/14:1                                                                                                                                 </span><br><span class="line">1726228 root      20   0    9392   4496   3808 S   0.3  0.0   0:00.67 containerd-shim                                                                                                                              </span><br><span class="line">1887128 root      20   0  273160   7932   3128 S   0.3  0.0  46:05.23 redis-server                                                                                                                                 </span><br><span class="line">2788111 root      20   0  273160   6300   3080 S   0.3  0.0  25:18.55 redis-server                                                                                                                                 </span><br><span class="line">3199297 root      20   0 1563160  44812   9624 S   0.3  0.1  31:13.73 falcon-agent</span><br></pre></td></tr></tbody></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>sandox创建失败的原因是各种各样的, 如[memory设置错误触发的异常][1],[dockerd异常][2].</p><p>针对此处问题是由于某些测试pod通过<code>while true; do echo hello; done</code>启动,死循环一直<code>echo hello</code>产生大量<code>read()</code>系统调用,所在cpu飙升.多个类似pod导致系统非常繁忙,无法正常处理其他请求.</p><p>此类问题不容易在pod创建时直接检测到,只能通过添加物理节点相关报警(dockerd cpu使用率, node cpu.sys使用率)及时发现问题.</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1] <a href="https://github.com/kubernetes/kubernetes/issues/56996" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/56996</a><br>[2] <a href="https://plugaru.org/2018/05/21/pod-sandbox-changed-it-will-be-killed-and-re-created/" target="_blank" rel="noopener">https://plugaru.org/2018/05/21/pod-sandbox-changed-it-will-be-killed-and-re-created/</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-matery主题美化以及zeit部署</title>
      <link href="/hexo-matery-beauty/"/>
      <url>/hexo-matery-beauty/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文主要介绍<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank" rel="noopener">Metery主题</a>的安装使用，<a href="https://github.com/qingwave/hexo-theme-matery" target="_blank" rel="noopener">主题优化</a>内容包括</p><ul><li>添加404，诗词样式</li><li>配置提出，在hexo项目<code>_data/matery.yml</code>中配置主题样式，可覆盖<code>themes/matery/_config</code>配置</li><li>调整配色，去掉彩虹配置</li></ul><h2 id="部署到Zeit-io"><a href="#部署到Zeit-io" class="headerlink" title="部署到Zeit.io"></a>部署到Zeit.io</h2><p>Github Page在国内访问较慢，可以将服务部署到静态站<a href="https://zeit.io" target="_blank" rel="noopener">zeit.io</a>, 每月20G免费流量，并且能同步github项目。</p><ul><li>Github 账户登陆zeit.io，授予 zeit repo 的 read 权限</li><li>导入项目，zeit.io导入项目名称不能包含<code>.</code>，可将项目名称<code>username.github.io</code>修改为<code>username</code>导入，后面再改回<br>部署成功</li></ul><p>过一会自动部署成功，如<a href="https://qingwave.github.io/">qingwave.github.io</a>可访问项目，项目的任何更新会触发重新部署。</p><p>也可通过zeit.io提供的DNS解析服务配置自己的域名，然后在百度站长里配置信息。</p><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ul><li>简单漂亮，文章内容美观易读</li><li><a href="https://material.io/" target="_blank" rel="noopener">Material Design</a> 设计</li><li>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</li><li>首页轮播文章及每天动态切换 <code>Banner</code> 图片</li><li>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</li><li>时间轴式的归档页</li><li><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</li><li>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</li><li>可自定义的数据的友情链接页面</li><li>支持文章置顶和文章打赏</li><li>支持 <code>MathJax</code></li><li><code>TOC</code> 目录</li><li>可设置复制文章内容时追加版权信息</li><li>可设置阅读文章时做密码验证</li><li><a href="https://gitalk.github.io/" target="_blank" rel="noopener">Gitalk</a>、<a href="https://imsun.github.io/gitment/" target="_blank" rel="noopener">Gitment</a>、<a href="https://valine.js.org/" target="_blank" rel="noopener">Valine</a> 和 <a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</li><li>集成了<a href="http://busuanzi.ibruce.info/" target="_blank" rel="noopener">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</li><li>支持在首页的音乐播放和视频播放功能</li><li>支持<code>emoji</code>表情，用<code>markdown emoji</code>语法书写直接生成对应的能<strong>跳跃</strong>的表情。</li><li>支持 <a href="http://www.daovoice.io/" target="_blank" rel="noopener">DaoVoice</a>、<a href="https://www.tidio.com/" target="_blank" rel="noopener">Tidio</a> 在线聊天功能。</li></ul><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>当你看到这里的时候，应该已经有一个自己的 <a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo</a> 博客了。如果还没有的话，不妨使用 Hexo 和 <a href="https://www.appinn.com/markdown/" target="_blank" rel="noopener">Markdown</a> 来写博客和文章。</p><p>点击 <a href="https://codeload.github.com/qingwave/hexo-theme-matery/zip/qinng" target="_blank" rel="noopener">这里</a> 下载 <code>master</code> 分支的最新稳定版的代码，解压缩后，将 <code>hexo-theme-matery</code> 的文件夹复制到你 Hexo 的 <code>themes</code> 文件夹中即可。</p><p>当然你也可以在你的 <code>themes</code> 文件夹下使用 <code>Git clone</code> 命令来下载:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/qingwave/hexo-theme-matery.git</span><br></pre></td></tr></tbody></table></figure><p>更推荐使用<code>git submodule</code>方式，可获取主题更新<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git submodule add https://github.com/qingwave/hexo-theme-matery.git themes/matery</span><br><span class="line">git submodule update --init</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="切换主题"><a href="#切换主题" class="headerlink" title="切换主题"></a>切换主题</h3><p>修改 Hexo 根目录下的 <code>_config.yml</code> 的  <code>theme</code> 的值：<code>theme: hexo-theme-matery</code></p><h4 id="config-yml-文件的其它修改建议"><a href="#config-yml-文件的其它修改建议" class="headerlink" title="_config.yml 文件的其它修改建议:"></a><code>_config.yml</code> 文件的其它修改建议:</h4><ul><li>请修改 <code>_config.yml</code> 的 <code>url</code> 的值为你的网站主 <code>URL</code>（如：<code>http://xxx.github.io</code>）。</li><li>建议修改两个 <code>per_page</code> 的分页条数值为 <code>6</code> 的倍数，如：<code>12</code>、<code>18</code> 等，这样文章列表在各个屏幕下都能较好的显示。</li><li>如果你是中文用户，则建议修改 <code>language</code> 的值为 <code>zh-CN</code>。</li></ul><h3 id="新建分类-categories-页"><a href="#新建分类-categories-页" class="headerlink" title="新建分类 categories 页"></a>新建分类 categories 页</h3><p><code>categories</code> 页是用来展示所有分类的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>categories/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">"categories"</span></span><br></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件 <code>/source/categories/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">categories</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-09</span><span class="bullet">-30</span> <span class="number">17</span><span class="string">:25:30</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"categories"</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">"categories"</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></tbody></table></figure><h3 id="新建标签-tags-页"><a href="#新建标签-tags-页" class="headerlink" title="新建标签 tags 页"></a>新建标签 tags 页</h3><p><code>tags</code> 页是用来展示所有标签的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>tags/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">"tags"</span></span><br></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件 <code>/source/tags/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">tags</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-09</span><span class="bullet">-30</span> <span class="number">18</span><span class="string">:23:38</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"tags"</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">"tags"</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></tbody></table></figure><h3 id="新建关于我-about-页"><a href="#新建关于我-about-页" class="headerlink" title="新建关于我 about 页"></a>新建关于我 about 页</h3><p><code>about</code> 页是用来展示<strong>关于我和我的博客</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>about/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">"about"</span></span><br></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件 <code>/source/about/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">about</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-09</span><span class="bullet">-30</span> <span class="number">17</span><span class="string">:25:30</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"about"</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">"about"</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></tbody></table></figure><h3 id="新建留言板-contact-页（可选的）"><a href="#新建留言板-contact-页（可选的）" class="headerlink" title="新建留言板 contact 页（可选的）"></a>新建留言板 contact 页（可选的）</h3><p><code>contact</code> 页是用来展示<strong>留言板</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>contact/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">"contact"</span></span><br></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件 <code>/source/contact/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">contact</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-09</span><span class="bullet">-30</span> <span class="number">17</span><span class="string">:25:30</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"contact"</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">"contact"</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>注</strong>：本留言板功能依赖于第三方评论系统，请<strong>激活</strong>你的评论系统才有效果。并且在主题的 <code>_config.yml</code> 文件中，第 <code>19</code> 至 <code>21</code> 行的“<strong>菜单</strong>”配置，取消关于留言板的注释即可。</p></blockquote><h3 id="新建友情连接-friends-页（可选的）"><a href="#新建友情连接-friends-页（可选的）" class="headerlink" title="新建友情连接 friends 页（可选的）"></a>新建友情连接 friends 页（可选的）</h3><p><code>friends</code> 页是用来展示<strong>友情连接</strong>信息的页面，如果在你的博客 <code>source</code> 目录下还没有 <code>friends/index.md</code> 文件，那么你就需要新建一个，命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">"friends"</span></span><br></pre></td></tr></tbody></table></figure><p>编辑你刚刚新建的页面文件 <code>/source/friends/index.md</code>，至少需要以下内容：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">friends</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-12</span><span class="bullet">-12</span> <span class="number">21</span><span class="string">:25:30</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"friends"</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">"friends"</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></tbody></table></figure><p>同时，在你的博客 <code>source</code> 目录下新建 <code>_data</code> 目录，在 <code>_data</code> 目录中新建 <code>friends.json</code> 文件，文件内容如下所示：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[{</span><br><span class="line">    <span class="attr">"avatar"</span>: <span class="string">"http://image.luokangyuan.com/4027734.jpeg"</span>,</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"闪烁之狐"</span>,</span><br><span class="line">    <span class="attr">"introduction"</span>: <span class="string">"编程界大佬，技术牛，人还特别好，不懂的都可以请教大佬"</span>,</span><br><span class="line">    <span class="attr">"url"</span>: <span class="string">"https://blinkfox.github.io/"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"前去学习"</span></span><br><span class="line">}, {</span><br><span class="line">    <span class="attr">"avatar"</span>: <span class="string">"https://qingwave.github.io/medias/avatar.jpg"</span>,</span><br><span class="line">    <span class="attr">"name"</span>: <span class="string">"Qinng"</span>,</span><br><span class="line">    <span class="attr">"introduction"</span>: <span class="string">"Reading Coding Daydraming"</span>,</span><br><span class="line">    <span class="attr">"url"</span>: <span class="string">"https://qingwave.github.io/"</span>,</span><br><span class="line">    <span class="attr">"title"</span>: <span class="string">"前去学习"</span></span><br><span class="line">}]</span><br></pre></td></tr></tbody></table></figure><h3 id="菜单导航配置"><a href="#菜单导航配置" class="headerlink" title="菜单导航配置"></a>菜单导航配置</h3><h4 id="配置基本菜单导航的名称、路径url和图标icon"><a href="#配置基本菜单导航的名称、路径url和图标icon" class="headerlink" title="配置基本菜单导航的名称、路径url和图标icon."></a>配置基本菜单导航的名称、路径url和图标icon.</h4><p>1.菜单导航名称可以是中文也可以是英文(如：<code>Index</code>或<code>主页</code>)<br>2.图标icon 可以在<a href="https://fontawesome.com/icons" target="_blank" rel="noopener">Font Awesome</a> 中查找   </p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line"><span class="attr">  Index:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-home</span></span><br><span class="line"><span class="attr">  Tags:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/tags</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-tags</span></span><br><span class="line"><span class="attr">  Categories:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/categories</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-bookmark</span></span><br><span class="line"><span class="attr">  Archives:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/archives</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-archive</span></span><br><span class="line"><span class="attr">  About:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/about</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-user-circle</span></span><br><span class="line"><span class="attr">  Friends:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/friends</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-address-book</span></span><br></pre></td></tr></tbody></table></figure><h4 id="二级菜单配置方法"><a href="#二级菜单配置方法" class="headerlink" title="二级菜单配置方法"></a>二级菜单配置方法</h4><p>如果你需要二级菜单则可以在原基本菜单导航的基础上如下操作<br>1.在需要添加二级菜单的一级菜单下添加<code>children</code>关键字(如:<code>About</code>菜单下添加<code>children</code>)<br>2.在<code>children</code>下创建二级菜单的 名称name,路径url和图标icon.<br>3.注意每个二级菜单模块前要加 <code>-</code>.<br>4.注意缩进格式  </p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line"><span class="attr">  Index:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-home</span></span><br><span class="line"><span class="attr">  Tags:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/tags</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-tags</span></span><br><span class="line"><span class="attr">  Categories:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/categories</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-bookmark</span></span><br><span class="line"><span class="attr">  Archives:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/archives</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-archive</span></span><br><span class="line"><span class="attr">  About:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/about</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-user-circle-o</span></span><br><span class="line"><span class="attr">  Friends:</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">/friends</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-address-book</span></span><br><span class="line"><span class="attr">  Medias:</span></span><br><span class="line"><span class="attr">    icon:</span> <span class="string">fas</span> <span class="string">fa-list</span></span><br><span class="line"><span class="attr">    children:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">Musics</span></span><br><span class="line"><span class="attr">        url:</span> <span class="string">/musics</span></span><br><span class="line"><span class="attr">        icon:</span> <span class="string">fas</span> <span class="string">fa-music</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">Movies</span></span><br><span class="line"><span class="attr">        url:</span> <span class="string">/movies</span></span><br><span class="line"><span class="attr">        icon:</span> <span class="string">fas</span> <span class="string">fa-film</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">Books</span></span><br><span class="line"><span class="attr">        url:</span> <span class="string">/books</span></span><br><span class="line"><span class="attr">        icon:</span> <span class="string">fas</span> <span class="string">fa-book</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">Galleries</span></span><br><span class="line"><span class="attr">        url:</span> <span class="string">/galleries</span></span><br><span class="line"><span class="attr">        icon:</span> <span class="string">fas</span> <span class="string">fa-image</span></span><br></pre></td></tr></tbody></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后就可以在文章中对应位置看到你用<code>emoji</code>语法写的表情了。</p><h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><p>由于 Hexo 自带的代码高亮主题显示不好看，所以主题中使用到了 <a href="https://github.com/ele828/hexo-prism-plugin" target="_blank" rel="noopener">hexo-prism-plugin</a> 的 Hexo 插件来做代码高亮，安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i -S hexo-prism-plugin</span><br></pre></td></tr></tbody></table></figure><p>然后，修改 Hexo 根目录下 <code>_config.yml</code> 文件中 <code>highlight.enable</code> 的值为 <code>false</code>，并新增 <code>prism</code> 插件相关的配置，主要配置如下：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">highlight:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">prism_plugin:</span></span><br><span class="line"><span class="attr">  mode:</span> <span class="string">'preprocess'</span>    <span class="comment"># realtime/preprocess</span></span><br><span class="line"><span class="attr">  theme:</span> <span class="string">'tomorrow'</span></span><br><span class="line"><span class="attr">  line_number:</span> <span class="literal">false</span>    <span class="comment"># default false</span></span><br><span class="line"><span class="attr">  custom_css:</span></span><br></pre></td></tr></tbody></table></figure><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>本主题中还使用到了 <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener">hexo-generator-search</a> 的 Hexo 插件来做内容搜索，安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">search:</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">search.xml</span></span><br><span class="line"><span class="attr">  field:</span> <span class="string">post</span></span><br></pre></td></tr></tbody></table></figure><h3 id="中文链接转拼音（建议安装）"><a href="#中文链接转拼音（建议安装）" class="headerlink" title="中文链接转拼音（建议安装）"></a>中文链接转拼音（建议安装）</h3><p>如果你的文章名称是中文的，那么 Hexo 默认生成的永久链接也会有中文，这样不利于 <code>SEO</code>，且 <code>gitment</code> 评论对中文链接也不支持。我们可以用 <a href="https://github.com/viko16/hexo-permalink-pinyin" target="_blank" rel="noopener">hexo-permalink-pinyin</a> Hexo 插件使在生成文章时生成中文拼音的永久链接。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-permalink-pinyin --save</span><br></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">permalink_pinyin:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  separator:</span> <span class="string">'-'</span> <span class="comment"># default: '-'</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>注</strong>：除了此插件外，<a href="https://github.com/rozbo/hexo-abbrlink" target="_blank" rel="noopener">hexo-abbrlink</a> 插件也可以生成非中文的链接。</p></blockquote><h3 id="文章字数统计插件（建议安装）"><a href="#文章字数统计插件（建议安装）" class="headerlink" title="文章字数统计插件（建议安装）"></a>文章字数统计插件（建议安装）</h3><p>如果你想要在文章中显示文章字数、阅读时长信息，可以安装 <a href="https://github.com/willin/hexo-wordcount" target="_blank" rel="noopener">hexo-wordcount</a>插件。</p><p>安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i --save hexo-wordcount</span><br></pre></td></tr></tbody></table></figure><p>然后只需在本主题下的 <code>_config.yml</code> 文件中，将各个文章字数相关的配置激活即可：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">postInfo:</span></span><br><span class="line"><span class="attr">  date:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  update:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  wordCount:</span> <span class="literal">false</span> <span class="comment"># 设置文章字数统计为 true.</span></span><br><span class="line"><span class="attr">  totalCount:</span> <span class="literal">false</span> <span class="comment"># 设置站点文章总字数统计为 true.</span></span><br><span class="line"><span class="attr">  min2read:</span> <span class="literal">false</span> <span class="comment"># 阅读时长.</span></span><br><span class="line"><span class="attr">  readCount:</span> <span class="literal">false</span> <span class="comment"># 阅读次数.</span></span><br></pre></td></tr></tbody></table></figure><h3 id="添加emoji表情支持（可选的）"><a href="#添加emoji表情支持（可选的）" class="headerlink" title="添加emoji表情支持（可选的）"></a>添加emoji表情支持（可选的）</h3><p>本主题新增了对<code>emoji</code>表情的支持，使用到了 <a href="https://npm.taobao.org/package/hexo-filter-github-emojis" target="_blank" rel="noopener">hexo-filter-github-emojis</a> 的 Hexo 插件来支持 <code>emoji</code>表情的生成，把对应的<code>markdown emoji</code>语法（<code>::</code>,例如：<code>:smile:</code>）转变成会跳跃的<code>emoji</code>表情，安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-filter-github-emojis --save</span><br></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">githubEmojis:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  className:</span> <span class="string">github-emoji</span></span><br><span class="line"><span class="attr">  inject:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  styles:</span></span><br><span class="line"><span class="attr">  customEmojis:</span></span><br></pre></td></tr></tbody></table></figure><h3 id="添加-RSS-订阅支持（可选的）"><a href="#添加-RSS-订阅支持（可选的）" class="headerlink" title="添加 RSS 订阅支持（可选的）"></a>添加 RSS 订阅支持（可选的）</h3><p>本主题中还使用到了 <a href="https://github.com/hexojs/hexo-generator-feed" target="_blank" rel="noopener">hexo-generator-feed</a> 的 Hexo 插件来做 <code>RSS</code>，安装命令如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-feed --save</span><br></pre></td></tr></tbody></table></figure><p>在 Hexo 根目录下的 <code>_config.yml</code> 文件中，新增以下的配置项：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">feed:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">atom</span></span><br><span class="line"><span class="attr">  path:</span> <span class="string">atom.xml</span></span><br><span class="line"><span class="attr">  limit:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">  hub:</span></span><br><span class="line"><span class="attr">  content:</span></span><br><span class="line"><span class="attr">  content_limit:</span> <span class="number">140</span></span><br><span class="line"><span class="attr">  content_limit_delim:</span> <span class="string">' '</span></span><br><span class="line"><span class="attr">  order_by:</span> <span class="bullet">-date</span></span><br></pre></td></tr></tbody></table></figure><p>执行 <code>hexo clean &amp;&amp; hexo g</code> 重新生成博客文件，然后在 <code>public</code> 文件夹中即可看到 <code>atom.xml</code> 文件，说明你已经安装成功了。</p><h3 id="添加-DaoVoice-在线聊天功能（可选的）"><a href="#添加-DaoVoice-在线聊天功能（可选的）" class="headerlink" title="添加 DaoVoice 在线聊天功能（可选的）"></a>添加 <a href="http://www.daovoice.io/" target="_blank" rel="noopener">DaoVoice</a> 在线聊天功能（可选的）</h3><p>前往 <a href="http://www.daovoice.io/" target="_blank" rel="noopener">DaoVoice</a> 官网注册并且获取 <code>app_id</code>，并将 <code>app_id</code> 填入主题的 <code>_config.yml</code> 文件中。</p><h3 id="添加-Tidio-在线聊天功能（可选的）"><a href="#添加-Tidio-在线聊天功能（可选的）" class="headerlink" title="添加 Tidio 在线聊天功能（可选的）"></a>添加 <a href="https://www.tidio.com/" target="_blank" rel="noopener">Tidio</a> 在线聊天功能（可选的）</h3><p>前往 <a href="https://www.tidio.com/" target="_blank" rel="noopener">Tidio</a> 官网注册并且获取 <code>Public Key</code>，并将 <code>Public Key</code> 填入主题的 <code>_config.yml</code> 文件中。</p><h3 id="修改页脚"><a href="#修改页脚" class="headerlink" title="修改页脚"></a>修改页脚</h3><p>页脚信息可能需要做定制化修改，而且它不便于做成配置信息，所以可能需要你自己去再修改和加工。修改的地方在主题文件的 <code>/layout/_partial/footer.ejs</code> 文件中，包括站点、使用的主题、访问量等。</p><h3 id="修改社交链接"><a href="#修改社交链接" class="headerlink" title="修改社交链接"></a>修改社交链接</h3><p>在主题的 <code>_config.yml</code> 文件中，默认支持 <code>QQ</code>、<code>GitHub</code> 和邮箱等的配置，你可以在主题文件的 <code>/layout/_partial/social-link.ejs</code> 文件中，新增、修改你需要的社交链接地址，增加链接可参考如下代码：</p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">%</span> <span class="attr">if</span> (<span class="attr">theme.socialLink.github</span>) { %&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"&lt;%= theme.socialLink.github %&gt;"</span> <span class="attr">class</span>=<span class="string">"tooltipped"</span> <span class="attr">target</span>=<span class="string">"_blank"</span> <span class="attr">data-tooltip</span>=<span class="string">"访问我的GitHub"</span> <span class="attr">data-position</span>=<span class="string">"top"</span> <span class="attr">data-delay</span>=<span class="string">"50"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">"fab fa-github"</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">%</span> } %&gt;</span></span><br></pre></td></tr></tbody></table></figure><p>其中，社交图标（如：<code>fa-github</code>）你可以在 <a href="https://fontawesome.com/icons" target="_blank" rel="noopener">Font Awesome</a> 中搜索找到。以下是常用社交图标的标识，供你参考：</p><ul><li>Facebook: <code>fab fa-facebook</code></li><li>Twitter: <code>fab fa-twitter</code></li><li>Google-plus: <code>fab fa-google-plus</code></li><li>Linkedin: <code>fab fa-linkedin</code></li><li>Tumblr: <code>fab fa-tumblr</code></li><li>Medium: <code>fab fa-medium</code></li><li>Slack: <code>fab fa-slack</code></li><li>Sina Weibo: <code>fab fa-weibo</code></li><li>Wechat: <code>fab fa-weixin</code></li><li>QQ: <code>fab fa-qq</code></li><li>Zhihu: <code>fab fa-zhihu</code></li></ul><blockquote><p><strong>注意</strong>: 本主题中使用的 <code>Font Awesome</code> 版本为 <code>5.11.0</code>。</p></blockquote><h3 id="修改打赏的二维码图片"><a href="#修改打赏的二维码图片" class="headerlink" title="修改打赏的二维码图片"></a>修改打赏的二维码图片</h3><p>在主题文件的 <code>source/medias/reward</code> 文件中，你可以替换成你的的微信和支付宝的打赏二维码图片。</p><h3 id="配置音乐播放器（可选的）"><a href="#配置音乐播放器（可选的）" class="headerlink" title="配置音乐播放器（可选的）"></a>配置音乐播放器（可选的）</h3><p>要支持音乐播放，就必须开启音乐的播放配置和音乐数据的文件。</p><p>首先，在你的博客 <code>source</code> 目录下的 <code>_data</code> 目录（没有的话就新建一个）中新建 <code>musics.json</code> 文件，文件内容如下所示：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[{</span><br><span class="line"><span class="attr">"name"</span>: <span class="string">"五月雨变奏电音"</span>,</span><br><span class="line"><span class="attr">"artist"</span>: <span class="string">"AnimeVibe"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://xxx.com/music1.mp3"</span>,</span><br><span class="line"><span class="attr">"cover"</span>: <span class="string">"http://xxx.com/music-cover1.png"</span></span><br><span class="line">}, {</span><br><span class="line"><span class="attr">"name"</span>: <span class="string">"Take me hand"</span>,</span><br><span class="line"><span class="attr">"artist"</span>: <span class="string">"DAISHI DANCE,Cecile Corbel"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"/medias/music/music2.mp3"</span>,</span><br><span class="line"><span class="attr">"cover"</span>: <span class="string">"/medias/music/cover2.png"</span></span><br><span class="line">}, {</span><br><span class="line"><span class="attr">"name"</span>: <span class="string">"Shape of You"</span>,</span><br><span class="line"><span class="attr">"artist"</span>: <span class="string">"J.Fla"</span>,</span><br><span class="line"><span class="attr">"url"</span>: <span class="string">"http://xxx.com/music3.mp3"</span>,</span><br><span class="line"><span class="attr">"cover"</span>: <span class="string">"http://xxx.com/music-cover3.png"</span></span><br><span class="line">}]</span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>注</strong>：以上 JSON 中的属性：<code>name</code>、<code>artist</code>、<code>url</code>、<code>cover</code> 分别表示音乐的名称、作者、音乐文件地址、音乐封面。</p></blockquote><p>然后，在主题的 <code>_config.yml</code> 配置文件中激活配置即可：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 是否在首页显示音乐.</span></span><br><span class="line"><span class="attr">music:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  showTitle:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  title:</span> <span class="string">听听音乐</span></span><br><span class="line"><span class="attr">  fixed:</span> <span class="literal">false</span> <span class="comment"># 是否开启吸底模式</span></span><br><span class="line"><span class="attr">  autoplay:</span> <span class="literal">false</span> <span class="comment"># 是否自动播放</span></span><br><span class="line"><span class="attr">  theme:</span> <span class="string">'#42b983'</span></span><br><span class="line"><span class="attr">  loop:</span> <span class="string">'all'</span> <span class="comment"># 音频循环播放, 可选值: 'all', 'one', 'none'</span></span><br><span class="line"><span class="attr">  order:</span> <span class="string">'list'</span> <span class="comment"># 音频循环顺序, 可选值: 'list', 'random'</span></span><br><span class="line"><span class="attr">  preload:</span> <span class="string">'auto'</span> <span class="comment"># 预加载，可选值: 'none', 'metadata', 'auto'</span></span><br><span class="line"><span class="attr">  volume:</span> <span class="number">0.7</span> <span class="comment"># 默认音量，请注意播放器会记忆用户设置，用户手动设置音量后默认音量即失效</span></span><br><span class="line"><span class="attr">  listFolded:</span> <span class="literal">false</span> <span class="comment"># 列表默认折叠</span></span><br><span class="line"><span class="attr">  listMaxHeight:</span> <span class="comment"># 列表最大高度</span></span><br></pre></td></tr></tbody></table></figure><h2 id="文章-Front-matter-介绍"><a href="#文章-Front-matter-介绍" class="headerlink" title="文章 Front-matter 介绍"></a>文章 Front-matter 介绍</h2><h3 id="Front-matter-选项详解"><a href="#Front-matter-选项详解" class="headerlink" title="Front-matter 选项详解"></a>Front-matter 选项详解</h3><p><code>Front-matter</code> 选项中的所有内容均为<strong>非必填</strong>的。但我仍然建议至少填写 <code>title</code> 和 <code>date</code> 的值。</p><table><thead><tr><th>配置选项</th><th>默认值</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td><code>Markdown</code> 的文件标题</td><td>文章标题，强烈建议填写此选项</td></tr><tr><td>date</td><td>文件创建时的日期时间</td><td>发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr><td>author</td><td>根 <code>_config.yml</code> 中的 <code>author</code></td><td>文章作者</td></tr><tr><td>img</td><td><code>featureImages</code> 中的某个值</td><td>文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径.如: <code>http://xxx.com/xxx.jpg</code></td></tr><tr><td>top</td><td><code>true</code></td><td>推荐文章（文章是否置顶），如果 <code>top</code> 值为 <code>true</code>，则会作为首页推荐文章</td></tr><tr><td>cover</td><td><code>false</code></td><td><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr><td>coverImg</td><td>无</td><td><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr><td>password</td><td>无</td><td>文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置 <code>password</code> 的值，该值必须是用 <code>SHA256</code> 加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code> 中激活了 <code>verifyPassword</code> 选项</td></tr><tr><td>toc</td><td><code>true</code></td><td>是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的 <code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr><td>mathjax</td><td><code>false</code></td><td>是否开启数学公式支持 ，本文章是否开启 <code>mathjax</code>，且需要在主题的 <code>_config.yml</code> 文件中也需要开启才行</td></tr><tr><td>summary</td><td>无</td><td>文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr><td>categories</td><td>无</td><td>文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr><td>tags</td><td>无</td><td>文章标签，一篇文章可以多个标签</td></tr><tr><td>keywords</td><td>文章标题</td><td>文章关键字，SEO 时需要</td></tr><tr><td>reprintPolicy</td><td>cc_by</td><td>文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc, cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table><blockquote><p><strong>注意</strong>:</p><ol><li>如果 <code>img</code> 属性不填写的话，文章特色图会根据文章标题的 <code>hashcode</code> 的值取余，然后选取主题中对应的特色图片，从而达到让所有文章都的特色图<strong>各有特色</strong>。</li><li><code>date</code> 的值尽量保证每篇文章是唯一的，因为本主题中 <code>Gitalk</code> 和 <code>Gitment</code> 识别 <code>id</code> 是通过 <code>date</code> 的值来作为唯一标识的。</li><li>如果要对文章设置阅读验证密码的功能，不仅要在 Front-matter 中设置采用了 SHA256 加密的 password 的值，还需要在主题的 <code>_config.yml</code> 中激活了配置。有些在线的 SHA256 加密的地址，可供你使用：<a href="http://tool.oschina.net/encrypt?type=2" target="_blank" rel="noopener">开源中国在线工具</a>、<a href="http://encode.chahuo.com/" target="_blank" rel="noopener">chahuo</a>、<a href="http://tool.chinaz.com/tools/hash.aspx" target="_blank" rel="noopener">站长工具</a>。</li><li>您可以在文章md文件的 front-matter 中指定 reprintPolicy 来给单个文章配置转载规则</li></ol></blockquote><p>以下为文章的 <code>Front-matter</code> 示例。</p><h3 id="最简示例"><a href="#最简示例" class="headerlink" title="最简示例"></a>最简示例</h3><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">typora-vue-theme主题介绍</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-09</span><span class="bullet">-07</span> <span class="number">09</span><span class="string">:25:00</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></tbody></table></figure><h3 id="最全示例"><a href="#最全示例" class="headerlink" title="最全示例"></a>最全示例</h3><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">typora-vue-theme主题介绍</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-09</span><span class="bullet">-07</span> <span class="number">09</span><span class="string">:25:00</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">赵奇</span></span><br><span class="line"><span class="attr">img:</span> <span class="string">/source/images/xxx.jpg</span></span><br><span class="line"><span class="attr">top:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">cover:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">coverImg:</span> <span class="string">/images/1.jpg</span></span><br><span class="line"><span class="attr">password:</span> <span class="number">8</span><span class="string">d969eef6ecad3c29a3a629280e686cf0c3f5d5a86aff3ca12020c923adc6c92</span></span><br><span class="line"><span class="attr">toc:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">summary:</span> <span class="string">这是你自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">Markdown</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">Typora</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">Markdown</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></tbody></table></figure><h3 id="诗词样式"><a href="#诗词样式" class="headerlink" title="诗词样式"></a>诗词样式</h3><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">糖</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">记某人的大痘</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020</span><span class="bullet">-02</span><span class="bullet">-02</span> <span class="number">14</span><span class="string">:36:59</span></span><br><span class="line"><span class="attr">layout:</span> <span class="string">"poem"</span> <span class="comment">#样式改为poem就行</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">"Qinng"</span></span><br><span class="line"><span class="attr">toc:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">诗</span></span><br><span class="line"><span class="attr">categories:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">杂记</span></span><br></pre></td></tr></tbody></table></figure><h2 id="效果截图"><a href="#效果截图" class="headerlink" title="效果截图"></a>效果截图</h2><p><img src="http://static.blinkfox.com/matery-20181202-1.png" alt="首页"></p><p><img src="http://static.blinkfox.com/matery-20181202-2.png" alt="首页推荐文章"></p><p><img src="http://static.blinkfox.com/matery-20181202-3.png" alt="首页文章列表"></p><p><img src="http://static.blinkfox.com/matery-20181202-7.png" alt="首页文章列表"></p><p><img src="http://static.blinkfox.com/matery-20181202-8.png" alt="首页文章列表"></p><h2 id="自定制修改"><a href="#自定制修改" class="headerlink" title="自定制修改"></a>自定制修改</h2><p>在本主题的 <code>_config.yml</code> 中可以修改部分自定义信息，有以下几个部分：</p><ul><li>菜单</li><li>我的梦想</li><li>首页的音乐播放器和视频播放器配置</li><li>是否显示推荐文章名称和按钮配置</li><li><code>favicon</code> 和 <code>Logo</code></li><li>个人信息</li><li>TOC 目录</li><li>文章打赏信息</li><li>复制文章内容时追加版权信息</li><li>MathJax</li><li>文章字数统计、阅读时长</li><li>点击页面的’爱心’效果</li><li>我的项目</li><li>我的技能</li><li>我的相册</li><li><code>Gitalk</code>、<code>Gitment</code>、<code>Valine</code> 和 <code>disqus</code> 评论配置</li><li><a href="http://busuanzi.ibruce.info/" target="_blank" rel="noopener">不蒜子统计</a>和谷歌分析（<code>Google Analytics</code>）</li><li>默认特色图的集合。当文章没有设置特色图时，本主题会根据文章标题的 <code>hashcode</code> 值取余，来选择展示对应的特色图</li></ul><p><strong>我认为个人博客应该都有自己的风格和特色</strong>。如果本主题中的诸多功能和主题色彩你不满意，可以在主题中自定义修改，很多更自由的功能和细节点的修改难以在主题的 <code>_config.yml</code> 中完成，需要修改源代码才来完成。以下列出了可能对你有用的地方：</p><h3 id="修改主题颜色"><a href="#修改主题颜色" class="headerlink" title="修改主题颜色"></a>修改主题颜色</h3><p>在主题文件的 <code>/source/css/matery.css</code> 文件中，搜索 <code>.bg-color</code> 来修改背景颜色：</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 整体背景颜色，包括导航、移动端的导航、页尾、标签页等的背景颜色. */</span></span><br><span class="line"><span class="selector-class">.bg-color</span> {</span><br><span class="line">    <span class="attribute">background-image</span>: <span class="built_in">linear-gradient</span>(to right, #4cbf30 0%, #0f9d58 100%);</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">@-<span class="keyword">webkit</span>-<span class="keyword">keyframes</span> rainbow {</span><br><span class="line">   <span class="comment">/* 动态切换背景颜色. */</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">@<span class="keyword">keyframes</span> rainbow {</span><br><span class="line">    <span class="comment">/* 动态切换背景颜色. */</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="修改-banner-图和文章特色图"><a href="#修改-banner-图和文章特色图" class="headerlink" title="修改 banner 图和文章特色图"></a>修改 banner 图和文章特色图</h3><p>你可以直接在 <code>/source/medias/banner</code> 文件夹中更换你喜欢的 <code>banner</code> 图片，主题代码中是每天动态切换一张，只需 <code>7</code> 张即可。如果你会 <code>JavaScript</code> 代码，可以修改成你自己喜欢切换逻辑，如：随机切换等，<code>banner</code> 切换的代码位置在 <code>/layout/_partial/bg-cover-content.ejs</code> 文件的 <code>&lt;script&gt;&lt;/script&gt;</code> 代码中：</p><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$(<span class="string">'.bg-cover'</span>).css(<span class="string">'background-image'</span>, <span class="string">'url(/medias/banner/'</span> + <span class="keyword">new</span> <span class="built_in">Date</span>().getDay() + <span class="string">'.jpg)'</span>);</span><br></pre></td></tr></tbody></table></figure><p>在 <code>/source/medias/featureimages</code> 文件夹中默认有 24 张特色图片，你可以再增加或者减少，并需要在 <code>_config.yml</code> 做同步修改。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WSL2+VSCode+Zsh打造Windows下Linux开发环境</title>
      <link href="/wsl2-vscode/"/>
      <url>/wsl2-vscode/</url>
      
        <content type="html"><![CDATA[<p>一直以来使用Ubuntu开发，前两天Ubuntu桌面环境崩了，一些工作软件在Ubuntu下很不好用，恰好WSL2(Windows Linux子系统)发布已经有一段日子，而且支持了Docker，上手看看可用性如何。</p><h2 id="配置WSL2"><a href="#配置WSL2" class="headerlink" title="配置WSL2"></a>配置WSL2</h2><h3 id="必要条件"><a href="#必要条件" class="headerlink" title="必要条件"></a>必要条件</h3><ul><li>Windows 10 Build 18917或更新版本</li><li>启用虚拟化</li></ul><h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3><ul><li>启用“虚拟机平台”可选组件，以管理员身份打开 PowerShell 并运行：<br><code>Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform</code></li><li>启用安装子系统<br><code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux</code><br>启用这些更改后，你需要重新启动计算机。</li><li>应用商店安装ubuntu，如<code>Ubuntu-18.04</code></li><li>使用命令行设置要由 WSL 2 支持的发行版，在 PowerShell 中运行：<br><code>wsl --set-version &lt;Distro&gt; 2</code></li></ul><h3 id="配置Ubuntu"><a href="#配置Ubuntu" class="headerlink" title="配置Ubuntu"></a>配置Ubuntu</h3><p>配置源，配置Sudo免密码，安装必要软件Python、Git、Docker等，终端美化可通过安装Zsh…</p><h2 id="安装VSCode-WSL插件"><a href="#安装VSCode-WSL插件" class="headerlink" title="安装VSCode WSL插件"></a>安装VSCode WSL插件</h2><p>VSCode已经支持了<a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl" target="_blank" rel="noopener">WSL插件</a></p><p>最终界面如下：<br><img src="/img/blogImg/wsl-vscode.png" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可以愉快的使用VSCode开发，目前也发现了几点小问题：</p><ul><li>Vscode Terminal改为WSL后，启动会有1-2秒延时</li><li>WSL2中的软件配置开机自启比较麻烦，网上有方案，我是通过快捷命令如启动 Docker <code>alias sds="sudo service docker start"</code></li><li>WSL2本质是个虚拟机，网络方式和本地有一定差异，对我来说影响不大</li></ul><p>目前在家办公已两周，此方案感觉良好。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> wsl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s+gitlab实现cicd</title>
      <link href="/k8s-gitlab-cicd/"/>
      <url>/k8s-gitlab-cicd/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前Gitlab11已经支持了Kubernetes Runner, 任务可以跑在Pod中。本文介绍如何通过CICD接入Kubernetes，开始前需要以下必备条件：</p><ul><li>Kubernetes集群</li><li>配置Kubernetes Runner, 网上有很多教程，若是生产环境或是多租户k8s集群，建议通过yaml手动配置；默认通过helm安装权限比较大，而且配置不灵活</li></ul><h2 id="CI过程"><a href="#CI过程" class="headerlink" title="CI过程"></a>CI过程</h2><p>通常编译镜像有三种方式：</p><ul><li>docker in docker：与物理方式类似，需要权限多，性能较差</li><li>kaniko：镜像编译工具，性能好</li></ul><p>我们使用kaniko编译镜像，push到镜像仓库，过程如下：</p><ol><li>配置变量<br>配置镜像相关变量，仓库的账户密码，推送的镜像名称<code>CI_REGISTRY_IMAGE</code>等<br><img src="/img/blogImg/gitlab-ci.png" alt="gitlab-ci"></li><li>gitlab-ci配置如下<figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">build:</span></span><br><span class="line"><span class="attr">  stage:</span> <span class="string">build</span></span><br><span class="line"><span class="attr">  image:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">gcr.io/kaniko-project/executor:debug</span></span><br><span class="line"><span class="attr">    entrypoint:</span> <span class="string">[""]</span></span><br><span class="line"><span class="attr">  script:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">echo</span> <span class="string">"{\"auths\":{\"$CI_REGISTRY\":{\"username\":\"$CI_REGISTRY_USER\",\"password\":\"$CI_REGISTRY_PASSWORD\"}}}"</span> <span class="string">&gt; /kaniko/.docker/config.json</span></span><br><span class="line"><span class="string">    - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG</span></span><br><span class="line"><span class="string"></span><span class="attr">  after_script:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">echo</span> <span class="string">"build completed"</span></span><br><span class="line"><span class="attr">  only:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">tags</span> <span class="comment"># 打tag才会执行，测试可去掉</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="CD过程"><a href="#CD过程" class="headerlink" title="CD过程"></a>CD过程</h2><p>CD即需要将生成的镜像更新到Kubernetes集群中，有如下几种方式：</p><ul><li>k8s restful api：需要对api较了解，更新过程需要调用<code>PATH</code>方法，不推荐</li><li>kubectl: 常规方式</li><li>helm: 如有可用的helm仓库，也可使用helm进行更新</li></ul><p>我们以kubectl为例，CD配置如下：</p><ol><li>配置变量<br>配置必须的集群地址，token，需要更新服务的namespace, container等</li><li><p>CD配置<br>配置与物理环境类似，首先配置kubectl token、集群等，最后调用<code>set image</code>更新服务</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line"><span class="attr">  image:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="attr">kubectl:1.17</span></span><br><span class="line"><span class="attr">    entrypoint:</span> <span class="string">[""]</span></span><br><span class="line"><span class="attr">  before_script:</span></span><br><span class="line"><span class="attr">  script:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">IMAGE=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">kubectl</span> <span class="string">config</span> <span class="string">set-credentials</span> <span class="string">$CD_USER</span> <span class="bullet">--token</span> <span class="string">$CD_APP_AK</span> </span><br><span class="line"><span class="bullet">    -</span> <span class="string">kubectl</span> <span class="string">config</span> <span class="string">set-cluster</span> <span class="string">$CD_CLUSTER</span> <span class="bullet">--server</span> <span class="attr">https://$CD_SERVER</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">kubectl</span> <span class="string">config</span> <span class="string">set-context</span> <span class="string">$CD_USER@$CD_CLUSTER/$CD_NAMESPACE</span> <span class="bullet">--user</span> <span class="string">$CD_USER</span> <span class="bullet">--cluster</span> <span class="string">$CD_CLUSTER</span> <span class="bullet">--namespace</span> <span class="string">$CD_NAMESPACE</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">kubectl</span> <span class="string">config</span> <span class="string">use-context</span> <span class="string">$CD_USER@$CD_CLUSTER/$CD_NAMESPACE</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">kubectl</span> <span class="string">set</span> <span class="string">image</span> <span class="bullet">-n</span> <span class="string">$CD_NAMESPACE</span> <span class="string">$CD_APP_TYPE/$CD_APP_NAME</span> <span class="string">$CD_CONTAINER=$IMAGE</span></span><br><span class="line"><span class="attr">  only:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">tags</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>运行结果</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl <span class="built_in">set</span> image -n <span class="variable">$CD_NAMESPACE</span> <span class="variable">$CD_APP_TYPE</span>/<span class="variable">$CD_APP_NAME</span> <span class="variable">$CD_CONTAINER</span>=<span class="variable">$IMAGE</span></span><br><span class="line">deployment.extensions/helloworld image updated</span><br><span class="line">Job succeeded</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h2><p>本文所列举的CICD过程较简单，可以使用CICD完成服务的多集群部署，更新结果检查等功能。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://docs.gitlab.com/ee/ci/docker/using_kaniko.html" target="_blank" rel="noopener">https://docs.gitlab.com/ee/ci/docker/using_kaniko.html</a></li><li><a href="https://docs.gitlab.com/runner/executors/kubernetes.html" target="_blank" rel="noopener">https://docs.gitlab.com/runner/executors/kubernetes.html</a></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> gitlab </tag>
            
            <tag> cicd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>糖</title>
      <link href="/candy/"/>
      <url>/candy/</url>
      
        <content type="html"><![CDATA[<p>一轮明月沉到了脸底<br>迷恋这红尘 不愿睡去<br>又变成红娘的大痣<br>在招摇过市</p><p>有了糖 她变得小心翼翼<br>吃饭睡觉也含着久久不肯放<br>是通灵宝玉 守护着片刻荣光<br>亦或是君子之遗 化作脸颊的相思</p><p>没人知道它的滋味<br>除了耳边的风<br>远方的惦记<br>以及轻轻拂过的吻</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 诗 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kube-apiserver认证源码分析</title>
      <link href="/kube-apiserver-authentication-code/"/>
      <url>/kube-apiserver-authentication-code/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>kube-apiserver中与权限相关的主要有三种机制，即认证、鉴权和准入控制。本文主要分析apiserver的认证流程。</p><h2 id="认证流程分析"><a href="#认证流程分析" class="headerlink" title="认证流程分析"></a>认证流程分析</h2><p>权限相关代码从<code>k8s.io/apiserver/pkg/server/config.go</code>中<code>DefaultBuildHandlerChain</code>函数开始执行</p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">DefaultBuildHandlerChain</span><span class="params">(apiHandler http.Handler, c *Config)</span> <span class="title">http</span>.<span class="title">Handler</span></span> {</span><br><span class="line">handler := genericapifilters.WithAuthorization(apiHandler, c.Authorization.Authorizer, c.Serializer)</span><br><span class="line">handler = genericfilters.WithMaxInFlightLimit(handler, c.MaxRequestsInFlight, c.MaxMutatingRequestsInFlight, c.LongRunningFunc)</span><br><span class="line">handler = genericapifilters.WithImpersonation(handler, c.Authorization.Authorizer, c.Serializer)</span><br><span class="line">handler = genericapifilters.WithAudit(handler, c.AuditBackend, c.AuditPolicyChecker, c.LongRunningFunc)</span><br><span class="line">failedHandler := genericapifilters.Unauthorized(c.Serializer, c.Authentication.SupportsBasicAuth)</span><br><span class="line">failedHandler = genericapifilters.WithFailedAuthenticationAudit(failedHandler, c.AuditBackend, c.AuditPolicyChecker)</span><br><span class="line">handler = genericapifilters.WithAuthentication(handler, c.Authentication.Authenticator, failedHandler, c.Authentication.APIAudiences)</span><br><span class="line">handler = genericfilters.WithCORS(handler, c.CorsAllowedOriginList, <span class="literal">nil</span>, <span class="literal">nil</span>, <span class="literal">nil</span>, <span class="string">"true"</span>)</span><br><span class="line">handler = genericfilters.WithTimeoutForNonLongRunningRequests(handler, c.LongRunningFunc, c.RequestTimeout)</span><br><span class="line">handler = genericfilters.WithWaitGroup(handler, c.LongRunningFunc, c.HandlerChainWaitGroup)</span><br><span class="line">handler = genericapifilters.WithRequestInfo(handler, c.RequestInfoResolver)</span><br><span class="line">handler = genericfilters.WithPanicRecovery(handler)</span><br><span class="line"><span class="keyword">return</span> handler</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><code>DefaultBuildHandlerChain</code>中包含了多种filter（如认证，链接数检验，RBAC权限检验等），认证步骤在<code>WithAuthorization</code>中，如下：</p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WithAuthentication creates an http handler that tries to authenticate the given request as a user, and then</span></span><br><span class="line"><span class="comment">// stores any such user found onto the provided context for the request. If authentication fails or returns an error</span></span><br><span class="line"><span class="comment">// the failed handler is used. On success, "Authorization" header is removed from the request and handler</span></span><br><span class="line"><span class="comment">// is invoked to serve the request.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithAuthentication</span><span class="params">(handler http.Handler, auth authenticator.Request, failed http.Handler, apiAuds authenticator.Audiences)</span> <span class="title">http</span>.<span class="title">Handler</span></span> {</span><br><span class="line"><span class="keyword">if</span> auth == <span class="literal">nil</span> {</span><br><span class="line">klog.Warningf(<span class="string">"Authentication is disabled"</span>)</span><br><span class="line"><span class="keyword">return</span> handler</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> http.HandlerFunc(<span class="function"><span class="keyword">func</span><span class="params">(w http.ResponseWriter, req *http.Request)</span></span> {</span><br><span class="line">authenticationStart := time.Now()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(apiAuds) &gt; <span class="number">0</span> {</span><br><span class="line">req = req.WithContext(authenticator.WithAudiences(req.Context(), apiAuds))</span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 认证请求</span></span><br><span class="line">resp, ok, err := auth.AuthenticateRequest(req)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> || !ok {</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">klog.Errorf(<span class="string">"Unable to authenticate the request due to an error: %v"</span>, err)</span><br><span class="line">authenticatedAttemptsCounter.WithLabelValues(errorLabel).Inc()</span><br><span class="line">authenticationLatency.WithLabelValues(errorLabel).Observe(time.Since(authenticationStart).Seconds())</span><br><span class="line">} <span class="keyword">else</span> <span class="keyword">if</span> !ok {</span><br><span class="line">authenticatedAttemptsCounter.WithLabelValues(failureLabel).Inc()</span><br><span class="line">authenticationLatency.WithLabelValues(failureLabel).Observe(time.Since(authenticationStart).Seconds())</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">failed.ServeHTTP(w, req)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(apiAuds) &gt; <span class="number">0</span> &amp;&amp; <span class="built_in">len</span>(resp.Audiences) &gt; <span class="number">0</span> &amp;&amp; <span class="built_in">len</span>(authenticator.Audiences(apiAuds).Intersect(resp.Audiences)) == <span class="number">0</span> {</span><br><span class="line">klog.Errorf(<span class="string">"Unable to match the audience: %v , accepted: %v"</span>, resp.Audiences, apiAuds)</span><br><span class="line">failed.ServeHTTP(w, req)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">        <span class="comment">// authorization header is not required anymore in case of a successful authentication.</span></span><br><span class="line">        <span class="comment">// 认证完则删除header认证信息，exec/log请求将不会携带Authorization，只使用token认证将无法通过</span></span><br><span class="line">req.Header.Del(<span class="string">"Authorization"</span>)</span><br><span class="line"></span><br><span class="line">req = req.WithContext(genericapirequest.WithUser(req.Context(), resp.User))</span><br><span class="line"></span><br><span class="line">authenticatedUserCounter.WithLabelValues(compressUsername(resp.User.GetName())).Inc()</span><br><span class="line">authenticatedAttemptsCounter.WithLabelValues(successLabel).Inc()</span><br><span class="line">authenticationLatency.WithLabelValues(successLabel).Observe(time.Since(authenticationStart).Seconds())</span><br><span class="line"></span><br><span class="line">handler.ServeHTTP(w, req)</span><br><span class="line">})</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><code>WithAuthentication</code>调用<code>AuthenticateRequest</code>进行认证：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AuthenticateRequest authenticates the request using a chain of authenticator.Request objects.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(authHandler *unionAuthRequestHandler)</span> <span class="title">AuthenticateRequest</span><span class="params">(req *http.Request)</span> <span class="params">(*authenticator.Response, <span class="keyword">bool</span>, error)</span></span> {</span><br><span class="line">    <span class="keyword">var</span> errlist []error</span><br><span class="line">    <span class="comment">// 按照Handlers顺序进行认证</span></span><br><span class="line">    <span class="keyword">for</span> _, currAuthRequestHandler := <span class="keyword">range</span> authHandler.Handlers {</span><br><span class="line">        resp, ok, err := currAuthRequestHandler.AuthenticateRequest(req)</span><br><span class="line">        <span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">            <span class="keyword">if</span> authHandler.FailOnError {</span><br><span class="line">                <span class="keyword">return</span> resp, ok, err</span><br><span class="line">            }</span><br><span class="line">            errlist = <span class="built_in">append</span>(errlist, err)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        }</span><br><span class="line"> </span><br><span class="line">        <span class="comment">// 只要有一个认证成功，则返回  </span></span><br><span class="line">        <span class="keyword">if</span> ok {</span><br><span class="line">            <span class="keyword">return</span> resp, ok, err</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">false</span>, utilerrors.NewAggregate(errlist)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>根据认证逻辑，会按照<code>authHandler.Handlers</code>顺序进行检验，只要有一个认证成功则返回。<code>Handlers</code>的定义在<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// New returns a request authenticator that validates credentials using a chain of authenticator.Request objects.</span></span><br><span class="line"><span class="comment">// The entire chain is tried until one succeeds. If all fail, an aggregate error is returned.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(authRequestHandlers ...authenticator.Request)</span> <span class="title">authenticator</span>.<span class="title">Request</span></span> {</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(authRequestHandlers) == <span class="number">1</span> {</span><br><span class="line"><span class="keyword">return</span> authRequestHandlers[<span class="number">0</span>]</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> &amp;unionAuthRequestHandler{Handlers: authRequestHandlers, FailOnError: <span class="literal">false</span>}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>最初的定义在<code>k8s.io/kubernetes/pkg/kubeapiserver/authenticator/config.go</code>中：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// New returns an authenticator.Request or an error that supports the standard</span></span><br><span class="line"><span class="comment">// Kubernetes authentication mechanisms.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(config Config)</span> <span class="title">New</span><span class="params">()</span> <span class="params">(authenticator.Request, *spec.SecurityDefinitions, error)</span></span> {</span><br><span class="line"><span class="keyword">var</span> authenticators []authenticator.Request</span><br><span class="line"><span class="keyword">var</span> tokenAuthenticators []authenticator.Token</span><br><span class="line">securityDefinitions := spec.SecurityDefinitions{}</span><br><span class="line"></span><br><span class="line"><span class="comment">// front-proxy, BasicAuth methods, local first, then remote</span></span><br><span class="line"><span class="comment">// Add the front proxy authenticator if requested</span></span><br><span class="line"><span class="keyword">if</span> config.RequestHeaderConfig != <span class="literal">nil</span> {</span><br><span class="line">requestHeaderAuthenticator := headerrequest.NewDynamicVerifyOptionsSecure(</span><br><span class="line">config.RequestHeaderConfig.CAContentProvider.VerifyOptions,</span><br><span class="line">config.RequestHeaderConfig.AllowedClientNames,</span><br><span class="line">config.RequestHeaderConfig.UsernameHeaders,</span><br><span class="line">config.RequestHeaderConfig.GroupHeaders,</span><br><span class="line">config.RequestHeaderConfig.ExtraHeaderPrefixes,</span><br><span class="line">)</span><br><span class="line">authenticators = <span class="built_in">append</span>(authenticators, authenticator.WrapAudienceAgnosticRequest(config.APIAudiences, requestHeaderAuthenticator))</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// basic auth</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(config.BasicAuthFile) &gt; <span class="number">0</span> {</span><br><span class="line">basicAuth, err := newAuthenticatorFromBasicAuthFile(config.BasicAuthFile)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">authenticators = <span class="built_in">append</span>(authenticators, authenticator.WrapAudienceAgnosticRequest(config.APIAudiences, basicAuth))</span><br><span class="line"></span><br><span class="line">securityDefinitions[<span class="string">"HTTPBasic"</span>] = &amp;spec.SecurityScheme{</span><br><span class="line">SecuritySchemeProps: spec.SecuritySchemeProps{</span><br><span class="line">Type:        <span class="string">"basic"</span>,</span><br><span class="line">Description: <span class="string">"HTTP Basic authentication"</span>,</span><br><span class="line">},</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// X509 methods</span></span><br><span class="line"><span class="keyword">if</span> config.ClientCAContentProvider != <span class="literal">nil</span> {</span><br><span class="line">certAuth := x509.NewDynamic(config.ClientCAContentProvider.VerifyOptions, x509.CommonNameUserConversion)</span><br><span class="line">authenticators = <span class="built_in">append</span>(authenticators, certAuth)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// Bearer token methods, local first, then remote</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(config.TokenAuthFile) &gt; <span class="number">0</span> {</span><br><span class="line">tokenAuth, err := newAuthenticatorFromTokenFile(config.TokenAuthFile)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">tokenAuthenticators = <span class="built_in">append</span>(tokenAuthenticators, authenticator.WrapAudienceAgnosticToken(config.APIAudiences, tokenAuth))</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(config.ServiceAccountKeyFiles) &gt; <span class="number">0</span> {</span><br><span class="line">serviceAccountAuth, err := newLegacyServiceAccountAuthenticator(config.ServiceAccountKeyFiles, config.ServiceAccountLookup, config.APIAudiences, config.ServiceAccountTokenGetter)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">tokenAuthenticators = <span class="built_in">append</span>(tokenAuthenticators, serviceAccountAuth)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> utilfeature.DefaultFeatureGate.Enabled(features.TokenRequest) &amp;&amp; config.ServiceAccountIssuer != <span class="string">""</span> {</span><br><span class="line">serviceAccountAuth, err := newServiceAccountAuthenticator(config.ServiceAccountIssuer, config.ServiceAccountKeyFiles, config.APIAudiences, config.ServiceAccountTokenGetter)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">tokenAuthenticators = <span class="built_in">append</span>(tokenAuthenticators, serviceAccountAuth)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> config.BootstrapToken {</span><br><span class="line"><span class="keyword">if</span> config.BootstrapTokenAuthenticator != <span class="literal">nil</span> {</span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> This can sometimes be nil because of</span></span><br><span class="line">tokenAuthenticators = <span class="built_in">append</span>(tokenAuthenticators, authenticator.WrapAudienceAgnosticToken(config.APIAudiences, config.BootstrapTokenAuthenticator))</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="comment">// NOTE(ericchiang): Keep the OpenID Connect after Service Accounts.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Because both plugins verify JWTs whichever comes first in the union experiences</span></span><br><span class="line"><span class="comment">// cache misses for all requests using the other. While the service account plugin</span></span><br><span class="line"><span class="comment">// simply returns an error, the OpenID Connect plugin may query the provider to</span></span><br><span class="line"><span class="comment">// update the keys, causing performance hits.</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(config.OIDCIssuerURL) &gt; <span class="number">0</span> &amp;&amp; <span class="built_in">len</span>(config.OIDCClientID) &gt; <span class="number">0</span> {</span><br><span class="line">oidcAuth, err := newAuthenticatorFromOIDCIssuerURL(oidc.Options{</span><br><span class="line">IssuerURL:            config.OIDCIssuerURL,</span><br><span class="line">ClientID:             config.OIDCClientID,</span><br><span class="line">APIAudiences:         config.APIAudiences,</span><br><span class="line">CAFile:               config.OIDCCAFile,</span><br><span class="line">UsernameClaim:        config.OIDCUsernameClaim,</span><br><span class="line">UsernamePrefix:       config.OIDCUsernamePrefix,</span><br><span class="line">GroupsClaim:          config.OIDCGroupsClaim,</span><br><span class="line">GroupsPrefix:         config.OIDCGroupsPrefix,</span><br><span class="line">SupportedSigningAlgs: config.OIDCSigningAlgs,</span><br><span class="line">RequiredClaims:       config.OIDCRequiredClaims,</span><br><span class="line">})</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">tokenAuthenticators = <span class="built_in">append</span>(tokenAuthenticators, oidcAuth)</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(config.WebhookTokenAuthnConfigFile) &gt; <span class="number">0</span> {</span><br><span class="line">webhookTokenAuth, err := newWebhookTokenAuthenticator(config.WebhookTokenAuthnConfigFile, config.WebhookTokenAuthnVersion, config.WebhookTokenAuthnCacheTTL, config.APIAudiences)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, <span class="literal">nil</span>, err</span><br><span class="line">}</span><br><span class="line">tokenAuthenticators = <span class="built_in">append</span>(tokenAuthenticators, webhookTokenAuth)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(tokenAuthenticators) &gt; <span class="number">0</span> {</span><br><span class="line"><span class="comment">// Union the token authenticators</span></span><br><span class="line">tokenAuth := tokenunion.New(tokenAuthenticators...)</span><br><span class="line"><span class="comment">// Optionally cache authentication results</span></span><br><span class="line"><span class="keyword">if</span> config.TokenSuccessCacheTTL &gt; <span class="number">0</span> || config.TokenFailureCacheTTL &gt; <span class="number">0</span> {</span><br><span class="line">tokenAuth = tokencache.New(tokenAuth, <span class="literal">true</span>, config.TokenSuccessCacheTTL, config.TokenFailureCacheTTL)</span><br><span class="line">}</span><br><span class="line">authenticators = <span class="built_in">append</span>(authenticators, bearertoken.New(tokenAuth), websocket.NewProtocolAuthenticator(tokenAuth))</span><br><span class="line">securityDefinitions[<span class="string">"BearerToken"</span>] = &amp;spec.SecurityScheme{</span><br><span class="line">SecuritySchemeProps: spec.SecuritySchemeProps{</span><br><span class="line">Type:        <span class="string">"apiKey"</span>,</span><br><span class="line">Name:        <span class="string">"authorization"</span>,</span><br><span class="line">In:          <span class="string">"header"</span>,</span><br><span class="line">Description: <span class="string">"Bearer Token authentication"</span>,</span><br><span class="line">},</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(authenticators) == <span class="number">0</span> {</span><br><span class="line"><span class="keyword">if</span> config.Anonymous {</span><br><span class="line"><span class="keyword">return</span> anonymous.NewAuthenticator(), &amp;securityDefinitions, <span class="literal">nil</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, &amp;securityDefinitions, <span class="literal">nil</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">authenticator := union.New(authenticators...)</span><br><span class="line"></span><br><span class="line">authenticator = group.NewAuthenticatedGroupAdder(authenticator)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> config.Anonymous {</span><br><span class="line"><span class="comment">// If the authenticator chain returns an error, return an error (don't consider a bad bearer token</span></span><br><span class="line"><span class="comment">// or invalid username/password combination anonymous).</span></span><br><span class="line">authenticator = union.NewFailOnError(authenticator, anonymous.NewAuthenticator())</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> authenticator, &amp;securityDefinitions, <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>认证顺序即代码执行顺序:</p><ol><li>Request header, RequestHeader认证，需配置<code>--requestheader-username-headers</code></li><li>Basic auth, 账号密码认证，通过文件<code>--basic-auth-file=SOMEFILE</code>配置对应用户</li><li>X509, 证书认证</li><li>Static token, 通过文件<code>--token-auth-file=SOMEFILE</code>匹配用户</li><li>ServiceAccout token, 一般用于认证Pod</li><li>Bootstrap token, 用于集群初始化阶段，通过配置<code>--experimental-bootstrap-token-auth</code>启用</li><li>OpenID Connect token, OAuth2认证</li><li>Webhook token, 通过webhook认证token，需配置<code>--authentication-token-webhook-config-file</code></li><li>Cache auth, 通过cache认证</li><li>Anonymous， 以上认证未通过则返回匿名用户</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Apiserver的认证方式有多种，通过源码分析每次请求都会安装固定的认证顺序执行，高qps下认证配置势必会影响Apiserver的响应延迟，需要根据集群的实际情况配置合理的认证方式。</p><p>目前在我们的线上系统，主要通过RequestHeader(认证普通用户)，基本认证(个别系统组件)，X509（认证kubelet），ServieceAccout（认证Pod）进行认证，仅供参考。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s基于资源锁的选主分析</title>
      <link href="/k8s-leaderelection-code/"/>
      <url>/k8s-leaderelection-code/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>k8s中为了实现高可用，需要部署多个副本，例如多个apiserver、scheduler、controller-manager等，其中apiserver是无状态的每个组件都可以工作，而scheduler与controller-manager是有状态的，同一时刻只能存在一个活跃的，需要进行选主。</p><p>k8s使用了资源锁（endpoints/configmap/lease）的方式来实现选主，多个副本去创建资源，创建成功则获得锁成为leader，leader在租约内去刷新锁，其他副本则通过比对锁的更新时间判断是否成为新的leader。</p><p>k8s采用了资源版本号的乐观锁方式来实现选主，对比etcd选主，效率更高，并发性更好。</p><h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><p>k8s选主实现在client-go中，包<code>k8s.io/client-go/tools/leaderelection</code></p><h3 id="结构定义"><a href="#结构定义" class="headerlink" title="结构定义"></a>结构定义</h3><p>锁结构定义如下：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/client-go/tools/leaderelection/resourcelock/interface.go</span></span><br><span class="line"><span class="keyword">type</span> LeaderElectionRecord <span class="keyword">struct</span> {</span><br><span class="line">  <span class="comment">// leader 标识，通常为 hostname</span></span><br><span class="line">  HolderIdentity       <span class="keyword">string</span>           <span class="string">`json:"holderIdentity"`</span></span><br><span class="line">  <span class="comment">// 同启动参数 --leader-elect-lease-duration</span></span><br><span class="line">  LeaseDurationSeconds <span class="keyword">int</span>              <span class="string">`json:"leaseDurationSeconds"`</span></span><br><span class="line">  <span class="comment">// Leader 第一次成功获得租约时的时间戳</span></span><br><span class="line">  AcquireTime          unversioned.Time <span class="string">`json:"acquireTime"`</span></span><br><span class="line">  <span class="comment">// leader 定时 renew 的时间戳</span></span><br><span class="line">  RenewTime            unversioned.Time <span class="string">`json:"renewTime"`</span></span><br><span class="line">  LeaderTransitions    <span class="keyword">int</span>              <span class="string">`json:"leaderTransitions"`</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>k8s中的选举锁需实现<code>resourcelock.Interface</code>接口，基本上实现CRU，将leader信息存在在annotation中<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/client-go/tools/leaderelection/resourcelock/interface.go</span></span><br><span class="line"><span class="keyword">type</span> Interface <span class="keyword">interface</span> {</span><br><span class="line"><span class="comment">// Get returns the LeaderElectionRecord</span></span><br><span class="line">Get() (*LeaderElectionRecord, []<span class="keyword">byte</span>, error)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create attempts to create a LeaderElectionRecord</span></span><br><span class="line">Create(ler LeaderElectionRecord) error</span><br><span class="line"></span><br><span class="line"><span class="comment">// Update will update and existing LeaderElectionRecord</span></span><br><span class="line">Update(ler LeaderElectionRecord) error</span><br><span class="line"></span><br><span class="line"><span class="comment">// RecordEvent 记录锁切换事件</span></span><br><span class="line">RecordEvent(<span class="keyword">string</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Identity will return the locks Identity</span></span><br><span class="line">Identity() <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Describe is used to convert details on current resource lock</span></span><br><span class="line"><span class="comment">// into a string</span></span><br><span class="line">Describe() <span class="keyword">string</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="创建资源锁"><a href="#创建资源锁" class="headerlink" title="创建资源锁"></a>创建资源锁</h3><p>锁类型包括：configmaps， endpoints, lease, 以及 multiLock<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/client-go/tools/leaderelection/resourcelock/interface.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(lockType <span class="keyword">string</span>, ns <span class="keyword">string</span>, name <span class="keyword">string</span>, coreClient corev1.CoreV1Interface, coordinationClient coordinationv1.CoordinationV1Interface, rlc ResourceLockConfig)</span> <span class="params">(Interface, error)</span></span> {</span><br><span class="line">endpointsLock := &amp;EndpointsLock{</span><br><span class="line">EndpointsMeta: metav1.ObjectMeta{</span><br><span class="line">Namespace: ns,</span><br><span class="line">Name:      name,</span><br><span class="line">},</span><br><span class="line">Client:     coreClient,</span><br><span class="line">LockConfig: rlc,</span><br><span class="line">}</span><br><span class="line">configmapLock := &amp;ConfigMapLock{</span><br><span class="line">ConfigMapMeta: metav1.ObjectMeta{</span><br><span class="line">Namespace: ns,</span><br><span class="line">Name:      name,</span><br><span class="line">},</span><br><span class="line">Client:     coreClient,</span><br><span class="line">LockConfig: rlc,</span><br><span class="line">}</span><br><span class="line">leaseLock := &amp;LeaseLock{</span><br><span class="line">LeaseMeta: metav1.ObjectMeta{</span><br><span class="line">Namespace: ns,</span><br><span class="line">Name:      name,</span><br><span class="line">},</span><br><span class="line">Client:     coordinationClient,</span><br><span class="line">LockConfig: rlc,</span><br><span class="line">}</span><br><span class="line"><span class="keyword">switch</span> lockType {</span><br><span class="line"><span class="keyword">case</span> EndpointsResourceLock:</span><br><span class="line"><span class="keyword">return</span> endpointsLock, <span class="literal">nil</span></span><br><span class="line"><span class="keyword">case</span> ConfigMapsResourceLock:</span><br><span class="line"><span class="keyword">return</span> configmapLock, <span class="literal">nil</span></span><br><span class="line"><span class="keyword">case</span> LeasesResourceLock:</span><br><span class="line"><span class="keyword">return</span> leaseLock, <span class="literal">nil</span></span><br><span class="line"><span class="keyword">case</span> EndpointsLeasesResourceLock:</span><br><span class="line"><span class="keyword">return</span> &amp;MultiLock{</span><br><span class="line">Primary:   endpointsLock,</span><br><span class="line">Secondary: leaseLock,</span><br><span class="line">}, <span class="literal">nil</span></span><br><span class="line"><span class="keyword">case</span> ConfigMapsLeasesResourceLock:</span><br><span class="line"><span class="keyword">return</span> &amp;MultiLock{</span><br><span class="line">Primary:   configmapLock,</span><br><span class="line">Secondary: leaseLock,</span><br><span class="line">}, <span class="literal">nil</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"Invalid lock-type %s"</span>, lockType)</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>使用者首先通过<code>new()</code>函数创建资源锁，需要提供锁类型、namespace、name、唯一标示等。</p><h3 id="进行选举"><a href="#进行选举" class="headerlink" title="进行选举"></a>进行选举</h3><p>创建选举配置，通常如下：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// start the leader election code loop</span></span><br><span class="line">leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{</span><br><span class="line">    <span class="comment">// 资源锁类型</span></span><br><span class="line">    Lock: lock,</span><br><span class="line">    <span class="comment">// 租约时长，非主候选者用来判断资源锁是否过期</span></span><br><span class="line">    LeaseDuration:   <span class="number">60</span> * time.Second,</span><br><span class="line">    <span class="comment">// leader刷新资源锁超时时间   </span></span><br><span class="line">    RenewDeadline:   <span class="number">15</span> * time.Second,</span><br><span class="line">    <span class="comment">// 调用资源锁间隔</span></span><br><span class="line">    RetryPeriod:     <span class="number">5</span> * time.Second,</span><br><span class="line">    <span class="comment">// 回调函数，根据选举不同事件触发</span></span><br><span class="line">    Callbacks: leaderelection.LeaderCallbacks{</span><br><span class="line">        OnStartedLeading: <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context)</span></span> {</span><br><span class="line">            run(ctx)</span><br><span class="line">        },</span><br><span class="line">        OnStoppedLeading: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line">            klog.Infof(<span class="string">"leader lost: %s"</span>, id)</span><br><span class="line">            os.Exit(<span class="number">0</span>) <span class="comment">// 必须要退出，重启开始选主，否则将不会参与到选主中</span></span><br><span class="line">        },</span><br><span class="line">        OnNewLeader: <span class="function"><span class="keyword">func</span><span class="params">(identity <span class="keyword">string</span>)</span></span> {</span><br><span class="line">            <span class="keyword">if</span> identity == id {</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            }</span><br><span class="line">            klog.Infof(<span class="string">"new leader elected: %s"</span>, identity)</span><br><span class="line">        },</span><br><span class="line">    },</span><br><span class="line">})</span><br></pre></td></tr></tbody></table></figure><p></p><p>创建选举对象后，执行<code>Run</code>函数开始选主</p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/client-go/tools/leaderelection/leaderelection.go</span></span><br><span class="line"><span class="comment">// Run starts the leader election loop</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">Run</span><span class="params">(ctx context.Context)</span></span> {</span><br><span class="line"><span class="keyword">defer</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line">        runtime.HandleCrash()</span><br><span class="line">        <span class="comment">// 锁丢失时执行OnStoppedLeading回调函数</span></span><br><span class="line">le.config.Callbacks.OnStoppedLeading()</span><br><span class="line">    }()</span><br><span class="line">    <span class="comment">// 尝试获得锁</span></span><br><span class="line"><span class="keyword">if</span> !le.acquire(ctx) {</span><br><span class="line"><span class="keyword">return</span> <span class="comment">// ctx signalled done</span></span><br><span class="line">}</span><br><span class="line">ctx, cancel := context.WithCancel(ctx)</span><br><span class="line">    <span class="keyword">defer</span> cancel()</span><br><span class="line">    <span class="comment">// 获得锁后执行OnStartedLeading回调函数</span></span><br><span class="line"><span class="keyword">go</span> le.config.Callbacks.OnStartedLeading(ctx)</span><br><span class="line">    <span class="comment">// 定期刷新锁</span></span><br><span class="line">    le.renew(ctx)</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>acruire方法：</p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/client-go/tools/leaderelection/leaderelection.go</span></span><br><span class="line"><span class="comment">// acquire loops calling tryAcquireOrRenew and returns true immediately when tryAcquireOrRenew succeeds.</span></span><br><span class="line"><span class="comment">// Returns false if ctx signals done.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">acquire</span><span class="params">(ctx context.Context)</span> <span class="title">bool</span></span> {</span><br><span class="line">ctx, cancel := context.WithCancel(ctx)</span><br><span class="line"><span class="keyword">defer</span> cancel()</span><br><span class="line">succeeded := <span class="literal">false</span></span><br><span class="line">desc := le.config.Lock.Describe()</span><br><span class="line">    klog.Infof(<span class="string">"attempting to acquire leader lease  %v..."</span>, desc)</span><br><span class="line">    <span class="comment">// 调用 JitterUntil 函数，以 RetryPeriod 为间隔去刷新资源锁，直到获取锁</span></span><br><span class="line">wait.JitterUntil(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line">        <span class="comment">// tryAcquireOrRenew 方法去调度资源更新接口，判断是否刷新成功</span></span><br><span class="line">succeeded = le.tryAcquireOrRenew()</span><br><span class="line">le.maybeReportTransition()</span><br><span class="line"><span class="keyword">if</span> !succeeded {</span><br><span class="line">klog.V(<span class="number">4</span>).Infof(<span class="string">"failed to acquire lease %v"</span>, desc)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">le.config.Lock.RecordEvent(<span class="string">"became leader"</span>)</span><br><span class="line">le.metrics.leaderOn(le.config.Name)</span><br><span class="line">klog.Infof(<span class="string">"successfully acquired lease %v"</span>, desc)</span><br><span class="line">cancel()</span><br><span class="line">}, le.config.RetryPeriod, JitterFactor, <span class="literal">true</span>, ctx.Done())</span><br><span class="line"><span class="keyword">return</span> succeeded</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>renew方法，只有在获取锁之后才会调用，它会通过持续更新资源锁的数据，来确保继续持有已获得的锁，保持自己的leader 状态。</p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// renew loops calling tryAcquireOrRenew and returns immediately when tryAcquireOrRenew fails or ctx signals done.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">renew</span><span class="params">(ctx context.Context)</span></span> {</span><br><span class="line">ctx, cancel := context.WithCancel(ctx)</span><br><span class="line"><span class="keyword">defer</span> cancel()</span><br><span class="line">wait.Until(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line">timeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline)</span><br><span class="line">        <span class="keyword">defer</span> timeoutCancel()</span><br><span class="line">        <span class="comment">// </span></span><br><span class="line">err := wait.PollImmediateUntil(le.config.RetryPeriod, <span class="function"><span class="keyword">func</span><span class="params">()</span> <span class="params">(<span class="keyword">bool</span>, error)</span></span> {</span><br><span class="line">done := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">bool</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> {</span><br><span class="line"><span class="keyword">defer</span> <span class="built_in">close</span>(done)</span><br><span class="line">done &lt;- le.tryAcquireOrRenew()</span><br><span class="line">}()</span><br><span class="line">            <span class="comment">// 超时返回error, 否则返回更新结果</span></span><br><span class="line"><span class="keyword">select</span> {</span><br><span class="line"><span class="keyword">case</span> &lt;-timeoutCtx.Done():</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span>, fmt.Errorf(<span class="string">"failed to tryAcquireOrRenew %s"</span>, timeoutCtx.Err())</span><br><span class="line"><span class="keyword">case</span> result := &lt;-done:</span><br><span class="line"><span class="keyword">return</span> result, <span class="literal">nil</span></span><br><span class="line">}</span><br><span class="line">}, timeoutCtx.Done())</span><br><span class="line"></span><br><span class="line">le.maybeReportTransition()</span><br><span class="line">desc := le.config.Lock.Describe()</span><br><span class="line"><span class="keyword">if</span> err == <span class="literal">nil</span> {</span><br><span class="line">klog.V(<span class="number">5</span>).Infof(<span class="string">"successfully renewed lease %v"</span>, desc)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">}</span><br><span class="line">le.config.Lock.RecordEvent(<span class="string">"stopped leading"</span>)</span><br><span class="line">le.metrics.leaderOff(le.config.Name)</span><br><span class="line">klog.Infof(<span class="string">"failed to renew lease %v: %v"</span>, desc, err)</span><br><span class="line">cancel()</span><br><span class="line">}, le.config.RetryPeriod, ctx.Done())</span><br><span class="line"></span><br><span class="line"><span class="comment">// if we hold the lease, give it up</span></span><br><span class="line"><span class="keyword">if</span> le.config.ReleaseOnCancel {</span><br><span class="line">le.release()</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>这里使用了wait包，<code>wait.Until</code>会不断的调用<code>wait.PollImmediateUntil</code>方法，前者是进行无限循环操作，直到 <code>stop chan</code>被关闭，<code>wait.PollImmediateUntil</code>则不断的对某一条件进行检查，以<code>RetryPeriod</code>为间隔，直到该条件返回true、error或者超时。这一条件是一个需要满足 func() (bool, error) 签名的方法，比如这个例子只是调用了 <code>le.tryAcquireOrRenew()</code>。</p><p>最后看下<code>tryAcquireOrRenew</code>方法：<br></p><figure class="highlight go"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tryAcquireOrRenew tries to acquire a leader lease if it is not already acquired,</span></span><br><span class="line"><span class="comment">// else it tries to renew the lease if it has already been acquired. Returns true</span></span><br><span class="line"><span class="comment">// on success else returns false.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(le *LeaderElector)</span> <span class="title">tryAcquireOrRenew</span><span class="params">()</span> <span class="title">bool</span></span> {</span><br><span class="line">    now := metav1.Now()</span><br><span class="line">    <span class="comment">// 这个 leaderElectionRecord 就是保存在 endpoint/configmap 的 annotation 中的值。</span></span><br><span class="line">    <span class="comment">// 每个节点都将 HolderIdentity 设置为自己，以及关于获取和更新锁的时间。后面会对时间进行修正，才会更新到 API server</span></span><br><span class="line">leaderElectionRecord := rl.LeaderElectionRecord{</span><br><span class="line">HolderIdentity:       le.config.Lock.Identity(),</span><br><span class="line">LeaseDurationSeconds: <span class="keyword">int</span>(le.config.LeaseDuration / time.Second),</span><br><span class="line">RenewTime:            now,</span><br><span class="line">AcquireTime:          now,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1. 获取或者创建 ElectionRecord</span></span><br><span class="line">oldLeaderElectionRecord, oldLeaderElectionRawRecord, err := le.config.Lock.Get()</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> {</span><br><span class="line">        <span class="comment">// 记录不存在的话，则创建一条新的记录</span></span><br><span class="line"><span class="keyword">if</span> !errors.IsNotFound(err) {</span><br><span class="line">klog.Errorf(<span class="string">"error retrieving resource lock %v: %v"</span>, le.config.Lock.Describe(), err)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> err = le.config.Lock.Create(leaderElectionRecord); err != <span class="literal">nil</span> {</span><br><span class="line">klog.Errorf(<span class="string">"error initially creating leader election record: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">        }</span><br><span class="line">        <span class="comment">// 创建记录成功，同时表示获得了锁，返回true</span></span><br><span class="line">le.observedRecord = leaderElectionRecord</span><br><span class="line">le.observedTime = le.clock.Now()</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2. 正常获取了锁资源的记录，检查锁持有者和更新时间。</span></span><br><span class="line"><span class="keyword">if</span> !bytes.Equal(le.observedRawRecord, oldLeaderElectionRawRecord) {</span><br><span class="line">        <span class="comment">// 记录之前的锁持有者</span></span><br><span class="line">le.observedRecord = *oldLeaderElectionRecord</span><br><span class="line">le.observedRawRecord = oldLeaderElectionRawRecord</span><br><span class="line">le.observedTime = le.clock.Now()</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 在满足以下所有的条件下，认为锁由他人持有，并且还没有过期，返回 false</span></span><br><span class="line">    <span class="comment">// a. 当前锁持有者的并非自己</span></span><br><span class="line">    <span class="comment">// b. 上一次观察时间 + 观测检查间隔大于现在时间，即距离上次观测的间隔，小于 `LeaseDuration` 的设置值。</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(oldLeaderElectionRecord.HolderIdentity) &gt; <span class="number">0</span> &amp;&amp;</span><br><span class="line">le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp;</span><br><span class="line">!le.IsLeader() {</span><br><span class="line">klog.V(<span class="number">4</span>).Infof(<span class="string">"lock is held by %v and has not yet expired"</span>, oldLeaderElectionRecord.HolderIdentity)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 更新资源的资源锁</span></span><br><span class="line"><span class="keyword">if</span> le.IsLeader() {</span><br><span class="line">leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime</span><br><span class="line">leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + <span class="number">1</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 调用资源锁更新接口</span></span><br><span class="line"><span class="keyword">if</span> err = le.config.Lock.Update(leaderElectionRecord); err != <span class="literal">nil</span> {</span><br><span class="line">klog.Errorf(<span class="string">"Failed to update lock: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">le.observedRecord = leaderElectionRecord</span><br><span class="line">le.observedTime = le.clock.Now()</span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>当应用在k8s上部署时，使用k8s的资源锁，可方便的实现高可用，但需要注意以下几点：</p><ul><li>推荐使用<code>configmap</code>作为资源锁，原因是某些组件如<code>kube-proxy</code>会去监听<code>endpoints</code>来更新节点iptables规则，当有大量资源锁时，势必会对性能有影响。</li><li>当选举结束时调用<code>OnStoppedLeading</code>需要<strong>退出程序</strong>(例如<code>os.Exit(0)</code>)，若不退出程序，所有副本选举结束不会去竞争资源锁，就没有leader，造成服务不可用而这时程序并没有异常。需要执行退出逻辑，让Daemon程序k8s/systemd等重启服务来重新参与选主。</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cgroup引起的应用延迟</title>
      <link href="/lxcfs-high-system-cpu/"/>
      <url>/lxcfs-high-system-cpu/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>用户发现线上某容器请求hbase延迟较大，其他容器无类似现象，发现问题容器宿主机系统cpu占用较大（30%左右，正常在5%以下）。通过top查看lxcfs占用cpu较多（200%以上）。</p><h2 id="探究"><a href="#探究" class="headerlink" title="探究"></a>探究</h2><p>查看宿主机(内核 4.9.2)<code>top</code>,<code>1</code>显示每个cpu使用信息。查看最高的cpu占用是lxcfs造成的。</p><p><code>strace</code>查看lxcfs调用<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看调用情况，read占用99%</span></span><br><span class="line">$ strace -p 18521 -c</span><br><span class="line">% time     seconds  usecs/call     calls    errors syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"> 99.82   78.360112       39797      1969           <span class="built_in">read</span></span><br><span class="line">  0.11    0.088295         122       722           munmap</span><br><span class="line">  0.01    0.011649         416        28           wait4</span><br><span class="line">  0.01    0.010611          14       736           open</span><br><span class="line">  0.01    0.005685          75        76        18 futex</span><br><span class="line">  0.01    0.005288           7       792           close</span><br><span class="line">  0.01    0.005115          14       366           writev</span><br><span class="line">  0.01    0.004750           7       722           mmap</span><br><span class="line">  0.00    0.003552           5       722           fstat</span><br><span class="line">  0.00    0.002989         107        28           epoll_wait</span><br><span class="line">  0.00    0.002102          17       126           <span class="built_in">stat</span></span><br><span class="line">  0.00    0.000202          14        14           socketpair</span><br><span class="line">  0.00    0.000157          11        14           write</span><br><span class="line">  0.00    0.000122           4        28           epoll_create</span><br><span class="line">  0.00    0.000111           8        14           recvmsg</span><br><span class="line">  0.00    0.000104           4        28           epoll_ctl</span><br><span class="line">  0.00    0.000091           3        28           <span class="built_in">clone</span></span><br><span class="line">  0.00    0.000071           5        14           setsockopt</span><br><span class="line">  0.00    0.000059           4        14           setns</span><br><span class="line">  0.00    0.000012           1        14           recvfrom</span><br><span class="line">  0.00    0.000011           1        14           sendmsg</span><br><span class="line">  0.00    0.000003           0        14           set_robust_list</span><br><span class="line">  0.00    0.000000           0        14           getpid</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看详细情况，大量读取cgroup下memory的调用</span></span><br><span class="line">$ strace -p 18521 -f -T -tt -o lx.log</span><br><span class="line"></span><br><span class="line">cat lx.log</span><br><span class="line">79153 14:20:31.122630 open(<span class="string">"/run/lxcfs/controllers/memory//kubepods/burstable/pod7077217d-de6f-11e9-9352-246e96d53468/bcac6516ca5b2a60880fcbc752bf6878ddc77905db71269d852d17f5dc90b148/memory.memsw.limit_in_bytes"</span>, O_RDONLY) = 5 &lt;0.000017&gt;</span><br></pre></td></tr></tbody></table></figure><p></p><p>经发现某个pod调用的次数明显高于其他pod，排查到其容器内每隔2s执行<code>ps -auf</code>，会调用/proc/pid/stat其中就有memory相关的。<br>开开心心联系业务将其驱逐，宿主机没有明显变化。。。，再次查看<code>top</code><br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">top - 13:43:56 up 120 days, 19:21,  1 user,  load average: 6.59, 3.26, 2.34</span><br><span class="line">Tasks: 630 total,   1 running, 629 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.8 us,  7.1 sy,  0.0 ni, 92.1 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem : 13170992+total, 93100928 free,  7571536 used, 31037456 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used. 11042460+avail Mem </span><br><span class="line"></span><br><span class="line">   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                                       </span><br><span class="line"> 20686 root      20   0  141152  49032  17960 S  51.3  0.0   5890:25 cadvisor                                                                                                                                      </span><br><span class="line">115798 root      20   0       0      0      0 D  19.5  0.0   0:09.62 kworker/14:0                                                                                                                                  </span><br><span class="line"> 95501 root      20   0       0      0      0 D  17.2  0.0   0:10.11 kworker/0:1                                                                                                                                   </span><br><span class="line"> 38620 root      20   0       0      0      0 D  13.9  0.0   0:07.92 kworker/2:1                                                                                                                                   </span><br><span class="line">111178 root      20   0       0      0      0 D  13.9  0.0   0:10.67 kworker/6:0                                                                                                                                   </span><br><span class="line"> 58741 root      20   0       0      0      0 D  12.3  0.0   0:10.50 kworker/15:1                                                                                                                                  </span><br><span class="line">104600 root      20   0       0      0      0 D  12.3  0.0   0:05.55 kworker/8:2                                                                                                                                   </span><br><span class="line"> 15166 root      20   0       0      0      0 D  10.9  0.0   0:04.44 kworker/16:1                                                                                                                                  </span><br><span class="line"> 89483 root      20   0       0      0      0 D  10.9  0.0   0:04.73 kworker/11:0                                                                                                                                  </span><br><span class="line"> 30487 root      20   0 3905496 152268  36216 S   9.3  0.1   3060:33 dockerd                                                                                                                                       </span><br><span class="line"> 41220 work      20   0  687540 300368  16012 S   4.0  0.2 235:53.07 lottery-service                                                                                                                               </span><br><span class="line">125923 root      20   0 4892136 181572  58924 S   3.6  0.1  21469:57 kubelet                                                                                                                                       </span><br><span class="line"> ...</span><br></pre></td></tr></tbody></table></figure><p></p><p>发现cadvisor占用较高的cpu，联系以前遇到的问题，cadvisor也是采集memory时变慢,测试居然需要2秒多！<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ time cat /sys/fs/cgroup/memory/memory.stat</span><br><span class="line">cache 25691987968</span><br><span class="line">rss 3426922496</span><br><span class="line">rss_huge 2759852032</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">real0m2.485s</span><br><span class="line">user0m0.000s</span><br><span class="line">sys0m2.484s</span><br></pre></td></tr></tbody></table></figure><p></p><p>主要原因是产生了某些僵尸cgroup(比如反复启动，进程不存在了，但cgroup还没来得及回收，cgroup会反复计算这些cgroup的内存会占用)，导致cpu使用增加<a href="https://github.com/google/cadvisor/issues/1774#issuecomment-406314361" target="_blank" rel="noopener">相关issue</a> 以及<a href="https://lkml.org/lkml/2018/7/3/101" target="_blank" rel="noopener">thread</a></p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>根本原因还需要进一步分析，临时解决办法，通过手动释放内存</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 2 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></tbody></table></figure><p>如果没效果可尝试<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 3 &gt; /proc/sys/vm/drop_caches</span><br></pre></td></tr></tbody></table></figure><p></p><p>释放后，果然系统cpu逐渐恢复正常了，从falcon查看cpu确实下降了<br><img src="/img/blogImg/lxcfs-cpu.png" alt="lxcf-cpu"></p><h2 id="跟进"><a href="#跟进" class="headerlink" title="跟进"></a>跟进</h2><p>经排查，我们使用的内核较旧为（4.9.2）;僵尸cgroup过多, 导致遍历cgroup读取per_cpu变量时可能引起锁的争用。</p><p>僵尸cgroup：没有进程运行，并已经被删除的cgroup，但是所占用的内存并没有被完全回收(inode，dentry等缓存资源)，在读取memory.stat仍会计算这部分cgroup的缓存空间。</p><p>目前该问题在新版的内核（如5.4）中得到修复，新内核引用新的数据结构解决该问题：每次分配内存时，会即时更新cgroup的内存使用情况存储到专用的统计变量，因此读取某个cgroup的mem stat不会涉及到per_cpu变量，可以立即返回。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s中fd与thread限制(二)</title>
      <link href="/k8s-limit-fd-and-thread2/"/>
      <url>/k8s-limit-fd-and-thread2/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在上线fd隔离后，多个用户反馈部署有问题，日志显示 <code>su could not open session</code>，dolphin（主进程） 启动用户程序时如果用户部署账号为work，会通过su切换到work下启动用户程序，报错正是这时产生。</p><h2 id="探究"><a href="#探究" class="headerlink" title="探究"></a>探究</h2><p>通过复现问题，确实存在su切换失败，通过<code>strace su work</code>显示：</p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">sh-4.1<span class="comment"># strace -o strace.log su work</span></span><br><span class="line">could not open session</span><br><span class="line">sh-4.1<span class="comment"># vim strace.log</span></span><br><span class="line">execve(<span class="string">"/bin/su"</span>, [<span class="string">"su"</span>, <span class="string">"work"</span>], [/* 18 vars */]) = 0</span><br><span class="line">brk(0)</span><br><span class="line">/su</span><br><span class="line">...</span><br><span class="line"><span class="built_in">stat</span>(<span class="string">"/etc/pam.d"</span>, {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0</span><br><span class="line">open(<span class="string">"/etc/pam.d/su"</span>, O_RDONLY)         = 3</span><br><span class="line">...</span><br><span class="line">open(<span class="string">"/etc/pam.d/system-auth"</span>, O_RDONLY) = 4</span><br><span class="line">...</span><br><span class="line">getrlimit(RLIMIT_CPU, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0 <span class="comment"># 通过getrlimit获取当前ulimit设置</span></span><br><span class="line">getrlimit(RLIMIT_FSIZE, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_DATA, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_STACK, {rlim_cur=8192*1024, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_CORE, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_RSS, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_NPROC, {rlim_cur=2048*1024, rlim_max=2048*1024}) = 0</span><br><span class="line">getrlimit(RLIMIT_NOFILE, {rlim_cur=10*1024, rlim_max=20*1024}) = 0</span><br><span class="line">getrlimit(RLIMIT_MEMLOCK, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_AS, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_LOCKS, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0</span><br><span class="line">getrlimit(RLIMIT_SIGPENDING, {rlim_cur=256736, rlim_max=256736}) = 0</span><br><span class="line">getrlimit(RLIMIT_MSGQUEUE, {rlim_cur=800*1024, rlim_max=800*1024}) = 0</span><br><span class="line">getrlimit(RLIMIT_NICE, {rlim_cur=0, rlim_max=0}) = 0</span><br><span class="line">getrlimit(RLIMIT_RTPRIO, {rlim_cur=0, rlim_max=0}) = 0</span><br><span class="line">getpriority(PRIO_PROCESS, 0)            = 20</span><br><span class="line">open(<span class="string">"/etc/security/limits.conf"</span>, O_RDONLY) = 3 <span class="comment"># 读取limits.conf配置</span></span><br><span class="line">fstat(3, {st_mode=S_IFREG|0644, st_size=1973, ...}) = 0</span><br><span class="line">mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f2b03deb000</span><br><span class="line"><span class="built_in">read</span>(3, <span class="string">"# /etc/security/limits.conf\n#\n#E"</span>..., 4096) = 1973</span><br><span class="line"><span class="built_in">read</span>(3, <span class="string">""</span>, 4096)                       = 0</span><br><span class="line">close(3)                                = 0</span><br><span class="line">munmap(0x7f2b03deb000, 4096)            = 0</span><br><span class="line">open(<span class="string">"/etc/security/limits.d"</span>, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3</span><br><span class="line">fcntl(3, F_GETFD)                       = 0x1 (flags FD_CLOEXEC)</span><br><span class="line">getdents(3, /* 2 entries */, 32768)     = 48</span><br><span class="line">open(<span class="string">"/usr/lib64/gconv/gconv-modules.cache"</span>, O_RDONLY) = 4</span><br><span class="line">fstat(4, {st_mode=S_IFREG|0644, st_size=26060, ...}) = 0</span><br><span class="line">mmap(NULL, 26060, PROT_READ, MAP_SHARED, 4, 0) = 0x7f2b03de5000</span><br><span class="line">close(4)                                = 0</span><br><span class="line">futex(0x7f2b037b6f60, FUTEX_WAKE_PRIVATE, 2147483647) = 0 </span><br><span class="line">getdents(3, /* 0 entries */, 32768)     = 0</span><br><span class="line">close(3)                                = 0</span><br><span class="line">setrlimit(RLIMIT_CORE, {rlim_cur=RLIM_INFINITY, rlim_max=RLIM_INFINITY}) = 0  </span><br><span class="line">setrlimit(RLIMIT_NOFILE, {rlim_cur=150240, rlim_max=300240}) = -1 EPERM (Operation not permitted) <span class="comment"># 设置nofile失败，返回权限不足，经查证setrlimit需要CAP_SYS_RESOURCE</span></span><br><span class="line">...</span><br></pre></td></tr></tbody></table></figure><p>整理下执行su的流程</p><ol><li><p>进行pam认证，su配置文件在/etc/pam.d/su，更多pam信息可参考pam.d</p></li><li><p>根据文件内容逐行认证，下面是线上centos6基础镜像的配置</p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#%PAM-1.0</span></span><br><span class="line">auth sufficient pam_rootok.so</span><br><span class="line"><span class="comment"># Uncomment the following line to implicitly trust users in the "wheel" group.</span></span><br><span class="line"><span class="comment">#auth sufficient pam_wheel.so trust use_uid</span></span><br><span class="line"><span class="comment"># Uncomment the following line to require a user to be in the "wheel" group.</span></span><br><span class="line"><span class="comment">#auth required pam_wheel.so use_uid</span></span><br><span class="line">auth include system-auth</span><br><span class="line">account sufficient pam_succeed_if.so uid = 0 use_uid quiet</span><br><span class="line">account include system-auth</span><br><span class="line">password include system-auth</span><br><span class="line">session include system-auth <span class="comment">#认证失败出现在这步</span></span><br><span class="line">session optional pam_xauth.so</span><br></pre></td></tr></tbody></table></figure></li><li><p>system-auth 真实内容存放在 system-auth-ac，内容为</p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># User changes will be destroyed the next time authconfig is run.</span></span><br><span class="line">auth required pam_env.so</span><br><span class="line">auth sufficient pam_fprintd.so</span><br><span class="line">auth sufficient pam_unix.so nullok try_first_pass</span><br><span class="line">auth requisite pam_succeed_if.so uid &gt;= 500 quiet</span><br><span class="line">auth required pam_deny.so</span><br><span class="line"></span><br><span class="line">account required pam_unix.so</span><br><span class="line">account sufficient pam_localuser.so</span><br><span class="line">account sufficient pam_succeed_if.so uid &lt; 500 quiet</span><br><span class="line">account required pam_permit.so</span><br><span class="line"></span><br><span class="line">password requisite pam_cracklib.so try_first_pass retry=3 <span class="built_in">type</span>=</span><br><span class="line">password sufficient pam_unix.so md5 shadow nullok try_first_pass use_authtok</span><br><span class="line">password required pam_deny.so</span><br><span class="line"></span><br><span class="line">session optional pam_keyinit.so revoke</span><br><span class="line">session required pam_limits.so <span class="comment"># limit 认证</span></span><br><span class="line">session [success=1 default=ignore] pam_succeed_if.so service <span class="keyword">in</span> crond quiet use_uid</span><br><span class="line">session required pam_unix.so</span><br></pre></td></tr></tbody></table></figure></li><li><p>system-auth调用pam_limit.so认证，并且类型为required，及若认证失败则继续执行最后返回失败信息</p></li><li><p>pam_limit会调用getrlimit获取当前ulimit信息，通过读取/etc/security/limits.conf，调用setrlimit设置ulimit，并且setrlimit有一定限制</p></li></ol><ul><li>任何进程可以将软限制改为小于或等于硬限制</li><li>任何进程都可以将硬限制降低，但普通用户降低了就无法提高，该值必须等于或大于软限制</li><li>只有超级用户（拥有CAP_SYS_RESOURCE权限）可以提高硬限制</li></ul><p>由于显示docker设置nofile最大hard限制为20480， 而/etc/security/limits.cof文件中为300240，在docker中root用户缺少<code>CAP_SYS_RESOURCE</code>，所以出现上述问题。</p><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>由于limits.conf，以及pam.so等配置文件是镜像中的配置，解决冲突必须修改对应配置,有两种方式</p><ul><li>通过dolphin将对应limits.conf以及limits.d目录下有关nofile的配置删除</li><li>基础镜像修改limits.conf配置</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s中fd与thread限制(一)</title>
      <link href="/k8s-limit-fd-and-thread1/"/>
      <url>/k8s-limit-fd-and-thread1/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>linux中为了防止进程恶意使用资源，系统使用ulimit来限制进程的资源使用情况（包括文件描述符，线程数，内存大小等）。同样地在容器化场景中，需要限制其系统资源的使用量。</p><h2 id="限制方法"><a href="#限制方法" class="headerlink" title="限制方法"></a>限制方法</h2><ul><li><strong>ulimit</strong>: docker 默认支持ulimit设置，可以在dockerd中配置 default-ulimits 可为宿主机所有容器配置默认的ulimit，docker启动时可添加 –ulimit 为每个容器配置ulimit会覆盖默认的设置；目前k8s暂不支持ulimit</li><li><strong>cgroup</strong>: docker 默认支持cgroup中内存、cpu、pid等的限制，对于线程限制可通过 –pids-limit 可限制每个容器的pid总数，dockerd暂无默认的pid limit设置；k8s 限制线程数，可通过在kubelet中开启SupportPodPidsLimit特性，设置pod级别pid limit</li><li><strong>/etc/securiy/limits.conf,systcl.confg</strong>: 通过ulimit命令设置只对当前登录用户有效，永久设置可通过limits.conf配置文件实现，以及系统级别限制可通过systcl.confg配置文件</li></ul><h2 id="实验对比"><a href="#实验对比" class="headerlink" title="实验对比"></a>实验对比</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p><strong>本地环境</strong>：<br>os: Ubuntu 16.04.6 LTS 4.4.0-154-generic<br>docker: 18.09.7<br>base-image: alpine:v3.9</p><p><strong>k8s环境</strong>：<br>kubelet: v1.10.11.1<br>docker: 18.09.6</p><h3 id="ulimit"><a href="#ulimit" class="headerlink" title="ulimit"></a>ulimit</h3><p>用户级别资源限制，分为soft限制与hard限制</p><ul><li>soft ： 用户可修改，但不能超过硬限制</li><li>hard：只有root用户可修改</li></ul><p><strong>修改方式</strong>： ulimit命令，临时修改；/etc/security/limits.conf，永久修改</p><p><strong>工作原理</strong>： 根据 PAM （ Pluggable Authentication Modules 简称 PAM）机制，应用程序启动时，按 /etc/pam.d 配置加载 pam_xxxx.so 模块。 /etc/pam.d 下包含了 login 、sshd 、su 、sudo 等程序的 PAM 配置文件， 因此用户重新登录时，将调用 pam_limits.so 加载 limits.conf 配置文件</p><h4 id="文件描述符限制"><a href="#文件描述符限制" class="headerlink" title="文件描述符限制"></a>文件描述符限制</h4><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">RLIMIT_NOFILE</span><br><span class="line">              This specifies a value one greater than the maximum file</span><br><span class="line">              descriptor number that can be opened by this process.</span><br><span class="line">              Attempts (open(2), pipe(2), dup(2), etc.)  to exceed this</span><br><span class="line">              limit yield the error EMFILE.  (Historically, this limit was</span><br><span class="line">              named RLIMIT_OFILE on BSD.)</span><br><span class="line"></span><br><span class="line">              Since Linux 4.5, this limit also defines the maximum number of</span><br><span class="line">              file descriptors that an unprivileged process (one without the</span><br><span class="line">              CAP_SYS_RESOURCE capability) may have "in flight" to other</span><br><span class="line">              processes, by being passed across UNIX domain sockets.  This</span><br><span class="line">              limit applies to the sendmsg(2) system call.  For further</span><br><span class="line">              details, see unix(7).</span><br></pre></td></tr></tbody></table></figure><p>根据定义，nofile 限制进程所能最多打开的文件数量，作用范围进程。</p><ol><li><p>设置 ulimit nofile限制soft 100/hard 200，默认启动为root用户</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --ulimit nofile=100:200  cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br></pre></td></tr></tbody></table></figure></li><li><p>进入容器查看， fd soft限制为100个</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # ulimit -a</span><br><span class="line">-f: file size (blocks)             unlimited</span><br><span class="line">-t: cpu time (seconds)             unlimited</span><br><span class="line">-d: data seg size (kb)             unlimited</span><br><span class="line">-s: stack size (kb)                8192</span><br><span class="line">-c: core file size (blocks)        unlimited</span><br><span class="line">-m: resident set size (kb)         unlimited</span><br><span class="line">-l: locked memory (kb)             64</span><br><span class="line">-p: processes                      unlimited</span><br><span class="line">-n: file descriptors               100</span><br><span class="line">-v: address space (kb)             unlimited</span><br><span class="line">-w: locks                          unlimited</span><br><span class="line">-e: scheduling priority            0</span><br><span class="line">-r: real-time priority             0</span><br></pre></td></tr></tbody></table></figure></li><li><p>使用ab测试，并发90个http请求，创建90个socket，正常运行</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># ab -n 1000000 -c 90 http://61.135.169.125:80/ &amp;</span></span><br><span class="line">/ <span class="comment"># lsof | wc -l </span></span><br><span class="line">108</span><br><span class="line">/ <span class="comment"># lsof | grep -c ab</span></span><br><span class="line">94</span><br></pre></td></tr></tbody></table></figure></li><li><p>并发100个http请求，受到ulimit限制</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment">#  ab -n 1000000 -c 100 http://61.135.169.125:80/</span></span><br><span class="line">This is ApacheBench, Version 2.3 &lt;<span class="variable">$Revision</span>: 1843412 $&gt;</span><br><span class="line">Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/</span><br><span class="line">Licensed to The Apache Software Foundation, http://www.apache.org/</span><br><span class="line"></span><br><span class="line">Benchmarking 61.135.169.125 (be patient)</span><br><span class="line">socket: No file descriptors available (24)</span><br></pre></td></tr></tbody></table></figure></li></ol><h4 id="线程限制"><a href="#线程限制" class="headerlink" title="线程限制"></a>线程限制</h4><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">RLIMIT_NPROC</span><br><span class="line">              This is a limit on the number of extant process (or, more pre‐</span><br><span class="line">              cisely on Linux, threads) for the real user ID of the calling</span><br><span class="line">              process.  So long as the current number of processes belonging</span><br><span class="line">              to this process's real user ID is greater than or equal to</span><br><span class="line">              this limit, fork(2) fails with the error EAGAIN.</span><br><span class="line"></span><br><span class="line">              The RLIMIT_NPROC limit is not enforced for processes that have</span><br><span class="line">              either the CAP_SYS_ADMIN or the CAP_SYS_RESOURCE capability.</span><br></pre></td></tr></tbody></table></figure><p>由定义可知，nproc进程限制的范围是对于每个uid，并且对于root用户无效。</p><h5 id="容器uid"><a href="#容器uid" class="headerlink" title="容器uid"></a>容器uid</h5><p>同一主机上运行的所有容器共享同一个内核(主机的内核)，docker通过namspace对pid/utc/network等进行了隔离，虽然docker中已经实现了user namespace，但由于各种原因，默认没有开启，见<a href="https://docs.docker.com/engine/security/userns-remap/" target="_blank" rel="noopener">docker user namespace</a></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d  cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br></pre></td></tr></tbody></table></figure><p>宿主机中查看top进程，显示root用户</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ps -ef |grep top</span><br><span class="line">root      4096  4080  0 15:01 ?        00:00:01 top</span><br></pre></td></tr></tbody></table></figure><p>容器中查看id，uid为0对应宿主机的root用户,虽然同为root用户，但Linux Capabilities不同，实际权限与宿主机root要少很多</p><p>在容器中切换用户到operator(uid为11)，执行sleep命令，主机中查看对应进程用户为app，对应uid同样为11<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/ # id</span><br><span class="line">uid=0(root) gid=0(root) groups=0(root),1(bin),2(daemon),3(sys),4(adm),6(disk),10(wheel),11(floppy),20(dialout),26(tape),27(video)</span><br><span class="line">/ # su operator</span><br><span class="line">/ $ id</span><br><span class="line">uid=11(operator) gid=0(root) groups=0(root)</span><br><span class="line">/ $ sleep 100</span><br><span class="line">$ ps -ef |grep 'sleep 100'</span><br><span class="line">app      19302 19297  0 16:39 pts/0    00:00:00 sleep 100</span><br><span class="line">$ cat /etc/passwd | grep app</span><br><span class="line">app:x:11:0::/home/app:</span><br></pre></td></tr></tbody></table></figure><p></p><h5 id="验证不同用户下ulimit的限制"><a href="#验证不同用户下ulimit的限制" class="headerlink" title="验证不同用户下ulimit的限制"></a>验证不同用户下ulimit的限制</h5><p>设置 ulimit nproc限制soft 10/hard 20，默认启动为root用户<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --ulimit nproc=10:20  cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br></pre></td></tr></tbody></table></figure><p></p><p>进入容器查看， fd soft限制为100个<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # ulimit -a</span><br><span class="line">-f: file size (blocks)             unlimited</span><br><span class="line">-t: cpu time (seconds)             unlimited</span><br><span class="line">-d: data seg size (kb)             unlimited</span><br><span class="line">-s: stack size (kb)                8192</span><br><span class="line">-c: core file size (blocks)        unlimited</span><br><span class="line">-m: resident set size (kb)         unlimited</span><br><span class="line">-l: locked memory (kb)             64</span><br><span class="line">-p: processes                      10</span><br><span class="line">-n: file descriptors               1048576</span><br><span class="line">-v: address space (kb)             unlimited</span><br><span class="line">-w: locks                          unlimited</span><br><span class="line">-e: scheduling priority            0</span><br><span class="line">-r: real-time priority             0</span><br></pre></td></tr></tbody></table></figure><p></p><p>启动30个进程<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/ # for i in `seq 30`;do sleep 100 &amp;; done</span><br><span class="line">/ # ps | wc -l </span><br><span class="line">36</span><br></pre></td></tr></tbody></table></figure><p></p><p>切换到operator用户<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/ # su operator</span><br><span class="line"># 启动多个进程，到第11个进程无法进行fork</span><br><span class="line">/ $ for i in `seq 8`; do</span><br><span class="line">&gt; sleep 100 &amp;</span><br><span class="line">&gt; done</span><br><span class="line">/ $ sleep 100 &amp;</span><br><span class="line">/ $ sleep 100 &amp;</span><br><span class="line">sh: can't fork: Resource temporarily unavailable</span><br></pre></td></tr></tbody></table></figure><p></p><p>root下查看<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # ps -ef | grep operator</span><br><span class="line">   79 operator  0:00 sh</span><br><span class="line">   99 operator  0:00 sleep 100</span><br><span class="line">  100 operator  0:00 sleep 100</span><br><span class="line">  101 operator  0:00 sleep 100</span><br><span class="line">  102 operator  0:00 sleep 100</span><br><span class="line">  103 operator  0:00 sleep 100</span><br><span class="line">  104 operator  0:00 sleep 100</span><br><span class="line">  105 operator  0:00 sleep 100</span><br><span class="line">  106 operator  0:00 sleep 100</span><br><span class="line">  107 operator  0:00 sleep 100</span><br><span class="line">  109 root      0:00 grep operator</span><br><span class="line">/ # ps -ef | grep operator| wc -l</span><br><span class="line">10</span><br></pre></td></tr></tbody></table></figure><p></p><h5 id="验证ulimit在不同容器相同uid下的限制"><a href="#验证ulimit在不同容器相同uid下的限制" class="headerlink" title="验证ulimit在不同容器相同uid下的限制"></a>验证ulimit在不同容器相同uid下的限制</h5><p>设置 ulimit nproc限制soft 3/hard 3，默认启动为operator用户,起4个容器，第四个启动失败<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --ulimit nproc=3:3 --name nproc1 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br><span class="line">eeb1551bf757ad4f112c61cc48d7cbe959185f65109e4b44f28085f246043e65</span><br><span class="line">$ docker run -d --ulimit nproc=3:3 --name nproc2 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br><span class="line">42ff29844565a9cb3af2c8dd560308b1f31306041d3dbd929011d65f1848a262</span><br><span class="line">$ docker run -d --ulimit nproc=3:3 --name nproc3 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br><span class="line">b7c9b469e73f969d922841dd77265467959eda28ed06301af8bf83bcf18e8c23</span><br><span class="line">$ docker run -d --ulimit nproc=3:3 --name nproc4 -u operator  cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br><span class="line">b49d8bb58757c88f69903059af2ee7e2a6cc2fa5774bc531941194c52edfd763</span><br><span class="line">$</span><br><span class="line">$ docker ps -a |grep nproc</span><br><span class="line">b49d8bb58757        cr.d.xiaomi.net/containercloud/alpine:webtool      "top"                    16 seconds ago      Exited (1) 15 seconds ago                               nproc4</span><br><span class="line">b7c9b469e73f        cr.d.xiaomi.net/containercloud/alpine:webtool      "top"                    23 seconds ago      Up 22 seconds                                           nproc3</span><br><span class="line">42ff29844565        cr.d.xiaomi.net/containercloud/alpine:webtool      "top"                    31 seconds ago      Up 29 seconds                                           nproc2</span><br><span class="line">eeb1551bf757        cr.d.xiaomi.net/containercloud/alpine:webtool      "top"                    38 seconds ago      Up 36 seconds                                           nproc1</span><br></pre></td></tr></tbody></table></figure><p></p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><ul><li>ulimit限制fd总数，限制级别进程，可对所有用户生效</li><li>ulimit限制线程总数，限制级别用户（uid)，限制同一个uid下所有线程/进程数，对于root账号无效</li><li>对于目前线上情况，有较小的概率因ulimit限制导致fork失败，如同一个宿主机中有多个work容器且基础镜像相同（即uid相同），若一个容器线程泄露，由于ulimit限制会影响其他容器正常运行</li></ul><h3 id="cgroup"><a href="#cgroup" class="headerlink" title="cgroup"></a>cgroup</h3><p>cgroup中对pid进行了隔离，通过更改docker/kubelet配置，可以限制pid总数，从而达到限制线程总数的目的。线程数限制与系统中多处配置有关，取最小值，参考<a href="https://stackoverflow.com/questions/34452302/how-to-increase-maximum-number-of-jvm-threads-linux-64bit" target="_blank" rel="noopener">stackoverflow上线程数的设置</a></p><ul><li>docker，容器启动时设置 –pids-limit 参数，限制容器级别pid总数</li><li>kubelet，开启SupportPodPidsLimit特性，设置–pod-max-pids参数，限制node每个pod的pid总数</li></ul><p>以kubelet为例，开启SupportPodPidsLimit，<code>--feature-gates=SupportPodPidsLimit=true</code></p><ol><li><p>配置kubelet，每个pod允许最大pid数目为150</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]<span class="comment"># ps -ef |grep kubelet</span></span><br><span class="line">root     18735     1 14 11:19 ?        00:53:28 ./kubelet --v=1 --address=0.0.0.0 --feature-gates=SupportPodPidsLimit=<span class="literal">true</span> --pod-max-pids=150 --allow-privileged=<span class="literal">true</span> --pod-infra-container-image=cr.d.xiaomi.net/kubernetes/pause-amd64:3.1 --root-dir=/home/kubelet --node-status-update-frequency=5s --kubeconfig=/home/xbox/kubelet/conf/kubelet-kubeconfig --fail-swap-on=<span class="literal">false</span> --max-pods=254 --runtime-cgroups=/systemd/system.slice/frigga.service --kubelet-cgroups=/systemd/system.slice/frigga.service --make-iptables-util-chains=<span class="literal">false</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>在pod中起测试线程，root下起100个线程</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/ # for i in `seq 100`; do</span><br><span class="line">&gt; sleep 1000 &amp;</span><br><span class="line">&gt; done</span><br><span class="line">/ # ps | wc -l</span><br><span class="line">106</span><br></pre></td></tr></tbody></table></figure></li><li><p>operator 下，创建线程受到限制，系统最多只能创建150个</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/ # su operator</span><br><span class="line">/ $ </span><br><span class="line">/ $ for i in `seq 100`; do</span><br><span class="line">&gt; sleep 1000 &amp;</span><br><span class="line">&gt; done</span><br><span class="line">sh: can't fork: Resource temporarily unavailable</span><br><span class="line">/ $ ps | wc -l</span><br><span class="line">150</span><br></pre></td></tr></tbody></table></figure></li><li><p>在cgroup中查看，pids达到最大限制</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 ~]# cat /sys/fs/cgroup/pids/kubepods/besteffort/pod8b61d4de-a7ad-11e9-b5b9-246e96ad0900/pids.current </span><br><span class="line">150</span><br><span class="line">[root@node01 ~]# cat /sys/fs/cgroup/pids/kubepods/besteffort/pod8b61d4de-a7ad-11e9-b5b9-246e96ad0900/pids.max </span><br><span class="line">150</span><br></pre></td></tr></tbody></table></figure></li><li><p>总结<br>cgroup对于pid的限制能够达到限制线程数目的，目前docker只支持对每个容器的限制，不支持全局配置；kubelet只支持对于node所有pod的全局配置，不支持具体每个pod的配置</p></li></ol><h3 id="limits-conf-sysctl-conf"><a href="#limits-conf-sysctl-conf" class="headerlink" title="limits.conf/sysctl.conf"></a>limits.conf/sysctl.conf</h3><p>limits.conf是ulimit的具体配置，目录项/etc/security/limit.d/中的配置会覆盖limits.conf。</p><p>sysctl.conf为机器级别的资源限制，root用户可修改，目录项/etc/security/sysctl.d/中的配置会覆盖sysctl.conf，在/etc/sysctl.conf中添加对应配置（fd: fs.file-max = {}; pid: kernel.pid_max = {}）</p><ol><li><p>测试容器中修改sysctl.conf文件</p> <figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br><span class="line">cb1250c8fd217258da51c6818fa2ce2e2f6e35bf1d52648f1f432e6ce579cf0d</span><br><span class="line">$ docker exec -it cb1250c sh</span><br><span class="line"></span><br><span class="line">/ # ulimit -a</span><br><span class="line">-f: file size (blocks)             unlimited</span><br><span class="line">-t: cpu time (seconds)             unlimited</span><br><span class="line">-d: data seg size (kb)             unlimited</span><br><span class="line">-s: stack size (kb)                8192</span><br><span class="line">-c: core file size (blocks)        unlimited</span><br><span class="line">-m: resident set size (kb)         unlimited</span><br><span class="line">-l: locked memory (kb)             64</span><br><span class="line">-p: processes                      unlimited</span><br><span class="line">-n: file descriptors               100</span><br><span class="line">-v: address space (kb)             unlimited</span><br><span class="line">-w: locks                          unlimited</span><br><span class="line">-e: scheduling priority            0</span><br><span class="line">-r: real-time priority             0</span><br><span class="line">/ # </span><br><span class="line">/ # echo 10 &gt; /proc/sys/kernel/pid_max</span><br><span class="line">sh: can't create /proc/sys/kernel/pid_max: Read-only file system</span><br><span class="line">/ # echo 10 &gt; /proc/sys/kernel/pid_max</span><br><span class="line">sh: can't create /proc/sys/kernel/pid_max: Read-only file system</span><br><span class="line">/ # echo "fs.file-max=5" &gt;&gt; /etc/sysctl.conf</span><br><span class="line">/ # sysctl -p</span><br><span class="line">sysctl: error setting key 'fs.file-max': Read-only file system</span><br></pre></td></tr></tbody></table></figure></li><li><p>以priviledged模式测试，谨慎测试</p> <figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/kernel/pid_max</span><br><span class="line">32768</span><br><span class="line">$ docker run -d -- --ulimit nofile=100:200 cr.d.xiaomi.net/containercloud/alpine:webtool top</span><br><span class="line">$ docker exec -it pedantic_vaughan sh</span><br><span class="line">/ # cat /proc/sys/kernel/pid_max</span><br><span class="line">32768</span><br><span class="line">/ # echo 50000 &gt; /proc/sys/kernel/pid_max</span><br><span class="line">/ # cat /proc/sys/kernel/pid_max</span><br><span class="line">50000</span><br><span class="line">/ # exit</span><br><span class="line">$ cat /proc/sys/kernel/pid_max</span><br><span class="line">50000 # 宿主机的文件也变成50000</span><br></pre></td></tr></tbody></table></figure></li><li><p>总结<br>由于docker隔离的不彻底，在docker中修改sysctl会覆盖主机中的配置，不能用来实现容器级别资源限制<br>limits.conf可以在容器中设置，效果同ulimit</p></li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><img src="/img/blogImg/pod-fd-limit.png" alt="pod-fd-limit"></p><p>推荐方案如下：</p><ul><li>fd限制： 修改dockerd配置<code>default-ulimits</code>，限制进程级别fd</li><li>thread限制：修改kubelet配置<code>--feature-gates=SupportPodPidsLimit=true --pod-max-pids={}</code>，cgroup级别限制pid，从而限制线程数</li><li>其他注意事项，调整节点pid.max参数；放开或者调大镜像中ulimit对非root账户nproc限制</li></ul><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ul><li><a href="https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit</a></li><li><a href="http://man7.org/linux/man-pages/man2/getrlimit.2.html" target="_blank" rel="noopener">http://man7.org/linux/man-pages/man2/getrlimit.2.html</a></li><li><a href="https://feichashao.com/ulimit_demo/" target="_blank" rel="noopener">https://feichashao.com/ulimit_demo/</a></li><li><a href="https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf" target="_blank" rel="noopener">https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf</a></li><li><a href="https://docs.docker.com/engine/security/userns-remap/" target="_blank" rel="noopener">https://docs.docker.com/engine/security/userns-remap/</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>容器内存分析</title>
      <link href="/container-memory/"/>
      <url>/container-memory/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在容器化环境中，平台需要提供准确的业务监控指标，已方便业务查看。那么如何准确计算容器或Pod的内存使用率，k8s/docker又是如何计算，本文通过实验与源码阅读相结合来分析容器的内存实际使用量。</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><p>不管docker还是k8s(通过cadvisor)最终都通过cgroup的memory group来得到内存的原始文件，memory相关的主要文件如下:<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cgroup.event_control       #用于eventfd的接口</span><br><span class="line">memory.usage_in_bytes      #显示当前已用的内存</span><br><span class="line">memory.limit_in_bytes      #设置/显示当前限制的内存额度</span><br><span class="line">memory.failcnt             #显示内存使用量达到限制值的次数</span><br><span class="line">memory.max_usage_in_bytes  #历史内存最大使用量</span><br><span class="line">memory.soft_limit_in_bytes #设置/显示当前限制的内存软额度</span><br><span class="line">memory.stat                #显示当前cgroup的内存使用情况</span><br><span class="line">memory.use_hierarchy       #设置/显示是否将子cgroup的内存使用情况统计到当前cgroup里面</span><br><span class="line">memory.force_empty         #触发系统立即尽可能的回收当前cgroup中可以回收的内存</span><br><span class="line">memory.pressure_level      #设置内存压力的通知事件，配合cgroup.event_control一起使用</span><br><span class="line">memory.swappiness          #设置和显示当前的swappiness</span><br><span class="line">memory.move_charge_at_immigrate #设置当进程移动到其他cgroup中时，它所占用的内存是否也随着移动过去</span><br><span class="line">memory.oom_control         #设置/显示oom controls相关的配置</span><br><span class="line">memory.numa_stat           #显示numa相关的内存</span><br></pre></td></tr></tbody></table></figure><p></p><p>更多信息可参考<a href="https://qingwave.github.io/2018/11/15/Pod-memory-usage-in-k8s/#Cadvisor%E4%B8%AD%E6%9C%89%E5%85%B3pod%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E7%8E%87%E7%9A%84%E6%8C%87%E6%A0%87">Pod memory usage in k8s</a></p><h2 id="查看源码"><a href="#查看源码" class="headerlink" title="查看源码"></a>查看源码</h2><h3 id="docker-stat"><a href="#docker-stat" class="headerlink" title="docker stat"></a>docker stat</h3><p>docker stat的源码在<a href="https://github.com/docker/cli/blob/37f9a88c696ae81be14c1697bd083d6421b4933c/cli/command/container/stats_helpers.go#L233" target="_blank" rel="noopener">stats_helpers.go</a>,如下：<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">calculateMemUsageUnixNoCache</span><span class="params">(mem types.MemoryStats)</span> <span class="title">float64</span></span> {</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">float64</span>(mem.Usage - mem.Stats[<span class="string">"cache"</span>])</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>内存使用量为<code>memory.usage=memory.usage_in_bytes-cache</code></p><h3 id="kubectl-top"><a href="#kubectl-top" class="headerlink" title="kubectl top"></a>kubectl top</h3><p>在k8s中，<code>kubectl top</code>命令通过<code>metric-server/heapster</code>获取cadvisor中<code>working_set</code>的值，来表示Pod实例使用内存大小(不包括pause),metrics-server 中<a href="https://github.com/kubernetes-sigs/metrics-server/blob/d4432d67b2fc435b9c71a89c13659882008a4c54/pkg/sources/summary/summary.go#L206" target="_blank" rel="noopener">pod内存</a>获取如下：<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">decodeMemory</span><span class="params">(target *resource.Quantity, memStats *stats.MemoryStats)</span> <span class="title">error</span></span> {</span><br><span class="line"><span class="keyword">if</span> memStats == <span class="literal">nil</span> || memStats.WorkingSetBytes == <span class="literal">nil</span> {</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"missing memory usage metric"</span>)</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">*target = *uint64Quantity(*memStats.WorkingSetBytes, <span class="number">0</span>)</span><br><span class="line">target.Format = resource.BinarySI</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>cadvisor中<a href="https://github.com/google/cadvisor/blob/0ff17b8d0df3712923c46ca484701b876d02dfee/container/libcontainer/handler.go#L706" target="_blank" rel="noopener">working_set</a>计算如下：<br></p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">setMemoryStats</span><span class="params">(s *cgroups.Stats, ret *info.ContainerStats)</span></span> {</span><br><span class="line">ret.Memory.Usage = s.MemoryStats.Usage.Usage</span><br><span class="line">ret.Memory.MaxUsage = s.MemoryStats.Usage.MaxUsage</span><br><span class="line">ret.Memory.Failcnt = s.MemoryStats.Usage.Failcnt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> s.MemoryStats.UseHierarchy {</span><br><span class="line">ret.Memory.Cache = s.MemoryStats.Stats[<span class="string">"total_cache"</span>]</span><br><span class="line">ret.Memory.RSS = s.MemoryStats.Stats[<span class="string">"total_rss"</span>]</span><br><span class="line">ret.Memory.Swap = s.MemoryStats.Stats[<span class="string">"total_swap"</span>]</span><br><span class="line">ret.Memory.MappedFile = s.MemoryStats.Stats[<span class="string">"total_mapped_file"</span>]</span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">ret.Memory.Cache = s.MemoryStats.Stats[<span class="string">"cache"</span>]</span><br><span class="line">ret.Memory.RSS = s.MemoryStats.Stats[<span class="string">"rss"</span>]</span><br><span class="line">ret.Memory.Swap = s.MemoryStats.Stats[<span class="string">"swap"</span>]</span><br><span class="line">ret.Memory.MappedFile = s.MemoryStats.Stats[<span class="string">"mapped_file"</span>]</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> v, ok := s.MemoryStats.Stats[<span class="string">"pgfault"</span>]; ok {</span><br><span class="line">ret.Memory.ContainerData.Pgfault = v</span><br><span class="line">ret.Memory.HierarchicalData.Pgfault = v</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> v, ok := s.MemoryStats.Stats[<span class="string">"pgmajfault"</span>]; ok {</span><br><span class="line">ret.Memory.ContainerData.Pgmajfault = v</span><br><span class="line">ret.Memory.HierarchicalData.Pgmajfault = v</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">workingSet := ret.Memory.Usage</span><br><span class="line"><span class="keyword">if</span> v, ok := s.MemoryStats.Stats[<span class="string">"total_inactive_file"</span>]; ok {</span><br><span class="line"><span class="keyword">if</span> workingSet &lt; v {</span><br><span class="line">workingSet = <span class="number">0</span></span><br><span class="line">} <span class="keyword">else</span> {</span><br><span class="line">workingSet -= v</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">ret.Memory.WorkingSet = workingSet</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p><code>working_set=memory.usage_in_bytes-total_inactive_file (&gt;=0)</code><br>在kubelet中节点内存不足时同样以<code>working_set</code>判断pod是否OOM的标准</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ol><li><p>创建Pod<br>Pod的资源申请如下：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"1"</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"0"</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="string">"0"</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>查看cgroup内存情况<br>找到容器某个进程，查看memory cgroup</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /proc/16062/cgroup </span></span><br><span class="line">...</span><br><span class="line">8:memory:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod21a55da5_f9f8_11e9_b051_fa163e7e981a.slice/docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope</span><br></pre></td></tr></tbody></table></figure></li></ol><p>进入容器memory cgroup对应的目录<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]# ls</span><br><span class="line">cgroup.clone_children  memory.kmem.failcnt             memory.kmem.tcp.limit_in_bytes      memory.max_usage_in_bytes        memory.move_charge_at_immigrate  memory.stat            tasks</span><br><span class="line">cgroup.event_control   memory.kmem.limit_in_bytes      memory.kmem.tcp.max_usage_in_bytes  memory.memsw.failcnt             memory.numa_stat                 memory.swappiness</span><br><span class="line">cgroup.procs           memory.kmem.max_usage_in_bytes  memory.kmem.tcp.usage_in_bytes      memory.memsw.limit_in_bytes      memory.oom_control               memory.usage_in_bytes</span><br><span class="line">memory.failcnt         memory.kmem.slabinfo            memory.kmem.usage_in_bytes          memory.memsw.max_usage_in_bytes  memory.pressure_level            memory.use_hierarchy</span><br><span class="line">memory.force_empty     memory.kmem.tcp.failcnt         memory.limit_in_bytes               memory.memsw.usage_in_bytes      memory.soft_limit_in_bytes       notify_on_release</span><br></pre></td></tr></tbody></table></figure><p></p><p>查看主要memory文件<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat memory.limit_in_bytes (容器memory limit值，即1Gi)</span></span><br><span class="line">1073741824</span><br><span class="line">[root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># cat memory.kmem.limit_in_bytes (容器内核使用memory limit值)</span></span><br><span class="line">9223372036854771712</span><br><span class="line">[root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># </span></span><br><span class="line">[root@node01 docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># cat memory.soft_limit_in_bytes</span></span><br><span class="line">9223372036854771712</span><br><span class="line">[docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># cat notify_on_release</span></span><br><span class="line">0</span><br><span class="line">[docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># cat memory.oom_control </span></span><br><span class="line">oom_kill_disable 0</span><br><span class="line">under_oom 0</span><br><span class="line">oom_kill 0</span><br><span class="line">[docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># cat memory.usage_in_bytes </span></span><br><span class="line">2265088</span><br><span class="line">[docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># cat memory.kmem.usage_in_bytes </span></span><br><span class="line">901120</span><br><span class="line">[docker-57ba1991ab4ba50a9b2eaf5bf90e2c20073198d767653becf77d55ee25e1a6f9.scope]<span class="comment"># cat memory.stat </span></span><br><span class="line">cache 12288</span><br><span class="line">rss 1351680</span><br><span class="line">rss_huge 0</span><br><span class="line">shmem 4096</span><br><span class="line">mapped_file 4096</span><br><span class="line">dirty 0</span><br><span class="line">writeback 0</span><br><span class="line">swap 0</span><br><span class="line">pgpgin 4544</span><br><span class="line">pgpgout 4211</span><br><span class="line">pgfault 1948</span><br><span class="line">pgmajfault 0</span><br><span class="line">inactive_anon 4096</span><br><span class="line">active_anon 1351680</span><br><span class="line">inactive_file 8192</span><br><span class="line">active_file 0</span><br><span class="line">unevictable 0</span><br><span class="line">hierarchical_memory_limit 1073741824</span><br><span class="line">hierarchical_memsw_limit 1073741824</span><br><span class="line">total_cache 12288</span><br><span class="line">total_rss 1351680</span><br><span class="line">total_rss_huge 0</span><br><span class="line">total_shmem 4096</span><br><span class="line">total_mapped_file 4096</span><br><span class="line">total_dirty 0</span><br><span class="line">total_writeback 0</span><br><span class="line">total_swap 0</span><br><span class="line">total_pgpgin 4544</span><br><span class="line">total_pgpgout 4211</span><br><span class="line">total_pgfault 1948</span><br><span class="line">total_pgmajfault 0</span><br><span class="line">total_inactive_anon 4096</span><br><span class="line">total_active_anon 1351680</span><br><span class="line">total_inactive_file 8192</span><br><span class="line">total_active_file 0</span><br><span class="line">total_unevictable 0</span><br></pre></td></tr></tbody></table></figure><p></p><p>根据memory可得到如下关系：<br><code>memory.usage_in_bytes = memory.kmem.usage_in_bytes + rss + cache</code><br>即2265088=901120+1351680+12288</p><p>那么容器的真实内存即：<br><code>memory.usage=memory.usage_in_bytes-cache</code><br>即<code>rss+kmem_usage</code></p><p>通过<code>docker stat</code>查看，与公式相符合<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        NAME                                                                                     CPU %               MEM USAGE / LIMIT   MEM %               NET I/O             BLOCK I/O           PIDS</span><br><span class="line">57ba1991ab4b        k8s...default_21a55da5-f9f8-11e9-b051-fa163e7e981a_0   0.00%               2.148MiB / 1GiB     0.21%               12MB / 68.8MB       0B / 0B             2</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>实际环境中，docker与k8s两种内存表示方式不同，一般<code>docker stat</code>总体值会小于<code>kubectl top</code></p><ul><li>docker中内存表示为：<br><code>memory.usage = memory.usage_in_bytes - cache</code></li><li>k8s中：<br><code>memory.usage = working_set = memory.usage_in_bytes - total_inactive_file (&gt;=0)</code><br>根据cgroup memory关系有：<br><code>memory.usage_in_bytes = memory.kmem.usage_in_bytes + rss + cache</code></li></ul><p>真实环境中两种表示相差不大，但更推荐使用<code>working_set</code>作为容器内存真实使用量(kubelt判断OOM的依据)，<br>则容器内存使用率可表示为：<br><code>container_memory_working_set_bytes / memory.limit_in_bytes</code></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt" target="_blank" rel="noopener">https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt</a></li><li><a href="https://medium.com/@zhimin.wen/memory-limit-of-pod-and-oom-killer-891ee1f1cad8" target="_blank" rel="noopener">https://medium.com/@zhimin.wen/memory-limit-of-pod-and-oom-killer-891ee1f1cad8</a></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
            <tag> cgroup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s与docker组件堆栈及Debug</title>
      <link href="/k8s-docker-stack/"/>
      <url>/k8s-docker-stack/</url>
      
        <content type="html"><![CDATA[<p>k8s组件日志级别热更新<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整日志级别到3</span></span><br><span class="line">curl -X PUT http://127.0.0.1:8081/debug/flags/v -d <span class="string">"3"</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>controller manager<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://localhost:10252/debug/pprof/profile</span><br><span class="line">wget http://localhost:10252/debug/pprof/heap</span><br><span class="line">curl http://127.0.0.1:10252/debug/pprof/goroutine?debug=1 &gt;&gt; debug1</span><br><span class="line">curl http://127.0.0.1:10252/debug/pprof/goroutine?debug=2 &gt;&gt; debug2</span><br></pre></td></tr></tbody></table></figure><p></p><p>scheduler<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -12 <span class="variable">${SCHED_PID}</span></span><br><span class="line">获取scheduler cache信息，输出到日志</span><br></pre></td></tr></tbody></table></figure><p></p><p>kubelet 堆栈信息<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://localhost:10250/debug/pprof/profile</span><br><span class="line">wget http://localhost:10250/debug/pprof/heap</span><br><span class="line">curl http://127.0.0.1:10250/debug/pprof/goroutine?debug=1 &gt;&gt; debug1</span><br><span class="line">curl http://127.0.0.1:10250/debug/pprof/goroutine?debug=2 &gt;&gt; debug2</span><br></pre></td></tr></tbody></table></figure><p></p><p>docker 堆栈信息<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/profile</span><br><span class="line">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/</span><br><span class="line">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/</span><br><span class="line"> </span><br><span class="line">sudo <span class="built_in">kill</span> -SIGUSR1 $(pidof dockerd)</span><br><span class="line">/var/run/docker/</span><br><span class="line"> </span><br><span class="line">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/profile &gt;&gt;docker.profile</span><br><span class="line">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/goroutine &gt;&gt; docker.goroutine</span><br><span class="line">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/goroutine?debug=2 &gt;&gt;docker.gorouting_debug_2</span><br><span class="line">curl --unix-socket /var/run/docker.sock -X GET http://v1.2/debug/pprof/heap?debug=2 &gt;&gt;docker.heap</span><br></pre></td></tr></tbody></table></figure><p></p><p>docker-registry 堆栈信息<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#先登入机器,然后执行</span></span><br><span class="line">wget localhost:5002/debug/pprof/profile <span class="comment">#这个是cpu占用时间的采样结果，要先等30s</span></span><br><span class="line">wget localhost:5002/debug/pprof/heap <span class="comment">#内存的使用情况</span></span><br><span class="line">wget localhost:5002/debug/pprof/goroutine?debug=2 <span class="comment">#调用栈的全部信息</span></span><br><span class="line">wget localhost:5002/debug/pprof/goroutine</span><br><span class="line">其他可用的profile:</span><br><span class="line">allocs block goroutine cmdline mutex threadcreate trace，替换上面命令pprof/后面的词即可</span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
            <tag> pprof </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>openssl常用命令</title>
      <link href="/openssl-cmd/"/>
      <url>/openssl-cmd/</url>
      
        <content type="html"><![CDATA[<h3 id="输出x509证书信息"><a href="#输出x509证书信息" class="headerlink" title="输出x509证书信息"></a>输出x509证书信息</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl x509 -noout -text  -<span class="keyword">in</span> ca.pem</span><br></pre></td></tr></tbody></table></figure><p>结果如下<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Certificate:</span><br><span class="line">    Data:</span><br><span class="line">        Version: 3 (0x2)</span><br><span class="line">        Serial Number:</span><br><span class="line">            5f:11:aa:b3:70:18:fd:89:b0:25:7a:9e:36:c5:e7:ce:33:5a:cc:b7</span><br><span class="line">    Signature Algorithm: sha256WithRSAEncryption</span><br><span class="line">        Issuer: C=CN, ST=BeiJing, L=BeiJing, O=xx, OU=xx, CN=xx</span><br><span class="line">        Validity</span><br><span class="line">            Not Before: Dec 26 06:17:00 2019 GMT</span><br><span class="line">            Not After : Dec  2 06:17:00 2119 GMT <span class="comment">#过期时间</span></span><br><span class="line">        Subject: C=CN, ST=BeiJing, L=BeiJing, O=xx, OU=xx, CN=xx</span><br><span class="line">        Subject Public Key Info:</span><br><span class="line">        ...</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="验证公钥私钥是否匹配"><a href="#验证公钥私钥是否匹配" class="headerlink" title="验证公钥私钥是否匹配"></a>验证公钥私钥是否匹配</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">diff -eq &lt;(openssl x509 -pubkey -noout -<span class="keyword">in</span> cert.crt) &lt;(openssl rsa -pubout -<span class="keyword">in</span> cert.key)</span><br></pre></td></tr></tbody></table></figure><p>正常会输出<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writing RSA key</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="验证证书CA"><a href="#验证证书CA" class="headerlink" title="验证证书CA"></a>验证证书CA</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openssl verify -CAfile ca.pem client.pem</span><br></pre></td></tr></tbody></table></figure><p>正常输出<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client.pem: OK</span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openssl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prometheus分区实践</title>
      <link href="/prometheus-federation/"/>
      <url>/prometheus-federation/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>单个Prometheus Server可以轻松的处理数以百万的时间序列。但当机器规模过大时，需要对其进行分区，Prometheus也提供了集群联邦的功能，方便对其扩展。</p><p>我们采用Prometheus来监控k8s集群，节点数400，采集的samples是280w，Prometheus官方的显示每秒可抓取10w samples。当集群规模扩大到上千节点时，单个Prometheus不足以处理大量数据，需要对其进行分区。</p><blockquote><p>可以根据<code>scrape_samples_scraped{job=${JOBNAME}}</code>来统计各个job的samples数目<br>可以根据<code>count({__name__=~".*:.*"})</code>来统计metrics总数</p></blockquote><h2 id="集群联邦"><a href="#集群联邦" class="headerlink" title="集群联邦"></a>集群联邦</h2><p>在Promehtues的源码中，<code>federate</code>联邦功能在<code>web</code>中，是一个特殊的查询接口，允许一个prometheus抓取另一个prometheus的metrics</p><p>可以通过全局的prometheus抓取其他slave prometheus从而达到分区的目的<br><img src="/img/blogImg/federate1.png" alt="federate"></p><p>使用federate进行分区通过有两种方式</p><h3 id="功能分区"><a href="#功能分区" class="headerlink" title="功能分区"></a>功能分区</h3><p>每个模块为一个分区，如node-exporter为一个分区，kube-state-metrics为一个分区，再使用全局的Prometheus汇总<br><img src="/img/blogImg/federate2.png" alt="federate by job"></p><p>实现简单，但当单个job采集任务过大（如node-exporter）时，单个Prometheus slave也会成为瓶颈</p><h3 id="水平扩展"><a href="#水平扩展" class="headerlink" title="水平扩展"></a>水平扩展</h3><p>针对功能分区的不足，将同一任务的不同实例的监控数据采集任务划分到不同的Prometheus实例。通过relabel设置，我们可以确保当前Prometheus Server只收集当前采集任务的一部分实例的监控指标。<br><img src="/img/blogImg/federate3.png" alt="federate by modulus"></p><p>下为官方提供的配置</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">global:</span></span><br><span class="line"><span class="attr">  external_labels:</span></span><br><span class="line"><span class="attr">    slave:</span> <span class="number">1</span>  <span class="comment"># This is the 2nd slave. This prevents clashes between slaves.</span></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line"><span class="attr">  - job_name:</span> <span class="string">some_job</span></span><br><span class="line">    <span class="comment"># Add usual service discovery here, such as static_configs</span></span><br><span class="line"><span class="attr">    relabel_configs:</span></span><br><span class="line"><span class="attr">    - source_labels:</span> <span class="string">[__address__]</span></span><br><span class="line"><span class="attr">      modulus:</span>       <span class="number">4</span>    <span class="comment"># 4 slaves</span></span><br><span class="line"><span class="attr">      target_label:</span>  <span class="string">__tmp_hash</span></span><br><span class="line"><span class="attr">      action:</span>        <span class="string">hashmod</span></span><br><span class="line"><span class="attr">    - source_labels:</span> <span class="string">[__tmp_hash]</span></span><br><span class="line"><span class="attr">      regex:</span>         <span class="string">^1$</span>  <span class="comment"># This is the 2nd slave</span></span><br><span class="line"><span class="attr">      action:</span>        <span class="string">keep</span></span><br></pre></td></tr></tbody></table></figure><p>并且通过当前数据中心的一个中心Prometheus Server将监控数据进行聚合到任务级别。</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- scrape_config:</span></span><br><span class="line"><span class="attr">  - job_name:</span> <span class="string">slaves</span></span><br><span class="line"><span class="attr">    honor_labels:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    metrics_path:</span> <span class="string">/federate</span></span><br><span class="line"><span class="attr">    params:</span></span><br><span class="line">      <span class="string">match[]:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">'{__name__=~"^slave:.*"}'</span>   <span class="comment"># Request all slave-level time series</span></span><br><span class="line"><span class="attr">    static_configs:</span></span><br><span class="line"><span class="attr">      - targets:</span></span><br><span class="line"><span class="attr">        - slave0:</span><span class="number">9090</span></span><br><span class="line"><span class="attr">        - slave1:</span><span class="number">9090</span></span><br><span class="line"><span class="attr">        - slave3:</span><span class="number">9090</span></span><br><span class="line"><span class="attr">        - slave4:</span><span class="number">9090</span></span><br></pre></td></tr></tbody></table></figure><p>水平扩展，即通过联邦集群的特性在任务的实例级别对Prometheus采集任务进行划分，以支持规模的扩展。</p><h2 id="我们的方案"><a href="#我们的方案" class="headerlink" title="我们的方案"></a>我们的方案</h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><ul><li>Promehtues以容器化的方式部署在k8s集群中</li><li>收集node-exporter、cadvisor、kubelet、kube-state-metrics、k8s核心组件、自定义metrics</li><li>通过实现opentsdb-adapter，对监控数据做持久化</li><li>通过falcon-adapter,为监控数据提供报警<br><img src="/img/blogImg/Promehtues-arch.png" alt="monitor"></li></ul><h3 id="分区方案"><a href="#分区方案" class="headerlink" title="分区方案"></a>分区方案</h3><ul><li>Prometheus分区包括master Prometheus 与 slave Promehtues</li><li>我们将监控数据分为多个层次: cluster, namespace, deployment/daemonset, pod, node</li><li>由于kubelet, node-exporter, cadvisor等是以node为单位采集的，所以安装node节点来划分不同job</li><li>slave Prometheus 按照node切片采集node，pod级别数据</li><li>kube-state-metrics暂时无法切片，可通过replicaset 设置多个，单独作为一个kube-state Prometheus，供其他slave Prometheus采集</li><li>其他etcd, apiserver等自定义组件可通过master Promehtues直接采集</li></ul><p>整体架构如下<br><img src="/img/blogImg/federate-plan.png" alt="federate-plan"></p><p>master Prometheus配置</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">global:</span></span><br><span class="line"><span class="attr">  scrape_interval:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line"><span class="attr">  scrape_timeout:</span> <span class="number">30</span><span class="string">s</span></span><br><span class="line"><span class="attr">  evaluation_interval:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line"><span class="attr">  external_labels:</span></span><br><span class="line"><span class="attr">    cluster:</span> <span class="string">{{CLUSTER}}</span></span><br><span class="line"><span class="attr">    production_environment:</span> <span class="string">{{ENV}}</span></span><br><span class="line"></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">cluster.yml</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">namespace.yml</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">deployment.yml</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">daemonset.yml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- job_name:</span> <span class="string">federate-slave</span></span><br><span class="line"><span class="attr">  honor_labels:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  metrics_path:</span> <span class="string">'/federate'</span></span><br><span class="line"><span class="attr">  params:</span></span><br><span class="line">    <span class="string">'match[]'</span><span class="string">:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">'{__name__=~"pod:.*|node:.*"}'</span></span><br><span class="line"><span class="attr">  kubernetes_sd_configs:</span></span><br><span class="line"><span class="attr">  - role:</span> <span class="string">pod</span></span><br><span class="line"><span class="attr">    namespaces:</span></span><br><span class="line"><span class="attr">      names:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  relabel_configs:</span></span><br><span class="line"><span class="attr">  - source_labels:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">__meta_kubernetes_pod_label_app</span></span><br><span class="line"><span class="attr">    action:</span> <span class="string">keep</span></span><br><span class="line"><span class="attr">    regex:</span> <span class="string">prometheus-slave.*</span></span><br><span class="line"><span class="attr">  - source_labels:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">__meta_kubernetes_pod_container_port_number</span></span><br><span class="line"><span class="attr">    action:</span> <span class="string">keep</span></span><br><span class="line"><span class="attr">    regex:</span> <span class="number">9090</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- job_name:</span> <span class="string">federate-kubestate</span></span><br><span class="line"><span class="attr">  honor_labels:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  metrics_path:</span> <span class="string">'/federate'</span></span><br><span class="line"><span class="attr">  params:</span></span><br><span class="line">    <span class="string">'match[]'</span><span class="string">:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">'{__name__=~"deployment:.*|daemonset:.*"}'</span></span><br><span class="line"><span class="attr">  kubernetes_sd_configs:</span></span><br><span class="line"><span class="attr">  - role:</span> <span class="string">pod</span></span><br><span class="line"><span class="attr">    namespaces:</span></span><br><span class="line"><span class="attr">      names:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  relabel_configs:</span></span><br><span class="line"><span class="attr">  - source_labels:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">__meta_kubernetes_pod_label_app</span></span><br><span class="line"><span class="attr">    action:</span> <span class="string">keep</span></span><br><span class="line"><span class="attr">    regex:</span> <span class="string">prometheus-kubestate.*</span></span><br><span class="line"><span class="attr">  - source_labels:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">__meta_kubernetes_pod_container_port_number</span></span><br><span class="line"><span class="attr">    action:</span> <span class="string">keep</span></span><br><span class="line"><span class="attr">    regex:</span> <span class="number">9090</span></span><br></pre></td></tr></tbody></table></figure><p>slave Prometheus配置</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">global:</span></span><br><span class="line"><span class="attr">  scrape_interval:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line"><span class="attr">  scrape_timeout:</span> <span class="number">30</span><span class="string">s</span></span><br><span class="line"><span class="attr">  evaluation_interval:</span> <span class="number">60</span><span class="string">s</span></span><br><span class="line"><span class="attr">  external_labels:</span></span><br><span class="line"><span class="attr">    cluster:</span> <span class="string">{{CLUSTER}}</span></span><br><span class="line"><span class="attr">    production_environment:</span> <span class="string">{{ENV}}</span></span><br><span class="line"></span><br><span class="line"><span class="attr">rule_files:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">node.yml</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">pod.yml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">scrape_configs:</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- job_name:</span> <span class="string">federate-kubestate</span></span><br><span class="line"><span class="attr">  honor_labels:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  metrics_path:</span> <span class="string">'/federate'</span></span><br><span class="line"><span class="attr">  params:</span></span><br><span class="line">    <span class="string">'match[]'</span><span class="string">:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">'{__name__=~"pod:.*|node:.*"}'</span></span><br><span class="line"><span class="attr">  kubernetes_sd_configs:</span></span><br><span class="line"><span class="attr">  - role:</span> <span class="string">pod</span></span><br><span class="line"><span class="attr">    namespaces:</span></span><br><span class="line"><span class="attr">      names:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  relabel_configs:</span></span><br><span class="line"><span class="attr">  - source_labels:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">__meta_kubernetes_pod_label_app</span></span><br><span class="line"><span class="attr">    action:</span> <span class="string">keep</span></span><br><span class="line"><span class="attr">    regex:</span> <span class="string">prometheus-kubestate.*</span></span><br><span class="line"><span class="attr">  - source_labels:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">__meta_kubernetes_pod_container_port_number</span></span><br><span class="line"><span class="attr">    action:</span> <span class="string">keep</span></span><br><span class="line"><span class="attr">    regex:</span> <span class="number">9090</span></span><br><span class="line"><span class="attr">  metric_relabel_configs:</span></span><br><span class="line"><span class="attr">  - source_labels:</span> <span class="string">[node]</span></span><br><span class="line"><span class="attr">    modulus:</span>       <span class="string">{{MODULES}}</span></span><br><span class="line"><span class="attr">    target_label:</span>  <span class="string">__tmp_hash</span></span><br><span class="line"><span class="attr">    action:</span>        <span class="string">hashmod</span></span><br><span class="line"><span class="attr">  - source_labels:</span> <span class="string">[__tmp_hash]</span></span><br><span class="line"><span class="attr">    regex:</span>         <span class="string">{{SLAVEID}}</span></span><br><span class="line"><span class="attr">    action:</span>        <span class="string">keep</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- job_name:</span> <span class="string">kubelet</span></span><br><span class="line"><span class="attr">  scheme:</span> <span class="string">https</span></span><br><span class="line"><span class="attr">  kubernetes_sd_configs:</span></span><br><span class="line"><span class="attr">  - role:</span> <span class="string">node</span></span><br><span class="line"><span class="attr">    namespaces:</span></span><br><span class="line"><span class="attr">      names:</span> <span class="string">[]</span></span><br><span class="line"><span class="attr">  tls_config:</span></span><br><span class="line"><span class="attr">    insecure_skip_verify:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  relabel_configs:</span></span><br><span class="line"><span class="attr">  - source_labels:</span> <span class="string">[]</span></span><br><span class="line"><span class="attr">    regex:</span> <span class="string">__meta_kubernetes_node_label_(.+)</span></span><br><span class="line"><span class="attr">    replacement:</span> <span class="string">"$1"</span></span><br><span class="line"><span class="attr">    action:</span> <span class="string">labelmap</span></span><br><span class="line"><span class="attr">  - source_labels:</span> <span class="string">[__meta_kubernetes_node_label_kubernetes_io_hostname]</span></span><br><span class="line"><span class="attr">    modulus:</span>       <span class="string">{{MODULES}}</span></span><br><span class="line"><span class="attr">    target_label:</span>  <span class="string">__tmp_hash</span></span><br><span class="line"><span class="attr">    action:</span>        <span class="string">hashmod</span></span><br><span class="line"><span class="attr">  - source_labels:</span> <span class="string">[__tmp_hash]</span></span><br><span class="line"><span class="attr">    regex:</span>         <span class="string">{{SLAVEID}}</span></span><br><span class="line"><span class="attr">    action:</span>        <span class="string">keep</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- job_name:</span> <span class="string">...</span></span><br></pre></td></tr></tbody></table></figure><h3 id="可能的问题"><a href="#可能的问题" class="headerlink" title="可能的问题"></a>可能的问题</h3><ul><li>如何部署，配置复杂，现在采用shell脚本加kustomize,是否有更简单的方法</li><li>分区的动态扩展随着node的规模</li><li>kube-state-metrics是否会成为瓶颈，目前的<a href="https://docs.google.com/document/d/1hm5XrM9dYYY085yOnmMDXu074E4RxjM7R5FS4-WOflo/edit" target="_blank" rel="noopener">kube-state-metrics性能测试</a></li><li>由于分区同一个job的不同instance采集的时间有偏差，对聚合有一定影响</li><li>可靠性保证，如果一个或多个slave的挂了如何处理，使用k8s来保证prometheus的可用性是否可靠</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>回家</title>
      <link href="/festival/"/>
      <url>/festival/</url>
      
        <content type="html"><![CDATA[<h2 id="春节到了"><a href="#春节到了" class="headerlink" title="春节到了"></a>春节到了</h2><p>一年又过去了<br>还没有下雪<br>匆匆忙忙<br>春节又来了<br>回家过年</p><h2 id="来年再见"><a href="#来年再见" class="headerlink" title="来年再见"></a>来年再见</h2><p>希望的<br>或许实现了<br>或许还在希望着<br>只能留到明年了</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 杂记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s节点资源不足时指定nodeName的replicaset会创建大量pod(显示OutOfcpu/OutOfmem)</title>
      <link href="/pod-outofcpu-error/"/>
      <url>/pod-outofcpu-error/</url>
      
        <content type="html"><![CDATA[<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>k8s: 1.10.2<br>docker: 17.03</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>当指定nodeName并且节点资源不足时，会创建大量pod，并显示outofcpu/outofmem<br>类似下面：<br>prometheus-slave01-68bd9bc854-slw92 0/2 OutOfcpu 0 1m<br>prometheus-slave01-68bd9bc854-svxbq 0/2 OutOfcpu 0 20s<br>prometheus-slave01-68bd9bc854-sw25t 0/2 OutOfcpu 0 1m<br>…</p><h2 id="相关issue"><a href="#相关issue" class="headerlink" title="相关issue"></a>相关issue</h2><p><a href="https://github.com/kubernetes/kubernetes/issues/38806" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/38806</a></p><h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2><p>设置nodeName会跳过调度，没有对容量做检测<br>分配到节点上显示资源不足，状态变为outofcpu/outofmem，k8s判断replicaset没有检测到期望pod的状态，会重新再起一个pod，而原pod不会主动删除，致使创建大量pod</p><h2 id="附件"><a href="#附件" class="headerlink" title="附件"></a>附件</h2><p>测试yaml<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      nodeName:</span> <span class="string">tj1-jm-cc-stag05.kscn</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">my-nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">200</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span></span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解K8s资源限制[转]</title>
      <link href="/understanding-resource-limits-in-kubernetes/"/>
      <url>/understanding-resource-limits-in-kubernetes/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>当我开始大范围使用Kubernetes的时候，我开始考虑一个我做实验时没有遇到的问题：当集群里的节点没有足够资源的时候，Pod会卡在Pending状态。你是没有办法给节点增加CPU或者内存的，那么你该怎么做才能将这个Pod从这个节点拿走？最简单的办法是添加另一个节点，我承认我总是这么干。最终这个策略无法发挥出Kubernetes最重要的一个能力：即它优化计算资源使用的能力。这些场景里面实际的问题并不是节点太小，而是我们没有仔细为Pod计算过资源限制。</p><p>资源限制是我们可以向Kubernetes提供的诸多配置之一，它意味着两点：工作负载运行需要哪些资源；最多允许消费多少资源。第一点对于调度器而言十分重要，因为它要以此选择合适的节点。第二点对于Kubelet非常重要，每个节点上的守护进程Kubelet负责Pod的运行健康状态。大多数本文的读者可能对资源限制有一定的了解，实际上这里面有很多有趣的细节。在这个系列的两篇文章中我会先仔细分析内存资源限制，然后第二篇文章中分析CPU资源限制。 </p><h2 id="资源限制"><a href="#资源限制" class="headerlink" title="资源限制"></a>资源限制</h2><p>资源限制是通过每个容器containerSpec的resources字段进行设置的，它是v1版本的ResourceRequirements类型的API对象。每个指定了”limits”和”requests”的对象都可以控制对应的资源。目前只有CPU和内存两种资源。第三种资源类型，持久化存储仍然是beta版本，我会在以后的博客里进行分析。大多数情况下，deployment、statefulset、daemonset的定义里都包含了podSpec和多个containerSpec。这里有个完整的v1资源对象的yaml格式配置： </p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span>  </span><br><span class="line"><span class="attr">    requests:</span>    </span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="number">50</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">  limits:</span>    </span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br></pre></td></tr></tbody></table></figure><p>这个对象可以这么理解：这个容器通常情况下，需要5%的CPU时间和50MiB的内存（requests），同时最多允许它使用10%的CPU时间和100MiB的内存（limits）。我会对requests和limits的区别做进一步讲解，但是一般来说，在调度的时候requests比较重要，在运行时limits比较重要。尽管资源限制配置在每个容器上，你可以认为Pod的资源限制就是它里面容器的资源限制之和，我们可以从系统的视角观察到这种关系。 </p><h3 id="内存限制"><a href="#内存限制" class="headerlink" title="内存限制"></a>内存限制</h3><p>通常情况下分析内存要比分析CPU简单一些，所以我从这里开始着手。我的一个目标是给大家展示内存在系统中是如何实现的，也就是Kubernetes对容器运行时（docker/containerd）所做的工作，容器运行时对Linux内核所做的工作。从分析内存资源限制开始也为后面分析CPU打好了基础。首先，让我们回顾一下前面的例子： </p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span>  </span><br><span class="line"><span class="attr">    requests:</span>    </span><br><span class="line"><span class="attr">        memory:</span> <span class="number">50</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    limits:</span>    </span><br><span class="line"><span class="attr">        memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br></pre></td></tr></tbody></table></figure><p>单位后缀Mi表示的是MiB，所以这个资源对象定义了这个容器需要50MiB并且最多能使用100MiB的内存。当然还有其他单位可以进行表示。为了了解如何用这些值是来控制容器进程，我们首先创建一个没有配置内存限制的Pod: </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run <span class="built_in">limit</span>-test --image=busybox --<span class="built_in">command</span> -- /bin/sh -c <span class="string">"while true; do sleep 2; done"</span></span><br><span class="line">deployment.apps <span class="string">"limit-test"</span> created</span><br></pre></td></tr></tbody></table></figure><p>用Kubectl命令我们可以验证这个Pod是没有资源限制的： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods <span class="built_in">limit</span>-test-7cff9996fc-zpjps -o=jsonpath=<span class="string">'{.spec.containers[0].resources}'</span></span><br><span class="line">map[]</span><br></pre></td></tr></tbody></table></figure><p>Kubernetes最酷的一点是你可以跳到系统以外的角度来观察每个构成部分，所以我们登录到运行Pod的节点，看看Docker是如何运行这个容器的：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps | grep busy | cut -d<span class="string">' '</span> -f1</span><br><span class="line">5c3af3101afb</span><br><span class="line">$ docker inspect 5c3af3101afb -f <span class="string">"{{.HostConfig.Memory}}"</span></span><br><span class="line">0</span><br></pre></td></tr></tbody></table></figure><p>这个容器的<code>.HostConfig.Memory</code>域对应了docker run时的<code>--memory</code>参数，0值表示未设定。Docker会对这个值做什么？为了控制容器进程能够访问的内存数量，Docker配置了一组control group，或者叫cgroup。Cgroup在2008年1月时合并到Linux 2.6.24版本的内核。它是一个很重要的话题。我们说cgroup是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU和各种设备都有对应的cgroup。Cgroup是具有层级的，这意味着每个cgroup拥有一个它可以继承属性的父亲，往上一直直到系统启动时创建的root cgroup。</p><p>Cgroup可以通过/proc和/sys伪文件系统轻松查看到，所以检查容器如何配置内存的cgroup就很简单了。在容器的Pid namespace里，根进程的pid为1，但是namespace以外它呈现的是系统级pid，我们可以用来查找它的cgroups： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ps ax | grep /bin/sh</span><br><span class="line">   9513 ?        Ss     0:00 /bin/sh -c <span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> sleep 2; <span class="keyword">done</span></span><br><span class="line">$ sudo cat /proc/9513/cgroup</span><br><span class="line">...</span><br><span class="line">6:memory:/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574</span><br></pre></td></tr></tbody></table></figure><p>我列出了内存cgroup，这正是我们所关注的。你在路径里可以看到前面提到的cgroup层级。一些比较重要的点是：首先，这个路径是以kubepods开始的cgroup，所以我们的进程继承了这个group的每个属性，还有burstable的属性（Kubernetes将Pod设置为burstable QoS类别）和一组用于审计的Pod表示。最后一段路径是我们进程实际使用的cgroup。我们可以把它追加到<code>/sys/fs/cgroups/memory</code>后面查看更多信息： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l /sys/fs/cgroup/memory/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574</span><br><span class="line">...</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 27 19:53 memory.limit_in_bytes</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 27 19:53 memory.soft_limit_in_bytes</span><br></pre></td></tr></tbody></table></figure><p>再一次，我只列出了我们所关心的记录。我们暂时不关注<code>memory.soft_limit_in_bytes</code>，而将重点转移到<code>memory.limit_in_bytes</code>属性，它设置了内存限制。它等价于Docker命令中的<code>--memory</code>参数，也就是Kubernetes里的内存资源限制。我们看看： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/podfbc202d3-da21-11e8-ab5e-42010a80014b/0a1b22ec1361a97c3511db37a4bae932d41b22264e5b97611748f8b662312574/memory.limit_in_bytes</span><br><span class="line">9223372036854771712</span><br></pre></td></tr></tbody></table></figure><p>这是没有设置资源限制时我的节点上显示的情况。这里有对它的一个简单的解释(<a href="https://unix.stackexchange.com/questions/420906/what-is-the-value-for-the-cgroups-limit-in-bytes-if-the-memory-is-not-restricte)。" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/420906/what-is-the-value-for-the-cgroups-limit-in-bytes-if-the-memory-is-not-restricte)。</a> 所以我们看到如果没有在Kubernetes里设置内存限制的话，会导致Docker设置<code>HostConfig.Memory</code>值为0，并进一步导致容器进程被放置在默认值为”no limit”的<code>memory.limit_in_bytes</code>内存cgroup下。我们现在创建使用100MiB内存限制的Pod： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run <span class="built_in">limit</span>-test --image=busybox --limits <span class="string">"memory=100Mi"</span> --<span class="built_in">command</span> -- /bin/sh -c <span class="string">"while true; do sleep 2; done"</span></span><br><span class="line">deployment.apps <span class="string">"limit-test"</span> created</span><br></pre></td></tr></tbody></table></figure><p>我们再一次使用kubectl验证我们的资源配置： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods <span class="built_in">limit</span>-test-5f5c7dc87d-8qtdx -o=jsonpath=<span class="string">'{.spec.containers[0].resources}'</span></span><br><span class="line">map[limits:map[memory:100Mi] requests:map[memory:100Mi]]</span><br></pre></td></tr></tbody></table></figure><p>你会注意到除了我们设置的limits外，Pod还增加了requests。当你设置limits而没有设置requests时，Kubernetes默认让requests等于limits。如果你从调度器的角度看这是非常有意义的。我会在下面进一步讨论requests。当这个Pod启动后，我们可以看到Docker如何配置的容器以及这个进程的内存cgroup： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps | grep busy | cut -d<span class="string">' '</span> -f1</span><br><span class="line">8fec6c7b6119</span><br><span class="line">$ docker inspect 8fec6c7b6119 --format <span class="string">'{{.HostConfig.Memory}}'</span></span><br><span class="line">104857600</span><br><span class="line">$ ps ax | grep /bin/sh</span><br><span class="line">   29532 ?      Ss     0:00 /bin/sh -c <span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> sleep 2; <span class="keyword">done</span></span><br><span class="line">$ sudo cat /proc/29532/cgroup</span><br><span class="line">...</span><br><span class="line">6:memory:/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11</span><br><span class="line">$ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11/memory.limit_in_bytes</span><br><span class="line">104857600</span><br></pre></td></tr></tbody></table></figure><p>正如你所见，Docker基于我们的containerSpec正确地设置了这个进程的内存cgroup。但是这对于运行时意味着什么？Linux内存管理是一个复杂的话题，Kubernetes工程师需要知道的是：当一个宿主机遇到了内存资源压力时，内核可能会有选择性地杀死进程。如果一个使用了多于限制内存的进程会有更高几率被杀死。因为Kubernetes的任务是尽可能多地向这些节点上安排Pod，这会导致节点内存压力异常。如果你的容器使用了过多内存，那么它很可能会被oom-killed。如果Docker收到了内核的通知，Kubernetes会找到这个容器并依据设置尝试重启这个Pod。</p><p>所以Kubernetes默认创建的内存requests是什么？拥有一个100MiB的内存请求会影响到cgroup？可能它设置了我们之前看到的<code>memory.soft_limit_in_bytes</code>？让我们看看： </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cat /sys/fs/cgroup/memory/kubepods/burstable/pod88f89108-daf7-11e8-b1e1-42010a800070/8fec6c7b61190e74cd9f88286181dd5fa3bbf9cf33c947574eb61462bc254d11/memory.soft_limit_in_bytes</span><br><span class="line">9223372036854771712</span><br></pre></td></tr></tbody></table></figure><p>你可以看到软限制仍然被设置为默认值“no limit”。即使Docker支持通过参数<code>--memory-reservation</code>进行设置，但Kubernetes并不支持这个参数。这是否意味着为你的容器指定内存requests并不重要？不，不是的。requests要比limits更重要。limits告诉Linux内核什么时候你的进程可以为了清理空间而被杀死。requests帮助Kubernetes调度找到合适的节点运行Pod。如果不设置它们，或者设置得非常低，那么可能会有不好的影响。</p><p>例如，假设你没有配置内存requests来运行Pod，而配置了一个较高的limits。正如我们所知道的Kubernetes默认会把requests的值指向limits，如果没有合适的资源的节点的话，Pod可能会调度失败，即使它实际需要的资源并没有那么多。另一方面，如果你运行了一个配置了较低requests值的Pod，你其实是在鼓励内核oom-kill掉它。为什么？假设你的Pod通常使用100MiB内存，你却只为它配置了50MiB内存requests。如果你有一个拥有75MiB内存空间的节点，那么这个Pod会被调度到这个节点。当Pod内存消耗扩大到100MiB时，会让这个节点压力变大，这个时候内核可能会选择杀掉你的进程。所以我们要正确配置Pod的内存requests和limits。</p><p>希望这篇文章能够帮助说明Kubernetes容器内存限制是如何设置和实现的，以及为什么你需要正确设置这些值。如果你为Kubernetes提供了它所需要的足够信息，它可以智能地调度你的任务并最大化使用你的云计算资源。 </p><h3 id="CPU限制"><a href="#CPU限制" class="headerlink" title="CPU限制"></a>CPU限制</h3><p>CPU 资源限制比内存资源限制更复杂，原因将在下文详述。幸运的是 CPU 资源限制和内存资源限制一样都是由 cgroup 控制的，上文中提到的思路和工具在这里同样适用，我们只需要关注他们的不同点就行了。首先，让我们将 CPU 资源限制添加到之前示例中的 yaml：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="number">50</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="number">100</span><span class="string">m</span></span><br></pre></td></tr></tbody></table></figure><p>单位后缀 m 表示千分之一核，也就是说 1 Core = 1000m。因此该资源对象指定容器进程需要 50/1000 核（5%）才能被调度，并且允许最多使用 100/1000 核（10%）。同样，2000m 表示两个完整的 CPU 核心，你也可以写成 2 或者 2.0。为了了解 Docker 和 cgroup 如何使用这些值来控制容器，我们首先创建一个只配置了 CPU requests 的 Pod：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run <span class="built_in">limit</span>-test --image=busybox --requests <span class="string">"cpu=50m"</span> --<span class="built_in">command</span> -- /bin/sh -c <span class="string">"while true; do sleep 2; done"</span></span><br><span class="line">deployment.apps <span class="string">"limit-test"</span> created</span><br></pre></td></tr></tbody></table></figure><p>通过 kubectl 命令我们可以验证这个 Pod 配置了 50m 的 CPU requests：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods <span class="built_in">limit</span>-test-5b4c495556-p2xkr -o=jsonpath=<span class="string">'{.spec.containers[0].resources}'</span></span><br><span class="line">map[requests:map[cpu:50m]]</span><br></pre></td></tr></tbody></table></figure><p>我们还可以看到 Docker 为容器配置了相同的资源限制：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps | grep busy | cut -d<span class="string">' '</span> -f1</span><br><span class="line">f2321226620e</span><br><span class="line"></span><br><span class="line">$ docker inspect f2321226620e --format <span class="string">'{{.HostConfig.CpuShares}}'</span></span><br><span class="line">51</span><br></pre></td></tr></tbody></table></figure><p>这里显示的为什么是 51，而不是 50？这是因为 Linux cgroup 和 Docker 都将 CPU 核心数分成了 1024 个时间片（shares），而 Kubernetes 将它分成了 1000 个 shares。<br>shares 用来设置 CPU 的相对值，并且是针对所有的 CPU（内核），默认值是 1024，假如系统中有两个 cgroup，分别是 A 和 B，A 的 shares 值是 1024，B 的 shares 值是 512，那么 A 将获得 1024/(1204+512)=66% 的 CPU 资源，而 B 将获得 33% 的 CPU 资源。</p><p>shares 有两个特点：</p><ol><li>如果 A 不忙，没有使用到 66% 的 CPU 时间，那么剩余的 CPU 时间将会被系统分配给 B，即 B 的 CPU 使用率可以超过 33%。</li><li>如果添加了一个新的 cgroup C，且它的 shares 值是 1024，那么 A 的限额变成了 1024/(1204+512+1024)=40%，B 的变成了 20%。</li></ol><p>从上面两个特点可以看出：</p><p>在闲的时候，shares 基本上不起作用，只有在 CPU 忙的时候起作用，这是一个优点。<br>由于 shares 是一个绝对值，需要和其它 cgroup 的值进行比较才能得到自己的相对限额，而在一个部署很多容器的机器上，cgroup 的数量是变化的，所以这个限额也是变化的，自己设置了一个高的值，但别人可能设置了一个更高的值，所以这个功能没法精确的控制 CPU 使用率。</p><p>与配置内存资源限制时 Docker 配置容器进程的内存 cgroup 的方式相同，设置 CPU 资源限制时 Docker 会配置容器进程的 cpu,cpuacct cgroup：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ ps ax | grep /bin/sh</span><br><span class="line">   60554 ?      Ss     0:00 /bin/sh -c <span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> sleep 2; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">$ sudo cat /proc/60554/cgroup</span><br><span class="line">...</span><br><span class="line">4:cpu,cpuacct:/kubepods/burstable/pode12b33b1-db07-11e8-b1e1-42010a800070/3be263e7a8372b12d2f8f8f9b4251f110b79c2a3bb9e6857b2f1473e640e8e75</span><br><span class="line"></span><br><span class="line">$ ls -l /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pode12b33b1-db07-11e8-b1e1-42010a800070/3be263e7a8372b12d2f8f8f9b4251f110b79c2a3bb9e6857b2f1473e640e8e75</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 Oct 28 23:19 .</span><br><span class="line">drwxr-xr-x 4 root root 0 Oct 28 23:19 ..</span><br><span class="line">...</span><br><span class="line">-rw-r--r-- 1 root root 0 Oct 28 23:19 cpu.shares</span><br></pre></td></tr></tbody></table></figure><p>Docker 容器的 HostConfig.CpuShares 属性映射到 cgroup 的 cpu.shares 属性，可以验证一下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/podb5c03ddf-db10-11e8-b1e1-42010a800070/64b5f1b636dafe6635ddd321c5b36854a8add51931c7117025a694281fb11444/cpu.shares</span><br><span class="line">51</span><br></pre></td></tr></tbody></table></figure><p>你可能会很惊讶，设置了 CPU requests 竟然会把值传播到 cgroup，而在上一篇文章中我们设置内存 requests 时并没有将值传播到 cgroup。这是因为内存的 soft limit 内核特性对 Kubernetes 不起作用，而设置了 cpu.shares 却对 Kubernetes 很有用。后面我会详细讨论为什么会这样。现在让我们先看看设置 CPU limits 时会发生什么：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run <span class="built_in">limit</span>-test --image=busybox --requests <span class="string">"cpu=50m"</span> --limits <span class="string">"cpu=100m"</span> --<span class="built_in">command</span> -- /bin/sh -c <span class="string">"while true; do</span></span><br><span class="line"><span class="string">sleep 2; done"</span></span><br><span class="line">deployment.apps <span class="string">"limit-test"</span> created</span><br></pre></td></tr></tbody></table></figure><p>再一次使用 kubectl 验证我们的资源配置：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods <span class="built_in">limit</span>-test-5b4fb64549-qpd4n -o=jsonpath=<span class="string">'{.spec.containers[0].resources}'</span></span><br><span class="line">map[limits:map[cpu:100m] requests:map[cpu:50m]]</span><br></pre></td></tr></tbody></table></figure><p>查看对应的 Docker 容器的配置：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps | grep busy | cut -d<span class="string">' '</span> -f1</span><br><span class="line">f2321226620e</span><br><span class="line">$ docker inspect 472abbce32a5 --format <span class="string">'{{.HostConfig.CpuShares}} {{.HostConfig.CpuQuota}} {{.HostConfig.CpuPeriod}}'</span></span><br><span class="line">51 10000 100000</span><br></pre></td></tr></tbody></table></figure><p>可以明显看出，CPU requests 对应于 Docker 容器的 HostConfig.CpuShares 属性。而 CPU limits 就不太明显了，它由两个属性控制：HostConfig.CpuPeriod 和 HostConfig.CpuQuota。Docker 容器中的这两个属性又会映射到进程的 cpu,couacct cgroup 的另外两个属性：cpu.cfs_period_us 和 cpu.cfs_quota_us。我们来看一下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod2f1b50b6-db13-11e8-b1e1-42010a800070/f0845c65c3073e0b7b0b95ce0c1eb27f69d12b1fe2382b50096c4b59e78cdf71/cpu.cfs_period_us</span><br><span class="line">100000</span><br><span class="line"></span><br><span class="line">$ sudo cat /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod2f1b50b6-db13-11e8-b1e1-42010a800070/f0845c65c3073e0b7b0b95ce0c1eb27f69d12b1fe2382b50096c4b59e78cdf71/cpu.cfs_quota_us</span><br><span class="line">10000</span><br></pre></td></tr></tbody></table></figure><p>如我所说，这些值与容器配置中指定的值相同。但是这两个属性的值是如何从我们在 Pod 中设置的 100m cpu limits 得出的呢，他们是如何实现该 limits 的呢？这是因为 cpu requests 和 cpu limits 是使用两个独立的控制系统来实现的。Requests 使用的是 cpu shares 系统，cpu shares 将每个 CPU 核心划分为 1024 个时间片，并保证每个进程将获得固定比例份额的时间片。如果总共有 1024 个时间片，并且两个进程中的每一个都将 cpu.shares 设置为 512，那么它们将分别获得大约一半的 CPU 可用时间。但 cpu shares 系统无法精确控制 CPU 使用率的上限，如果一个进程没有设置 shares，则另一个进程可用自由使用 CPU 资源。</p><p>大约在 2010 年左右，谷歌团队和其他一部分人注意到了这个问题。为了解决这个问题，后来在 linux 内核中增加了第二个功能更强大的控制系统：CPU 带宽控制组。带宽控制组定义了一个 周期，通常为 1/10 秒（即 100000 微秒）。还定义了一个 配额，表示允许进程在设置的周期长度内所能使用的 CPU 时间数，两个文件配合起来设置CPU的使用上限。两个文件的单位都是微秒（us），cfs_period_us 的取值范围为 1 毫秒（ms）到 1 秒（s），cfs_quota_us 的取值大于 1ms 即可，如果 cfs_quota_us 的值为 -1（默认值），表示不受 CPU 时间的限制。</p><p>下面是几个例子：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.限制只能使用1个CPU（每250ms能使用250ms的CPU时间）</span></span><br><span class="line">$ <span class="built_in">echo</span> 250000 &gt; cpu.cfs_quota_us /* quota = 250ms */</span><br><span class="line">$ <span class="built_in">echo</span> 250000 &gt; cpu.cfs_period_us /* period = 250ms */</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.限制使用2个CPU（内核）（每500ms能使用1000ms的CPU时间，即使用两个内核）</span></span><br><span class="line">$ <span class="built_in">echo</span> 1000000 &gt; cpu.cfs_quota_us /* quota = 1000ms */</span><br><span class="line">$ <span class="built_in">echo</span> 500000 &gt; cpu.cfs_period_us /* period = 500ms */</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.限制使用1个CPU的20%（每50ms能使用10ms的CPU时间，即使用一个CPU核心的20%）</span></span><br><span class="line">$ <span class="built_in">echo</span> 10000 &gt; cpu.cfs_quota_us /* quota = 10ms */</span><br><span class="line">$ <span class="built_in">echo</span> 50000 &gt; cpu.cfs_period_us /* period = 50ms */</span><br></pre></td></tr></tbody></table></figure><p>在本例中我们将 Pod 的 cpu limits 设置为 100m，这表示 100/1000 个 CPU 核心，即 100000 微秒的 CPU 时间周期中的 10000。所以该 limits 翻译到 cpu,cpuacct cgroup 中被设置为 cpu.cfs_period_us=100000 和 cpu.cfs_quota_us=10000。顺便说一下，其中的 cfs 代表 Completely Fair Scheduler（绝对公平调度），这是 Linux 系统中默认的 CPU 调度算法。还有一个实时调度算法，它也有自己相应的配额值。</p><p>现在让我们来总结一下：</p><ul><li>在 Kubernetes 中设置的 cpu requests 最终会被 cgroup 设置为 cpu.shares 属性的值， cpu limits 会被带宽控制组设置为 cpu.cfs_period_us 和 cpu.cfs_quota_us 属性的值。与内存一样，cpu requests 主要用于在调度时通知调度器节点上至少需要多少个 cpu shares 才可以被调度。</li><li>与 内存 requests 不同，设置了 cpu requests 会在 cgroup 中设置一个属性，以确保内核会将该数量的 shares 分配给进程。</li><li>cpu limits 与 内存 limits 也有所不同。如果容器进程使用的内存资源超过了内存使用限制，那么该进程将会成为 oom-killing 的候选者。但是容器进程基本上永远不能超过设置的 CPU 配额，所以容器永远不会因为尝试使用比分配的更多的 CPU 时间而被驱逐。系统会在调度程序中强制进行 CPU 资源限制，以确保进程不会超过这个限制。</li></ul><p>如果你没有在容器中设置这些属性，或将他们设置为不准确的值，会发生什么呢？与内存一样，如果只设置了 limits 而没有设置 requests，Kubernetes 会将 CPU 的 requests 设置为 与 limits 的值一样。如果你对你的工作负载所需要的 CPU 时间了如指掌，那再好不过了。如果只设置了 CPU requests 却没有设置 CPU limits 会怎么样呢？这种情况下，Kubernetes 会确保该 Pod 被调度到合适的节点，并且该节点的内核会确保节点上的可用 cpu shares 大于 Pod 请求的 cpu shares，但是你的进程不会被阻止使用超过所请求的 CPU 数量。既不设置 requests 也不设置 limits 是最糟糕的情况：调度程序不知道容器需要什么，并且进程对 cpu shares 的使用是无限制的，这可能会对 node 产生一些负面影响。</p><p>最后我还想告诉你们的是：为每个 pod 都手动配置这些参数是挺麻烦的事情，kubernetes 提供了 LimitRange 资源，可以让我们配置某个 namespace 默认的 request 和 limit 值。</p><h2 id="默认限制"><a href="#默认限制" class="headerlink" title="默认限制"></a>默认限制</h2><p>通过上文的讨论大家已经知道了忽略资源限制会对 Pod 产生负面影响，因此你可能会想，如果能够配置某个 namespace 默认的 request 和 limit 值就好了，这样每次创建新 Pod 都会默认加上这些限制。Kubernetes 允许我们通过 LimitRange 资源对每个命名空间设置资源限制。要创建默认的资源限制，需要在对应的命名空间中创建一个 LimitRange 资源。下面是一个例子：</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">LimitRange</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">default-limit</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">  - default:</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">    defaultRequest:</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">50</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line"><span class="attr">  - max:</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">512</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">500</span><span class="string">m</span></span><br><span class="line"><span class="attr">  - min:</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">50</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">50</span><span class="string">m</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Container</span></span><br></pre></td></tr></tbody></table></figure><p>这里的几个字段可能会让你们有些困惑，我拆开来给你们分析一下。</p><ul><li>limits 字段下面的 default 字段表示每个 Pod 的默认的 limits 配置，所以任何没有分配资源的 limits 的 Pod 都会被自动分配 100Mi limits 的内存和 100m limits 的 CPU。</li><li>defaultRequest 字段表示每个 Pod 的默认 requests 配置，所以任何没有分配资源的 requests 的 Pod 都会被自动分配 50Mi requests 的内存和 50m requests 的 CPU。</li><li>max 和 min 字段比较特殊，如果设置了这两个字段，那么只要这个命名空间中的 Pod 设置的 limits 和 requests 超过了这个上限和下限，就不会允许这个 Pod 被创建。我暂时还没有发现这两个字段的用途，如果你知道，欢迎在留言告诉我。</li><li>LimitRange 中设定的默认值最后由 Kubernetes 中的准入控制器 LimitRanger 插件来实现。准入控制器由一系列插件组成，它会在 API 接收对象之后创建 Pod 之前对 Pod 的 Spec - 字段进行修改。对于 LimitRanger 插件来说，它会检查每个 Pod 是否设置了 limits 和 requests，如果没有设置，就给它配置 LimitRange 中设定的默认值。通过检查 Pod 中的 annotations 注释，你可以看到 LimitRanger 插件已经在你的 Pod 中设置了默认值。例如：</li></ul><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/limit-ranger:</span> <span class="string">'LimitRanger plugin set: cpu request for container</span></span><br><span class="line"><span class="string">      limit-test'</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">limit-test-859d78bc65-g6657</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - args:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">/bin/sh</span></span><br><span class="line"><span class="bullet">    -</span> <span class="bullet">-c</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">while</span> <span class="literal">true</span><span class="string">;</span> <span class="string">do</span> <span class="string">sleep</span> <span class="number">2</span><span class="string">;</span> <span class="string">done</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">limit-test</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      requests:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="number">100</span><span class="string">m</span></span><br></pre></td></tr></tbody></table></figure><p>以上就是我对 Kubernetes 资源限制的全部见解，希望能对你有所帮助。如果你想了解更多关于 Kubernetes 中资源的 limits 和 requests、以及 linux cgroup 和内存管理的更多详细信息，可以查看我在文末提供的参考链接。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b" target="_blank" rel="noopener">https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-cpu-time-9eff74d3161b</a></li><li><a href="https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-memory-6b41e9a955f9" target="_blank" rel="noopener">https://medium.com/@betz.mark/understanding-resource-limits-in-kubernetes-memory-6b41e9a955f9</a></li><li><a href="https://www.yangcs.net/posts/understanding-resource-limits-in-kubernetes-cpu-time/" target="_blank" rel="noopener">https://www.yangcs.net/posts/understanding-resource-limits-in-kubernetes-cpu-time/</a></li><li><a href="https://www.yangcs.net/posts/understanding-resource-limits-in-kubernetes-cpu-time/" target="_blank" rel="noopener">https://www.yangcs.net/posts/understanding-resource-limits-in-kubernetes-cpu-time/</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
            <tag> cgroup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一次golang性能分析</title>
      <link href="/golang-pprof/"/>
      <url>/golang-pprof/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近在做 prometheus 监控，需要将 prometheus 聚合数据打向 falcon, 写了个 falcon-adapter，部署到小集群上没问题，最后部署在线上集群 400 nodes k8s 集群出现 部分数据抓取不上， falcon断点严重， 部署节点 load 过高到达 50.</p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>追溯某些数据发现 prometheus 采集不到，确认抓取组件没有问题<br>查看 falcon，数据采集时间过长，导致断点<br>查看 falcon-adapter, 有大量 TIME_WAIT 链接<br>无法确定哪一环出现问题，借助 pprof 进行性能分析<br>查看 falcon-adapter 得到火焰图如下<br><img src="/img/blogImg/golang分析.png" alt="before"><br>其中 falcon-adapter 的 metricFilter 操作占了 71.75%， 大部分是 regexp.MatchString 和 regexp.compile 占用的，这在一个转发程序是不正常的，通常应该 HTTP IO 操作占大头<br>在 metricFilter 中主要实现了prometheus metrics 的过滤，保存需要的 metrics, 其中有大量正则匹配，可能因此出现性能问题<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">func metricFilter(str string) string {</span><br><span class="line">    for _, scope := range config.Scope {</span><br><span class="line">        regStr := "^" + scope + ":"</span><br><span class="line">        if match, _ := regexp.MatchString(regStr, str); match {</span><br><span class="line">            return scope</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    return ""</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>将所有正则操作全部替换为常规字符串操作后，得到火焰图如下<br><img src="/img/blogImg/golang分析1.png" alt="after"><br>net 占有大部分 cpu 时间，meticFilter 只占用 1.67%！</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>在大规模集群中编程一定要考虑性能问题，避免出现类似问题<br>善于利用 pprof 类似工具分析程序性能问题</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> golang </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pod memory usage in k8s</title>
      <link href="/pod-memory-usage-in-k8s/"/>
      <url>/pod-memory-usage-in-k8s/</url>
      
        <content type="html"><![CDATA[<h2 id="Cadvisor内存使用率指标"><a href="#Cadvisor内存使用率指标" class="headerlink" title="Cadvisor内存使用率指标"></a>Cadvisor内存使用率指标</h2><h3 id="Cadvisor中有关pod内存使用率的指标"><a href="#Cadvisor中有关pod内存使用率的指标" class="headerlink" title="Cadvisor中有关pod内存使用率的指标"></a>Cadvisor中有关pod内存使用率的指标</h3><table><thead><tr><th>指标</th><th>说明</th></tr></thead><tbody><tr><td>container_memory_cache</td><td>Number of bytes of page cache memory.</td></tr><tr><td>container_memory_rss</td><td>Size of RSS in bytes.(包括匿名映射页和交换区缓存)</td></tr><tr><td>container_memory_swap</td><td>Container swap usage in bytes.</td></tr><tr><td>container_memory_usage_bytes</td><td>Current memory usage in bytes,including all memory regardless ofwhen it was accessed. (包括 cache, rss, swap等)</td></tr><tr><td>container_memory_max_usage_bytes</td><td>Maximum memory usage recorded in bytes.</td></tr><tr><td>container_memory_working_set_bytes</td><td>Current working set in bytes. （工作区内存使用量=活跃的匿名与和缓存,以及file-baked页 &lt;=container_memory_usage_bytes）</td></tr><tr><td>container_memory_failcnt</td><td>Number of memory usage hits limits.</td></tr><tr><td>container_memory_failures_total</td><td>Cumulative count of memory allocation failures.</td></tr></tbody></table><p>其中<br><code>container_memory_max_usage_bytes &gt; container_memory_usage_bytes &gt;= container_memory_working_set_bytes &gt; container_memory_rss</code></p><h3 id="Cadvisor中相关定义"><a href="#Cadvisor中相关定义" class="headerlink" title="Cadvisor中相关定义"></a>Cadvisor中相关定义</h3><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">type MemoryStats struct { // Current memory usage, this includes all memory regardless of when it was // accessed. // Units: Bytes. Usage uint64 json:"usage"</span><br><span class="line"></span><br><span class="line">// Maximum memory usage recorded.</span><br><span class="line">// Units: Bytes.</span><br><span class="line">MaxUsage uint64 `json:"max_usage"`</span><br><span class="line"></span><br><span class="line">// Number of bytes of page cache memory.</span><br><span class="line">// Units: Bytes.</span><br><span class="line">Cache uint64 `json:"cache"`</span><br><span class="line"></span><br><span class="line">// The amount of anonymous and swap cache memory (includes transparent</span><br><span class="line">// hugepages).</span><br><span class="line">// Units: Bytes.</span><br><span class="line">RSS uint64 `json:"rss"`</span><br><span class="line"></span><br><span class="line">// The amount of swap currently used by the processes in this cgroup</span><br><span class="line">// Units: Bytes.</span><br><span class="line">Swap uint64 `json:"swap"`</span><br><span class="line"></span><br><span class="line">// The amount of working set memory, this includes recently accessed memory,</span><br><span class="line">// dirty memory, and kernel memory. Working set is &lt;= "usage".</span><br><span class="line">// Units: Bytes.</span><br><span class="line">WorkingSet uint64 `json:"working_set"`</span><br><span class="line"></span><br><span class="line">Failcnt uint64 `json:"failcnt"`</span><br><span class="line"></span><br><span class="line">ContainerData    MemoryStatsMemoryData `json:"container_data,omitempty"`</span><br><span class="line">HierarchicalData MemoryStatsMemoryData `json:"hierarchical_data,omitempty"`</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><blockquote><p>You might think that memory utilization is easily tracked with container_memory_usage_bytes, however, this metric also includes cached (think filesystem cache) items that can be evicted under memory pressure. The better metric is container_memory_working_set_bytes as this is what the OOM killer is watching for.<br>To calculate container memory utilization we use: sum(container_memory_working_set_bytes{name!~”POD”}) by (name)</p></blockquote><p>kubelet 通过 watch container_memory_working_set_bytes 来判断是否OOM， 所以用 working set来评价容器内存使用量更科学</p><h2 id="Cgroup中关于mem指标"><a href="#Cgroup中关于mem指标" class="headerlink" title="Cgroup中关于mem指标"></a>Cgroup中关于mem指标</h2><p>cgroup目录相关文件</p><table><thead><tr><th>文件名</th><th>说明</th><th>cadvisor中对应指标</th></tr></thead><tbody><tr><td>memory.usage_in_bytes</td><td>已使用的内存量(包含cache和buffer)(字节)，相当于linux的used_meme</td><td>container_memory_usage_bytes</td></tr><tr><td>memory.limit_in_bytes</td><td>限制的内存总量(字节)，相当于linux的total_mem</td><td></td></tr><tr><td>memory.failcnt</td><td>申请内存失败次数计数</td><td></td></tr><tr><td>memory.memsw.usage_in_bytes</td><td>已使用的内存和swap(字节)</td><td></td></tr><tr><td>memory.memsw.limit_in_bytes</td><td>限制的内存和swap容量(字节)</td><td></td></tr><tr><td>memory.memsw.failcnt</td><td>申请内存和swap失败次数计数</td><td></td></tr><tr><td>memory.stat</td><td>内存相关状态</td><td></td></tr></tbody></table><p>memory.stat中包含有的内存信息</p><table><thead><tr><th>统计</th><th>描述</th><th>cadvisor中对应指标</th></tr></thead><tbody><tr><td>cache</td><td>页缓存，包括 tmpfs（shmem），单位为字节</td><td>container_memory_cache</td></tr><tr><td>rss</td><td>匿名和 swap 缓存，不包括 tmpfs（shmem），单位为字节</td><td>container_memory_rss</td></tr><tr><td>mapped_file</td><td>memory-mapped 映射的文件大小，包括 tmpfs（shmem），单位为字节</td><td></td></tr><tr><td>pgpgin</td><td>存入内存中的页数</td><td></td></tr><tr><td>pgpgout</td><td>从内存中读出的页数</td><td></td></tr><tr><td>swap</td><td>swap 用量，单位为字节</td><td>container_memory_swap</td></tr><tr><td>active_anon</td><td>在活跃的最近最少使用（least-recently-used，LRU）列表中的匿名和 swap 缓存，包括 tmpfs（shmem），单位为字节</td><td></td></tr><tr><td>inactive_anon</td><td>不活跃的 LRU 列表中的匿名和 swap 缓存，包括 tmpfs（shmem），单位为字节</td><td></td></tr><tr><td>active_file</td><td>活跃 LRU 列表中的 file-backed 内存，以字节为单位</td><td></td></tr><tr><td>inactive_file</td><td>不活跃 LRU 列表中的 file-backed 内存，以字节为单位</td><td></td></tr><tr><td>unevictable</td><td>无法再生的内存，以字节为单位</td><td></td></tr><tr><td>hierarchical_memory_limit</td><td>包含 memory cgroup 的层级的内存限制，单位为字节</td><td></td></tr><tr><td>hierarchical_memsw_limit</td><td>包含 memory cgroup 的层级的内存加 swap 限制，单位为字节</td><td></td></tr></tbody></table><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">active_anon + inactive_anon = anonymous memory + file cache for tmpfs + swap cache = rss + file cache for tmpfs </span><br><span class="line">active_file + inactive_file = cache - size of tmpfs</span><br><span class="line">working set = usage - total_inactive(k8s根据workingset 来判断是否驱逐pod)</span><br></pre></td></tr></tbody></table></figure><p>mstat看到的active/inactive memory就分别是active list和inactive list中的内存大小。如果inactive list很大，表明在必要时可以回收的页面很多；而如果inactive list很小，说明可以回收的页面不多。<br>Active/inactive memory是针对用户进程所占用的内存而言的，内核占用的内存（包括slab）不在其中。<br>至于在源代码中看到的ACTIVE_ANON和ACTIVE_FILE，分别表示anonymous pages和file-backed pages。用户进程的内存页分为两种：与文件关联的内存（比如程序文件、数据文件所对应的内存页）和与文件无关的内存（比如进程的堆栈，用malloc申请的内存），前者称为file-backed pages，后者称为anonymous pages。File-backed pages在发生换页(page-in或page-out)时，是从它对应的文件读入或写出；anonymous pages在发生换页时，是对交换区进行读/写操作。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66" target="_blank" rel="noopener">https://blog.freshtracks.io/a-deep-dive-into-kubernetes-metrics-part-3-container-resource-metrics-361c5ee46e66</a></li><li><a href="https://github.com/google/cadvisor/blob/08f0c2397cbca790a4db0f1212cb592cc88f6e26/info/v1/container.go#L338:6" target="_blank" rel="noopener">https://github.com/google/cadvisor/blob/08f0c2397cbca790a4db0f1212cb592cc88f6e26/info/v1/container.go#L338:6</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
            <tag> cadvisor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pod一直显示Terminating</title>
      <link href="/pod-terminating-long-time/"/>
      <url>/pod-terminating-long-time/</url>
      
        <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>集群中有一个pod一直显示Terminating</p><h3 id="event"><a href="#event" class="headerlink" title="event"></a>event</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Normal Scheduled 1h default-scheduler Successfully assigned feed-426565da19777e5d325f-5994dc5cff-znqmh to node01</span><br><span class="line">Normal SuccessfulMountVolume 1h kubelet, node01 (combined from similar events): MountVolume.SetUp succeeded <span class="keyword">for</span> volume <span class="string">"lvm"</span></span><br><span class="line">Normal Pulled 1h kubelet, node01 Container image  already present on machine</span><br><span class="line">Normal Created 1h kubelet, node01 Created container</span><br><span class="line">Normal Started 1h kubelet, node01 Started container</span><br><span class="line">Warning Unhealthy 9m (x44 over 1h) kubelet, node01 Liveness probe failed: Get http://*:65318/state.json: dial tcp *.*.*.*:65318: getsockopt: connection refused</span><br><span class="line">Warning Unhealthy 9m (x45 over 1h) kubelet, node01 Readiness probe failed: Get http://*:65318/state.json: dial tcp *.*.*.*:65318: getsockopt: connection refused</span><br><span class="line">Normal Killing 9m kubelet, node01 Killing container with id docker://main:Need to <span class="built_in">kill</span> Pod</span><br><span class="line">Warning FailedKillPod 5m (x2 over 7m) kubelet, node01 error killing pod: failed to <span class="string">"KillPodSandbox"</span> <span class="keyword">for</span> <span class="string">"163f99a9-1aec-11e9-a7cd-246e96ab9970"</span> with KillPodSandboxError: <span class="string">"rpc error: code = DeadlineExceeded desc = context deadline exceeded"</span></span><br></pre></td></tr></tbody></table></figure><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubelet: error killing pod: failed to <span class="string">"KillPodSandbox"</span> <span class="keyword">for</span> <span class="string">"163f99a9-1aec-11e9-a7cd-246e96ab9970"</span> with KillPodSandboxError: <span class="string">"rpc error: code = DeadlineExceeded desc = context deadline exceeded"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="探究"><a href="#探究" class="headerlink" title="探究"></a>探究</h2><h3 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ps aux |grep D <span class="comment">#查看无法终止的进程（stat D）</span></span><br><span class="line">root 2626 0.0 0.0 0 0 ? Ds 14:42 0:00 [pause]</span><br><span class="line"></span><br><span class="line">ps afx |grep -C 10 2626 <span class="comment">#显示父进程</span></span><br><span class="line">root 2626 2603 0 14:42 ? 00:00:00 [pause]</span><br><span class="line"></span><br><span class="line">ps -ef |grep 2603 <span class="comment">#查看父进程</span></span><br><span class="line">root 2603 27573 0 14:42 ? 00:00:00 docker-containerd-shim -namespace moby -workdir /home/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/ba519a9f1a1102a922bcc74ced7a7fc9fd3f963feea4b8de</span><br><span class="line"></span><br><span class="line">ps -ef |grep 27573</span><br><span class="line">root 27573 27553 0 2018 ? 17:02:40 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line"></span><br><span class="line">ps -ef |grep 27553</span><br><span class="line">root 27553 1 3 2018 ? 3-16:03:11 /usr/bin/dockerd --bip=10.126.64.193/26 --mtu=1500 -g /home/docker -D -H tcp://127.0.0.1:1983 -H unix:///var/run/docker.sock --tlsverify --iptables=<span class="literal">false</span> --storage-driver=devicemapper --storage-opt dm.override_udev_sync_check=<span class="literal">true</span> --storage-opt dm.datadev=/dev/vg_root/dmdata --storage-opt dm.metadatadev=/dev/vg_root/dmmeta --<span class="built_in">exec</span>-opt native.cgroupdriver=cgroupfs</span><br><span class="line"></span><br><span class="line">docker ps |grep ba519a9f1a1 <span class="comment">#查看docker</span></span><br><span class="line">ba519a9f1a11 k8s.gcr.io/pause-amd64:3.1 <span class="string">"/pause"</span> 2 hours ago Up 2 hours k8s_POD_feed-426565da19777e5d325f-5994dc5cff-znqmh_ocean-feed_163f99a9-1aec-11e9-a7cd-246e96ab9970_0</span><br></pre></td></tr></tbody></table></figure><h3 id="可能原因"><a href="#可能原因" class="headerlink" title="可能原因"></a>可能原因</h3><p>可能是内核原因<br><a href="https://stackoverflow.com/questions/34552232/cant-kill-processes-originating-in-a-docker-container" target="_blank" rel="noopener">https://stackoverflow.com/questions/34552232/cant-kill-processes-originating-in-a-docker-container</a><br>目前只有重启物理机才能解决</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pod中时区设置</title>
      <link href="/pod-timezone/"/>
      <url>/pod-timezone/</url>
      
        <content type="html"><![CDATA[<h2 id="Pod设置本地时区的两种方法"><a href="#Pod设置本地时区的两种方法" class="headerlink" title="Pod设置本地时区的两种方法"></a>Pod设置本地时区的两种方法</h2><p>我们下载的很多容器内的时区都是格林尼治时间，与北京时间差8小时，这将导致容器内的日志和文件创建时间与实际时区不符，有两种方式解决这个问题：</p><ul><li>修改镜像中的时区配置文件</li><li>将宿主机的时区配置文件/etc/localtime使用volume方式挂载到容器中</li></ul><h3 id="修改Dockfile"><a href="#修改Dockfile" class="headerlink" title="修改Dockfile"></a>修改Dockfile</h3><p>修改前</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d nginx:latest</span><br><span class="line"></span><br><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS               NAMES</span><br><span class="line">ca7aacad1493        nginx               <span class="string">"nginx -g 'daemon of…"</span>   About a minute ago   Up About a minute   80/tcp              inspiring_elbakyan</span><br><span class="line"></span><br><span class="line">$ docker <span class="built_in">exec</span> -it inspiring_elbakyan date</span><br><span class="line">Wed Feb 13 06:51:41 UTC 2019</span><br><span class="line"></span><br><span class="line">date</span><br><span class="line">Wed Feb 13 14:51:45 CST 2019</span><br></pre></td></tr></tbody></table></figure><p>创建timezone-dockerfile</p><figure class="highlight dockerfile"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> nginx</span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> /bin/cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \</span></span><br><span class="line"><span class="bash">     &amp;&amp; <span class="built_in">echo</span> <span class="string">'Asia/Shanghai'</span> &gt;/etc/timezone</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t timezone -f timezone-dockerfile .</span><br><span class="line"></span><br><span class="line">$ docker run -d timezone</span><br><span class="line">af39a27d8c8b48b80fb9b052144bd682d75d994dba2e03a02101514304f363d0</span><br><span class="line"></span><br><span class="line">$ docker <span class="built_in">exec</span> -it af39a27d8c8b date</span><br><span class="line">Wed Feb 13 15:05:14 CST 2019</span><br><span class="line"></span><br><span class="line">$ date</span><br><span class="line">Wed Feb 13 15:05:16 CST 2019</span><br></pre></td></tr></tbody></table></figure><h3 id="挂载localtime文件"><a href="#挂载localtime文件" class="headerlink" title="挂载localtime文件"></a>挂载localtime文件</h3><p>第二种方式实现更简单，不需要更改镜像，只需要配置yaml文件，步骤如下：</p><p>创建测试pod，busybox-pod.yaml</p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> <span class="attr">busybox:1.28.3</span></span><br><span class="line"><span class="attr">    command:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">sleep</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">    imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">host-time</span></span><br><span class="line"><span class="attr">        mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line"><span class="attr">        readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">host-time</span></span><br><span class="line"><span class="attr">      hostPath:</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/etc/localtime</span></span><br><span class="line"><span class="attr">  restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></tbody></table></figure><p>测试时间</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f busybox-pod.yaml</span><br><span class="line">pod/busybox created</span><br><span class="line"></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it busybox date</span><br><span class="line">Wed Feb 13 06:16:35 UTC 2019</span><br><span class="line"></span><br><span class="line">$ date</span><br><span class="line">Wed Feb 13 14:16:39 CST 2019</span><br></pre></td></tr></tbody></table></figure><p>将/etc/localtime挂载到pod中，配置如下:<br></p><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> <span class="attr">busybox:1.28.3</span></span><br><span class="line"><span class="attr">    command:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">sleep</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"3600"</span></span><br><span class="line"><span class="attr">    imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">busybox</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">host-time</span></span><br><span class="line"><span class="attr">        mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line"><span class="attr">        readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">host-time</span></span><br><span class="line"><span class="attr">      hostPath:</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">/etc/localtime</span></span><br><span class="line"><span class="attr">  restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>  测试时间</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f busybox-pod.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -it busybox date</span><br><span class="line">Wed Feb 13 14:17:50 CST 2019 <span class="comment">#与当前时间一致</span></span><br><span class="line"></span><br><span class="line">$ date</span><br><span class="line">Wed Feb 13 14:17:52 CST 2019</span><br></pre></td></tr></tbody></table></figure><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>/var/log/message 归档探究</title>
      <link href="/var-log-message-logrotate/"/>
      <url>/var-log-message-logrotate/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>由于项目要收集/var/log/messages的日志到es中，发现messages日志按天切割，但归档的时间却不一直，于是乎查了点资料探究下</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>/var/log/messages是由journald生成的，流程如下<br><code>systemd --&gt; systemd-journald --&gt; ram DB --&gt; rsyslog -&gt; /var/log</code><br>当 systemd 启动后，systemd-journald 也会立即启动。将日志存入RAM中，当rsyslog 启动后会读取该RAM并完成筛选分类写入目录 /var/log 。所以牵扯到DB，操作就会很舒服。</p><h3 id="相关服务"><a href="#相关服务" class="headerlink" title="相关服务"></a>相关服务</h3><p>针对日志文件所需的功能，我们需要的服务于进程有：</p><ul><li>systemd-journald.service：最主要的信息收受者，由systemd提供；</li><li>rsystem.service：主要登录系统于网络等服务的信息；</li><li>logrotate：主要在进行日志的轮替功能</li></ul><p>Centos7.x使用systemd提供的journalctl日志管理，基本上，系统由systemd所管理，那所有经由systemd启动的服务（）如果在启动或结束的过程中发生了一些问题或是正常的信息），就会将该信息由systemd-journald.service以二进制的方式记录下来，之后再将信息发个rsyslog.service作进一步的记载。<br>systemd-journald.service的记录主要都放置与内存中，因此在存取方面效能比较好。也能透过journalctl以及systemctl status unit.service 来查看各个不同服务的日志。</p><h3 id="相关配置"><a href="#相关配置" class="headerlink" title="相关配置"></a>相关配置</h3><p>journald配置文件<br><code>cat /etc/systemd/journald.conf</code><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># This file is part of systemd.</span><br><span class="line">#</span><br><span class="line"># systemd is free software; you can redistribute it and/or modify it</span><br><span class="line"># under the terms of the GNU Lesser General Public License as published by</span><br><span class="line"># the Free Software Foundation; either version 2.1 of the License, or</span><br><span class="line"># (at your option) any later version.</span><br><span class="line">#</span><br><span class="line"># Entries in this file show the compile time defaults.</span><br><span class="line"># You can change settings by editing this file.</span><br><span class="line"># Defaults can be restored by simply deleting this file.</span><br><span class="line">#</span><br><span class="line"># See journald.conf(5) for details.</span><br><span class="line"></span><br><span class="line">[Journal]</span><br><span class="line">#Storage=auto</span><br><span class="line">#Compress=yes</span><br><span class="line">#Seal=yes</span><br><span class="line">#SplitMode=uid</span><br><span class="line">#SyncIntervalSec=5m</span><br><span class="line">#RateLimitInterval=30s</span><br><span class="line">#RateLimitBurst=1000</span><br><span class="line">#SystemMaxUse=</span><br><span class="line">#SystemKeepFree=</span><br><span class="line">#SystemMaxFileSize=</span><br><span class="line">#RuntimeMaxUse=</span><br><span class="line">#RuntimeKeepFree=</span><br><span class="line">#RuntimeMaxFileSize=</span><br><span class="line">#MaxRetentionSec=</span><br><span class="line">#MaxFileSec=1month</span><br><span class="line">#ForwardToSyslog=yes #默认转向syslog</span><br><span class="line">#ForwardToKMsg=no</span><br><span class="line">#ForwardToConsole=no</span><br><span class="line">#ForwardToWall=yes</span><br><span class="line">#TTYPath=/dev/console</span><br><span class="line">#MaxLevelStore=debug</span><br><span class="line">#MaxLevelSyslog=debug</span><br><span class="line">#MaxLevelKMsg=notice</span><br><span class="line">#MaxLevelConsole=info</span><br><span class="line">#MaxLevelWall=emerg</span><br></pre></td></tr></tbody></table></figure><p></p><ol><li>目前，centos log 由 rsyslog 管理，设置文件  /var/lib/rsyslog 并兼容syslog的配置文件</li><li>其中messages文件记录系统日志，包括mail、定时任务、系统异常等（*.info;mail.none;authpriv.none;cron.none /var/log/messages）</li><li>Logrotate 实现日志切割，具体由 CRON 实现</li></ol><p>logrotate配置文件<br><code>cat /etc/cron.daily/logrotate</code><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf</span><br><span class="line">EXITVALUE=$?</span><br><span class="line">if [ $EXITVALUE != 0 ]; then</span><br><span class="line">/usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"</span><br><span class="line">fi</span><br><span class="line">exit 0</span><br><span class="line"></span><br><span class="line">实际运行时，Logrotate会调用配置文件「/etc/logrotate.conf」</span><br><span class="line"># see "man logrotate" for details</span><br><span class="line"># rotate log files weekly</span><br><span class="line">weekly # 每月归档一次</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># keep 4 weeks worth of backlogs</span><br><span class="line">rotate 4 # 归档4个周期</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># create new (empty) log files after rotating old ones</span><br><span class="line">create</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># use date as a suffix of the rotated file</span><br><span class="line">dateext </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># uncomment this if you want your log files compressed</span><br><span class="line">#compress # 默认不压缩</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># RPM packages drop log rotation information into this directory</span><br><span class="line">include /etc/logrotate.d #包含其下配置文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># no packages own wtmp and btmp -- we'll rotate them here</span><br><span class="line">/var/log/wtmp {</span><br><span class="line">monthly</span><br><span class="line">create 0664 root utmp</span><br><span class="line">minsize 1M</span><br><span class="line">rotate 1</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/var/log/btmp {</span><br><span class="line">missingok</span><br><span class="line">monthly</span><br><span class="line">create 0600 root utmp</span><br><span class="line">rotate 1</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># system-specific logs may be also be configured here.</span><br></pre></td></tr></tbody></table></figure><p></p><p>设置特殊文件的归档方式<br><code>cat /etc/logrotate.d/syslog</code><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">/var/log/cron</span><br><span class="line">/var/log/maillog</span><br><span class="line">/var/log/messages</span><br><span class="line">/var/log/secure</span><br><span class="line">/var/log/spooler</span><br><span class="line">{</span><br><span class="line">daily</span><br><span class="line">rotate 4</span><br><span class="line">compress</span><br><span class="line">delaycompress # 延迟一个周期压缩</span><br><span class="line">missingok # 日志丢失不报错</span><br><span class="line">sharedscripts # 运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本</span><br><span class="line">postrotate # 在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行</span><br><span class="line">/bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</span><br><span class="line">endscript</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p><p>messages中日志生成时间大多是晚上3点多，这是由cron控制的<br><code>cat /etc/anacrontab</code><br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># /etc/anacrontab: configuration file for anacron</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># See anacron(8) and anacrontab(5) for details.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SHELL=/bin/sh</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line"># the maximal random delay added to the base delay of the jobs</span><br><span class="line">RANDOM_DELAY=45 # 随机延迟最大时间</span><br><span class="line"># the jobs will be started during the following hours only</span><br><span class="line">START_HOURS_RANGE=3-22 # 3点到22点执行</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#period in days delay in minutes job-identifier command</span><br><span class="line">1 5 cron.daily nice run-parts /etc/cron.daily # 第一天执行，延迟5分钟</span><br><span class="line">7 25 cron.weekly nice run-parts /etc/cron.weekly</span><br><span class="line">@monthly 45 cron.monthly nice run-parts /etc/cron.monthly</span><br><span class="line">日志生成时间在03:05~03:50 随机延迟时间 5~5+45</span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s serviceaccount挂载pod问题</title>
      <link href="/k8s-serviceaccount-mount-failed/"/>
      <url>/k8s-serviceaccount-mount-failed/</url>
      
        <content type="html"><![CDATA[<h2 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h2><p>用户创建role失败，报错:<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f role.yml </span><br><span class="line">Error from server (Forbidden): error when creating <span class="string">"role.yml"</span>: roles.rbac.authorization.k8s.io <span class="string">"pod-modifier"</span> is forbidden: attempt to grant extra privileges: [PolicyRule{APIGroups:[<span class="string">""</span>], Resources:[<span class="string">"pods"</span>], Verbs:[<span class="string">"get"</span>]}] user=&amp;{<span class="built_in">test</span> <span class="built_in">test</span> [system:authenticated] map[]} ownerrules=[PolicyRule{APIGroups:[<span class="string">"authorization.k8s.io"</span>], Resources:[<span class="string">"selfsubjectaccessreviews"</span> <span class="string">"selfsubjectrulesreviews"</span>], Verbs:[<span class="string">"create"</span>]} PolicyRule{NonResourceURLs:[<span class="string">"/api"</span> <span class="string">"/api/*"</span> <span class="string">"/apis"</span> <span class="string">"/apis/*"</span> <span class="string">"/healthz"</span> <span class="string">"/openapi"</span> <span class="string">"/openapi/*"</span> <span class="string">"/swagger-2.0.0.pb-v1"</span> <span class="string">"/swagger.json"</span> <span class="string">"/swaggerapi"</span> <span class="string">"/swaggerapi/*"</span> <span class="string">"/version"</span> <span class="string">"/version/"</span>], Verbs:[<span class="string">"get"</span>]}] ruleResolutionErrors=[]</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>错误显示<code>user=&amp;{test test [system:authenticated] map[]}</code>这个user没有权限，在<code>~/.kube/config</code>添加admin用户</p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>创建serviceaccount后，没有挂载到pod中</p><h3 id="解决-1"><a href="#解决-1" class="headerlink" title="解决"></a>解决</h3><p>需要在apiserver配置中开启，添加<code>--admission-control=ServiceAccount --authorization-mode=RBAC</code>，重启<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl restart kube-apiserver</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p>添加配置后,直接创建pod能够加载sa与token，创建deployment则不行</p><h3 id="解决-2"><a href="#解决-2" class="headerlink" title="解决"></a>解决</h3><p>kubeconfig配置有问题，确认其中user的配置</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>apline 容器中执行shell脚本报错 executable file not found</title>
      <link href="/apline-shell-executable-file-not-found/"/>
      <url>/apline-shell-executable-file-not-found/</url>
      
        <content type="html"><![CDATA[<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>今天在创建java镜像时，使用了(openjdk:8-jdk-alpine)[<a href="https://hub.docker.com/_/openjdk]，启动容器后需要运行一java脚本，直接执行" target="_blank" rel="noopener">https://hub.docker.com/_/openjdk]，启动容器后需要运行一java脚本，直接执行</a><br><code>./test.sh</code>报错<em>sh:.sh test.sh: not found</em></p><p>网上有说由于权限文件不能执行，<code>chmod +x test.sh</code>，仍然报错</p><p>在k8s pod中部署，describe报错<br><em>Error: failed to start container “docker-registry”: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting container process caused “exec: \”sh +x start-registry.sh test\”: executable file not found in $PATH”: unknown</em></p><p>若使用<code>sh +x test.sh</code>则可运行， pod中仍然报错</p><h2 id="探究"><a href="#探究" class="headerlink" title="探究"></a>探究</h2><p>直到看到stackoverflow上有同样的问题(docker alpine /bin/sh script.sh not found)[<a href="https://stackoverflow.com/questions/45860784/bin-bash-command-not-found-in-alpine-docker#]" target="_blank" rel="noopener">https://stackoverflow.com/questions/45860784/bin-bash-command-not-found-in-alpine-docker#]</a>,<br>说与shell脚本中的执行器有关，test.sh的执行器是<code>#!/bin/bash</code>，而alpine默认的是<code>/bin/ash, /bin/sh</code>没有bash，所以执行会报错</p><p>问题找到，至于bash与sh的区别，网上有不少解释<br>alpine中sh只是一个符号链接<br><code>sh -&gt; /bin/busybox</code></p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><h3 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h3><p>将shell脚本中<code>#!/bin/bash</code>改为<code>/bin/sh</code></p><h3 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h3><p>镜像中添加bash<br></p><figure class="highlight dockerfile"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> alpine:<span class="number">3.9</span></span><br><span class="line"><span class="keyword">RUN</span> apk update &amp;&amp; \</span><br><span class="line">    apk add --no-cache bash &amp;&amp; \</span><br><span class="line">    rm -rf /var/cache/apk/* /tmp/* /var/tmp/* $HOME/.cache</span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deployment升级的几种方式</title>
      <link href="/k8s-deployment-upgrade-kinds/"/>
      <url>/k8s-deployment-upgrade-kinds/</url>
      
        <content type="html"><![CDATA[<p>对于已经运行的deploy，有以下几种升级方式</p><h3 id="kubectl升级方式"><a href="#kubectl升级方式" class="headerlink" title="kubectl升级方式"></a>kubectl升级方式</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">set</span> image&nbsp;deployments/busy busy=busybox:1.29</span><br><span class="line"></span><br><span class="line">kubectl apply -f rolling-update-test.yaml</span><br><span class="line"></span><br><span class="line">kubectl edit deployment/rolling-update-test</span><br><span class="line"></span><br><span class="line">kubectl scale --replicas=3 rs/foo</span><br></pre></td></tr></tbody></table></figure><h3 id="kubectl回滚"><a href="#kubectl回滚" class="headerlink" title="kubectl回滚"></a>kubectl回滚</h3><p>查看升级<br><code>kubectl rollout status deployment/rolling-update-test</code><br>回滚<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl rollout undo deployments/busy</span><br><span class="line">kubectl rollout status deployments/busy</span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>apline镜像添加时区与字符设置</title>
      <link href="/apline-timezone/"/>
      <url>/apline-timezone/</url>
      
        <content type="html"><![CDATA[<ol><li><p>添加时区<br>设置<code>TZ</code>与安装tzdata</p></li><li><p>添加work用户<br><code>addgroup -S work &amp;&amp; adduser -S -G work work -s /bin/sh</code></p></li><li><p>设置字符格式<br>设置环境变量<code>LANG</code>与<code>LC_ALL</code></p></li></ol><p>Dockerfile如下：<br></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine</span><br><span class="line"></span><br><span class="line">ENV TZ=Asia/Shanghai \</span><br><span class="line">    LANG=en_US.UTF-8  \</span><br><span class="line">    LC_ALL=en_US.UTF8</span><br><span class="line">    </span><br><span class="line">RUN apk update &amp;&amp; \</span><br><span class="line">    apk add --no-cache tzdata &amp;&amp; \</span><br><span class="line">    addgroup -S work &amp;&amp; adduser -S -G work work -s /bin/bash &amp;&amp; \</span><br><span class="line">    rm -rf /var/cache/apk/* /tmp/* /var/tmp/* $HOME/.cache</span><br></pre></td></tr></tbody></table></figure><p></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker多阶段构建</title>
      <link href="/multi-stage-dockerfile/"/>
      <url>/multi-stage-dockerfile/</url>
      
        <content type="html"><![CDATA[<h2 id="之前的做法"><a href="#之前的做法" class="headerlink" title="之前的做法"></a>之前的做法</h2><p>在 Docker 17.05 版本之前，我们构建 Docker 镜像时，通常会采用两种方式：</p><h3 id="全部放入一个-Dockerfile"><a href="#全部放入一个-Dockerfile" class="headerlink" title="全部放入一个 Dockerfile"></a>全部放入一个 Dockerfile</h3><p>一种方式是将所有的构建过程编包含在一个  Dockerfile  中，包括项目及其依赖库的编译、测试、打包等流程，这里可能会带来的一些问题：</p><ul><li>Dockerfile  特别长，可维护性降低</li><li>镜像层次多，镜像体积较大，部署时间变长</li><li>源代码存在泄露的风险</li></ul><p>例如编写  app.go  文件，该程序输出  Hello World! </p><figure class="highlight golang"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main  </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span>  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span>{  </span><br><span class="line">    fmt.Printf(<span class="string">"Hello World!"</span>);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>编写  Dockerfile.one  文件</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.9-alpine</span><br><span class="line"></span><br><span class="line">RUN apk --no-cache add git ca-certificates</span><br><span class="line"></span><br><span class="line">WORKDIR /go/src/github.com/go/helloworld/</span><br><span class="line"></span><br><span class="line">COPY app.go .</span><br><span class="line"></span><br><span class="line">RUN go get -d -v github.com/go-sql-driver/mysql \</span><br><span class="line">  &amp;&amp; CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app . \</span><br><span class="line">  &amp;&amp; cp /go/src/github.com/go/helloworld/app /root</span><br><span class="line"></span><br><span class="line">WORKDIR /root/</span><br><span class="line"></span><br><span class="line">CMD ["./app"]</span><br></pre></td></tr></tbody></table></figure><p>构建镜像</p><p><code>$ docker build -t go/helloworld:1 -f Dockerfile.one .</code></p><h3 id="分散到多个-Dockerfile"><a href="#分散到多个-Dockerfile" class="headerlink" title="分散到多个 Dockerfile"></a>分散到多个 Dockerfile</h3><p>另一种方式，就是我们事先在一个  Dockerfile  将项目及其依赖库编译测试打包好后，再将其拷贝到运行环境中，这种方式需要我们编写两个  Dockerfile  和一些编译脚本才能将其两个阶段自动整合起来，这种方式虽然可以很好地规避第一种方式存在的风险，但明显部署过程较复杂。</p><p>例如，编写  Dockerfile.build  文件</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.9-alpine</span><br><span class="line"></span><br><span class="line">RUN apk --no-cache add git</span><br><span class="line"></span><br><span class="line">WORKDIR /go/src/github.com/go/helloworld</span><br><span class="line"></span><br><span class="line">COPY app.go .</span><br><span class="line"></span><br><span class="line">RUN go get -d -v github.com/go-sql-driver/mysql \</span><br><span class="line">  &amp;&amp; CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br></pre></td></tr></tbody></table></figure><p>编写  Dockerfile.copy  文件</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest</span><br><span class="line"></span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line"></span><br><span class="line">WORKDIR /root/</span><br><span class="line"></span><br><span class="line">COPY app .</span><br><span class="line"></span><br><span class="line">CMD ["./app"]</span><br></pre></td></tr></tbody></table></figure><p>新建  build.sh </p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"><span class="built_in">echo</span> Building go/helloworld:build</span><br><span class="line"></span><br><span class="line">docker build -t go/helloworld:build . -f Dockerfile.build</span><br><span class="line"></span><br><span class="line">docker create --name extract go/helloworld:build</span><br><span class="line">docker cp extract:/go/src/github.com/go/helloworld/app ./app</span><br><span class="line">docker rm -f extract</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> Building go/helloworld:2</span><br><span class="line"></span><br><span class="line">docker build --no-cache -t go/helloworld:2 . -f Dockerfile.copy</span><br><span class="line">rm ./app</span><br></pre></td></tr></tbody></table></figure><p>现在运行脚本即可构建镜像</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ chmod +x build.sh</span><br><span class="line">$ ./build.sh</span><br></pre></td></tr></tbody></table></figure><p>对比两种方式生成的镜像大小</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker image ls</span><br><span class="line"></span><br><span class="line">REPOSITORY      TAG    IMAGE ID        CREATED         SIZE</span><br><span class="line">go/helloworld   2      f7cf3465432c    22 seconds ago  6.47MB</span><br><span class="line">go/helloworld   1      f55d3e16affc    2 minutes ago   295MB</span><br></pre></td></tr></tbody></table></figure><h2 id="使用多阶段构建"><a href="#使用多阶段构建" class="headerlink" title="使用多阶段构建"></a>使用多阶段构建</h2><p>为解决以上问题，Docker v17.05 开始支持多阶段构建 ( multistage builds )。使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个  Dockerfile ：</p><p>例如，编写  Dockerfile  文件</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.9-alpine as builder</span><br><span class="line"></span><br><span class="line">RUN apk --no-cache add git</span><br><span class="line"></span><br><span class="line">WORKDIR /go/src/github.com/go/helloworld/</span><br><span class="line"></span><br><span class="line">RUN go get -d -v github.com/go-sql-driver/mysql</span><br><span class="line"></span><br><span class="line">COPY app.go .</span><br><span class="line"></span><br><span class="line">RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .</span><br><span class="line"></span><br><span class="line">FROM alpine:latest as prod</span><br><span class="line"></span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line"></span><br><span class="line">WORKDIR /root/</span><br><span class="line"></span><br><span class="line">COPY --from=0 /go/src/github.com/go/helloworld/app .</span><br><span class="line"></span><br><span class="line">CMD ["./app"]</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">构建镜像</span><br><span class="line"></span><br><span class="line">`$ docker build -t go/helloworld:3 .`</span><br><span class="line"> </span><br><span class="line">对比三个镜像大小</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">$ docker image ls</span><br><span class="line"></span><br><span class="line">REPOSITORY        TAG   IMAGE ID         CREATED            SIZE</span><br><span class="line">go/helloworld     3     d6911ed9c846     7 seconds ago      6.47MB</span><br><span class="line">go/helloworld     2     f7cf3465432c     22 seconds ago     6.47MB</span><br><span class="line">go/helloworld     1     f55d3e16affc     2 minutes ago      295MB</span><br></pre></td></tr></tbody></table></figure><p>很明显使用多阶段构建的镜像体积小，同时也完美解决了上边提到的问题。</p><p>只构建某一阶段的镜像<br>我们可以使用  as  来为某一阶段命名，例如</p><p><code>FROM golang:1.9-alpine as builder</code></p><p>例如当我们只想构建  builder  阶段的镜像时，我们可以在使用  docker build  命令时加上  –target  参数即可</p><p><code>$ docker build --target builder -t username/imagename:tag .</code></p><p>构建时从其他镜像复制文件<br>上面例子中我们使用  COPY –from=0 /go/src/github.com/go/helloworld/app .  从上一阶段的镜像中复制文件，我们也可以复制任意镜像中的文件。</p><p><code>$ COPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf</code></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kubectl命令行自动补全</title>
      <link href="/kubectl-auto-completion/"/>
      <url>/kubectl-auto-completion/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source &lt;(kubectl completion bash)</span><br><span class="line">echo "source &lt;(kubectl completion bash)" &gt;&gt; ~/.bashrc</span><br></pre></td></tr></tbody></table></figure><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> cloud </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo建立个人blog</title>
      <link href="/hexo-blog/"/>
      <url>/hexo-blog/</url>
      
        <content type="html"><![CDATA[<p>学到的分享出来才更有意义，春节前事不多使用hexo建立了个人blog，记录下。</p><h2 id="gitlab-pages"><a href="#gitlab-pages" class="headerlink" title="gitlab pages"></a>gitlab pages</h2><p>创建个以<code>your_github_name.github.io</code>为名称的仓库，创建个<a href="https://help.github.com/articles/configuring-a-publishing-source-for-github-pages/" target="_blank" rel="noopener">github page</a></p><h2 id="hexo建站"><a href="#hexo建站" class="headerlink" title="hexo建站"></a>hexo建站</h2><h3 id="创建hexo分支"><a href="#创建hexo分支" class="headerlink" title="创建hexo分支"></a>创建hexo分支</h3><p>为<code>your_github_name.github.io</code>创建hexo分支，存放<a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">hexo</a>配置，避免维护两个仓库</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git@...your_github_name.github.io</span><br><span class="line"><span class="built_in">cd</span> your_github_name.github.io</span><br><span class="line">git checkout --orphan hexo</span><br></pre></td></tr></tbody></table></figure><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>hexo初始化<br><code>hexo init</code></p><p>安装需要组件<br><code>sudo npm install</code></p><p>根据hexo官网配置<code>_config.yml</code>文件</p><h2 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h2><p>使用<a href="https://molunerfinn.com/hexo-theme-melody-doc/#/zh-Hans/quick-start" target="_blank" rel="noopener">melody主题</a>，参考官网自定义相关配置</p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo g <span class="comment">#生成静态文件</span></span><br><span class="line">hexo n <span class="string">"name"</span> <span class="comment">#创建blo</span></span><br><span class="line">hexo server <span class="comment">#预览</span></span><br><span class="line">hexo deploy <span class="comment">#推送到github</span></span><br></pre></td></tr></tbody></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://help.github.com/articles/configuring-a-publishing-source-for-github-pages/" target="_blank" rel="noopener">https://help.github.com/articles/configuring-a-publishing-source-for-github-pages/</a></li><li><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">https://hexo.io/zh-cn/docs/</a></li><li><a href="https://molunerfinn.com/hexo-theme-melody-doc/#/zh-Hans/quick-star" target="_blank" rel="noopener">https://molunerfinn.com/hexo-theme-melody-doc/#/zh-Hans/quick-star</a></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
